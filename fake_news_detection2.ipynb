{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fake_news_detection2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzncODGgVu4a",
        "outputId": "9d00ce9e-1598-4936-a1bc-c3ec62f0af1b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95rM3srFhFjF",
        "outputId": "d04a2808-cc13-41dc-a998-1d5c978f9980"
      },
      "source": [
        "!pip install --upgrade tensorflow-hub"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlJgORzLmuLO",
        "outputId": "55895b74-7bd8-4756-ee9d-2d079f179d84"
      },
      "source": [
        "!pip install spektral"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spektral\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/b2/7b51a6b2719a085fa70ab5ef523d0ca6a68bf1751c02d3487c8e0e6c11a3/spektral-1.0.2-py3-none-any.whl (111kB)\n",
            "\r\u001b[K     |███                             | 10kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 20kB 22.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 30kB 21.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 40kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 51kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 61kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 71kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 81kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 92kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 102kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from spektral) (2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from spektral) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from spektral) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from spektral) (1.1.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from spektral) (4.2.6)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from spektral) (2.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from spektral) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from spektral) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from spektral) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from spektral) (0.17.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->spektral) (4.4.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->spektral) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->spektral) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->spektral) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->spektral) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->spektral) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->spektral) (2.8.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (0.3.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (2.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (3.12.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (0.2.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (2.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.12.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (2.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (0.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.33.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (0.35.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (50.3.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (2.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (3.1.0)\n",
            "Installing collected packages: spektral\n",
            "Successfully installed spektral-1.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waVF_0MNW-dv"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import scipy.sparse as sp\n",
        "import pandas as pd \n",
        "import random\n",
        "import json\n",
        "import os\n",
        "\n",
        "# from feature_matrix import FeatureMatrix\n",
        "\n",
        "import pandas as pd \n",
        "import tensorflow_hub as hub\n",
        "from bert import run_classifier\n",
        "from bert import tokenization\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba_5jRy6XBxP"
      },
      "source": [
        "##Code referenced from: \n",
        "##Website Title: BERT in Keras with Tensorflow hub\n",
        "##URL : https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\")\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                              tokenization_info[\"do_lower_case\"]])\n",
        "\n",
        "  return tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZgYEexvaC0u"
      },
      "source": [
        "    def getFeatures( dataset=\"BuzzFeed\"):\n",
        "        feature_df =  FM.get_feature_matrix(dataset)\n",
        "        label = feature_df['label'].tolist()\n",
        "        label_comp = [0 if each else 1 for each in label]\n",
        "        global label_zip\n",
        "        label_zip = list(zip(label_comp, label))\n",
        "        feature_df.drop(['label'], axis=1)\n",
        "        feature_np = feature_df.values\n",
        "\n",
        "        return sp.csr_matrix(feature_np, dtype=float).tolil()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZakw4OzXF0Q"
      },
      "source": [
        "def getYs():\n",
        "\n",
        "  random.seed(1)\n",
        "  yTrain =  label_zip[:]\n",
        "  yVal =  label_zip[:]\n",
        "  yTest =  label_zip[:]\n",
        "  train_mask = [False] * len(yTrain)\n",
        "  val_mask = [False] * len(yTrain)\n",
        "  test_mask = [False] * len(yTrain)\n",
        "  n = len(yTrain)\n",
        "\n",
        "  set_of_records_range = set(range(n))\n",
        "\n",
        "  train_range = set(random.sample(set_of_records_range, k=int(n * 0.6)))\n",
        "  set_of_records_range = set_of_records_range - train_range\n",
        "\n",
        "  val_range = set(random.sample(set_of_records_range, k=int(n * 0.2)))\n",
        "  set_of_records_range = set_of_records_range - train_range\n",
        "\n",
        "  test_range = set(random.sample(set_of_records_range, k=int(n * 0.2)))\n",
        "\n",
        "  for i in train_range:\n",
        "      yVal[i] = (0,0)\n",
        "      yTest[i] = (0,0)\n",
        "      train_mask[i] = True\n",
        "  for i in val_range:\n",
        "      yTrain[i] = (0,0)\n",
        "      yTest[i] = (0,0)\n",
        "      val_mask[i] = True\n",
        "  for i in test_range:\n",
        "      yVal[i] = (0,0)\n",
        "      yTrain[i] = (0,0)\n",
        "      test_mask[i] = True\n",
        "\n",
        "  return yTrain, yVal, yTest, train_mask, val_mask, test_mask\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_p_V2wJ_mcp"
      },
      "source": [
        "def sparse_to_tuple(sparse_mx):\n",
        "  \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "  def to_tuple(mx):\n",
        "    if not sp.isspmatrix_coo(mx):\n",
        "        mx = mx.tocoo()\n",
        "    coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "    values = mx.data\n",
        "    shape = mx.shape\n",
        "    return coords, values, shape\n",
        "\n",
        "  if isinstance(sparse_mx, list):\n",
        "    for i in range(len(sparse_mx)):\n",
        "        sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "  else:\n",
        "    sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "  return sparse_mx"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy2gbWs_f1vI"
      },
      "source": [
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1, dtype=float).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return features.todense(), sparse_to_tuple(features)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8olrwshX3qR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f2e3531-7e39-41aa-bf33-480793c9ebb2"
      },
      "source": [
        "base_path = \"/content/drive/MyDrive/Colab_Notebooks/DL_final/\"\n",
        "folder = \"BuzzFeed\"\n",
        "data_list = []\n",
        "for subfolder in [\"FakeNewsContent\", \"RealNewsContent\"]:\n",
        "  print(\"Getting data from subfolder: \", subfolder)\n",
        "  file_array = [f for f in os.listdir(base_path + folder + \"/\" + subfolder) if f.endswith('.json')]\n",
        "  file_array.sort() # file is sorted list\n",
        "  file_array = [os.path.join(base_path + folder + \"/\" + subfolder, name) for name in file_array]\n",
        "\n",
        "  for file in file_array:\n",
        "    # print(\"file: \", file)\n",
        "    # print(\"path: \",base_path + folder + \"/\" + subfolder)\n",
        "    with open(file, 'r') as json_file:\n",
        "      data = json.load(json_file)\n",
        "      if file.split(\"/\")[-2] == \"FakeNewsContent\":\n",
        "          data_list.append([data['text'], 1])\n",
        "      else:\n",
        "          data_list.append([data['text'], 0])\n",
        "\n",
        "print(\"Creating data frame\")\n",
        "data_frame = pd.DataFrame(data_list, columns=[\"text\", \"label\"])\n",
        "data_frame = data_frame.sample(frac=1)\n",
        "\n",
        "input = data_frame.apply(lambda x: run_classifier.InputExample(guid=None, \n",
        "                                                                       text_a=x['text'], text_b=None, label=x['label']), axis=1)\n",
        "print(\"extracting features\")\n",
        "tokenizer = create_tokenizer_from_hub_module()\n",
        "features = run_classifier.convert_examples_to_features(input, [0, 1], 128, tokenizer)\n",
        "\n",
        "train_features_list = []\n",
        "for item in features:\n",
        "  temp = item.input_ids\n",
        "  temp.append(item.label_id)\n",
        "  train_features_list.append(temp)\n",
        "column_names = [\"feature\" + str(i) for i in range(128)]\n",
        "column_names.append(\"label\")\n",
        "features_frame = pd.DataFrame(train_features_list, columns=column_names)  ##extracted features data frame\n",
        "# /content/drive/MyDrive/Colab_Notebooks/DL_final/BuzzFeed/FakeNewsContent/BuzzFeed_Fake_3-Webpage.json\n",
        "\n",
        "label = features_frame['label'].tolist()\n",
        "label_comp = [0 if each else 1 for each in label]\n",
        "global label_zip\n",
        "label_zip = list(zip(label_comp, label))\n",
        "features_frame.drop(['label'], axis=1)\n",
        "feature_np = features_frame.values"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting data from subfolder:  FakeNewsContent\n",
            "Getting data from subfolder:  RealNewsContent\n",
            "Creating data frame\n",
            "extracting features\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 182\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 182\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] breaking : pipe bombs found in new jersey train station first a pipe bomb exploded on the jersey shore , then new york was next . now , pipe bombs have been found in a new jersey train station . according to the new york daily news , “ authorities discovered three pipe bombs and two smaller devices at a train station in elizabeth . ” new jersey : elizabeth mayor says a bag with wires and pipes was found in a trash can near nj ##t station , fbi on scene . https : / / t . co / q ##60 ##l ##ggs ##52 ##o — ko ##l ##ha ##ola ##m ( @ ko ##l ##ha ##ola ##m ) september 19 , 2016 since [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] breaking : pipe bombs found in new jersey train station first a pipe bomb exploded on the jersey shore , then new york was next . now , pipe bombs have been found in a new jersey train station . according to the new york daily news , “ authorities discovered three pipe bombs and two smaller devices at a train station in elizabeth . ” new jersey : elizabeth mayor says a bag with wires and pipes was found in a trash can near nj ##t station , fbi on scene . https : / / t . co / q ##60 ##l ##ggs ##52 ##o — ko ##l ##ha ##ola ##m ( @ ko ##l ##ha ##ola ##m ) september 19 , 2016 since [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4911 1024 8667 9767 2179 1999 2047 3933 3345 2276 2034 1037 8667 5968 9913 2006 1996 3933 5370 1010 2059 2047 2259 2001 2279 1012 2085 1010 8667 9767 2031 2042 2179 1999 1037 2047 3933 3345 2276 1012 2429 2000 1996 2047 2259 3679 2739 1010 1523 4614 3603 2093 8667 9767 1998 2048 3760 5733 2012 1037 3345 2276 1999 3870 1012 1524 2047 3933 1024 3870 3664 2758 1037 4524 2007 14666 1998 12432 2001 2179 1999 1037 11669 2064 2379 19193 2102 2276 1010 8495 2006 3496 1012 16770 1024 1013 1013 1056 1012 2522 1013 1053 16086 2140 21314 25746 2080 1517 12849 2140 3270 6030 2213 1006 1030 12849 2140 3270 6030 2213 1007 2244 2539 1010 2355 2144 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4911 1024 8667 9767 2179 1999 2047 3933 3345 2276 2034 1037 8667 5968 9913 2006 1996 3933 5370 1010 2059 2047 2259 2001 2279 1012 2085 1010 8667 9767 2031 2042 2179 1999 1037 2047 3933 3345 2276 1012 2429 2000 1996 2047 2259 3679 2739 1010 1523 4614 3603 2093 8667 9767 1998 2048 3760 5733 2012 1037 3345 2276 1999 3870 1012 1524 2047 3933 1024 3870 3664 2758 1037 4524 2007 14666 1998 12432 2001 2179 1999 1037 11669 2064 2379 19193 2102 2276 1010 8495 2006 3496 1012 16770 1024 1013 1013 1056 1012 2522 1013 1053 16086 2140 21314 25746 2080 1517 12849 2140 3270 6030 2213 1006 1030 12849 2140 3270 6030 2213 1007 2244 2539 1010 2355 2144 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 61 . 3 ##k shares facebook twitter update : buzz ##fe ##ed has deemed that our description of the crime and the victims is false . we have utterly de ##bu ##nk ##ed buzz ##fe ##ed ’ s claims . you can view our follow ##up article here . ~ ~ ~ you can ’ t make this stuff up . people used to tell crazy stories , but have no way to prove if the story was true or not . then along came video . this is by far one of the cr ##azi ##est things i ’ ve ever seen . these guys are out in the middle of the street and it looks like there was supposed to be a fight of [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 61 . 3 ##k shares facebook twitter update : buzz ##fe ##ed has deemed that our description of the crime and the victims is false . we have utterly de ##bu ##nk ##ed buzz ##fe ##ed ’ s claims . you can view our follow ##up article here . ~ ~ ~ you can ’ t make this stuff up . people used to tell crazy stories , but have no way to prove if the story was true or not . then along came video . this is by far one of the cr ##azi ##est things i ’ ve ever seen . these guys are out in the middle of the street and it looks like there was supposed to be a fight of [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6079 1012 1017 2243 6661 9130 10474 10651 1024 12610 7959 2098 2038 8357 2008 2256 6412 1997 1996 4126 1998 1996 5694 2003 6270 1012 2057 2031 12580 2139 8569 8950 2098 12610 7959 2098 1521 1055 4447 1012 2017 2064 3193 2256 3582 6279 3720 2182 1012 1066 1066 1066 2017 2064 1521 1056 2191 2023 4933 2039 1012 2111 2109 2000 2425 4689 3441 1010 2021 2031 2053 2126 2000 6011 2065 1996 2466 2001 2995 2030 2025 1012 2059 2247 2234 2678 1012 2023 2003 2011 2521 2028 1997 1996 13675 16103 4355 2477 1045 1521 2310 2412 2464 1012 2122 4364 2024 2041 1999 1996 2690 1997 1996 2395 1998 2009 3504 2066 2045 2001 4011 2000 2022 1037 2954 1997 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6079 1012 1017 2243 6661 9130 10474 10651 1024 12610 7959 2098 2038 8357 2008 2256 6412 1997 1996 4126 1998 1996 5694 2003 6270 1012 2057 2031 12580 2139 8569 8950 2098 12610 7959 2098 1521 1055 4447 1012 2017 2064 3193 2256 3582 6279 3720 2182 1012 1066 1066 1066 2017 2064 1521 1056 2191 2023 4933 2039 1012 2111 2109 2000 2425 4689 3441 1010 2021 2031 2053 2126 2000 6011 2065 1996 2466 2001 2995 2030 2025 1012 2059 2247 2234 2678 1012 2023 2003 2011 2521 2028 1997 1996 13675 16103 4355 2477 1045 1521 2310 2412 2464 1012 2122 4364 2024 2041 1999 1996 2690 1997 1996 2395 1998 2009 3504 2066 2045 2001 4011 2000 2022 1037 2954 1997 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] hillary ’ s top donor country just auction ##ed off isis sex slaves … why in the hell do we still support saudi arabia ? we don ’ t need their oil … we would make do . the majority of the 9 / 11 terrorists came from saudi arabia , as does the bin laden family . the saudi deputy crown prince told reporters in june the kingdom of saudi arabia has funded 20 % of hillary clinton ’ s campaign . she ’ s taking money from a stone cold enemy and that ’ s not the only one by any means . you do not consort or take money from those who intend to destroy your country … period . that goes for [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] hillary ’ s top donor country just auction ##ed off isis sex slaves … why in the hell do we still support saudi arabia ? we don ’ t need their oil … we would make do . the majority of the 9 / 11 terrorists came from saudi arabia , as does the bin laden family . the saudi deputy crown prince told reporters in june the kingdom of saudi arabia has funded 20 % of hillary clinton ’ s campaign . she ’ s taking money from a stone cold enemy and that ’ s not the only one by any means . you do not consort or take money from those who intend to destroy your country … period . that goes for [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 18520 1521 1055 2327 15009 2406 2074 10470 2098 2125 18301 3348 7179 1529 2339 1999 1996 3109 2079 2057 2145 2490 8174 9264 1029 2057 2123 1521 1056 2342 2037 3514 1529 2057 2052 2191 2079 1012 1996 3484 1997 1996 1023 1013 2340 15554 2234 2013 8174 9264 1010 2004 2515 1996 8026 14887 2155 1012 1996 8174 4112 4410 3159 2409 12060 1999 2238 1996 2983 1997 8174 9264 2038 6787 2322 1003 1997 18520 7207 1521 1055 3049 1012 2016 1521 1055 2635 2769 2013 1037 2962 3147 4099 1998 2008 1521 1055 2025 1996 2069 2028 2011 2151 2965 1012 2017 2079 2025 13440 2030 2202 2769 2013 2216 2040 13566 2000 6033 2115 2406 1529 2558 1012 2008 3632 2005 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 18520 1521 1055 2327 15009 2406 2074 10470 2098 2125 18301 3348 7179 1529 2339 1999 1996 3109 2079 2057 2145 2490 8174 9264 1029 2057 2123 1521 1056 2342 2037 3514 1529 2057 2052 2191 2079 1012 1996 3484 1997 1996 1023 1013 2340 15554 2234 2013 8174 9264 1010 2004 2515 1996 8026 14887 2155 1012 1996 8174 4112 4410 3159 2409 12060 1999 2238 1996 2983 1997 8174 9264 2038 6787 2322 1003 1997 18520 7207 1521 1055 3049 1012 2016 1521 1055 2635 2769 2013 1037 2962 3147 4099 1998 2008 1521 1055 2025 1996 2069 2028 2011 2151 2965 1012 2017 2079 2025 13440 2030 2202 2769 2013 2216 2040 13566 2000 6033 2115 2406 1529 2558 1012 2008 3632 2005 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 7 . 5 ##k shares facebook twitter a public high school has been accused of indo ##ct ##rina ##ted islam into their students . allegedly , the school has been man ##dating children prof ##ess the islamic statement of faith , memo ##rize the five pillars of islam , as well as teaching students that the muslims faith is stronger than a christian or jews . this is all according to lawsuit filed in federal court this past wednesday . the lawsuit was filed on behalf of john and melissa wood with the thomas more law center and the action is being taken against la plata high school in maryland . according to john wood the school banished him from their property when he complained about [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 7 . 5 ##k shares facebook twitter a public high school has been accused of indo ##ct ##rina ##ted islam into their students . allegedly , the school has been man ##dating children prof ##ess the islamic statement of faith , memo ##rize the five pillars of islam , as well as teaching students that the muslims faith is stronger than a christian or jews . this is all according to lawsuit filed in federal court this past wednesday . the lawsuit was filed on behalf of john and melissa wood with the thomas more law center and the action is being taken against la plata high school in maryland . according to john wood the school banished him from their property when he complained about [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1021 1012 1019 2243 6661 9130 10474 1037 2270 2152 2082 2038 2042 5496 1997 11424 6593 11796 3064 7025 2046 2037 2493 1012 9382 1010 1996 2082 2038 2042 2158 16616 2336 11268 7971 1996 5499 4861 1997 4752 1010 24443 25709 1996 2274 13766 1997 7025 1010 2004 2092 2004 4252 2493 2008 1996 7486 4752 2003 6428 2084 1037 3017 2030 5181 1012 2023 2003 2035 2429 2000 9870 6406 1999 2976 2457 2023 2627 9317 1012 1996 9870 2001 6406 2006 6852 1997 2198 1998 9606 3536 2007 1996 2726 2062 2375 2415 1998 1996 2895 2003 2108 2579 2114 2474 19534 2152 2082 1999 5374 1012 2429 2000 2198 3536 1996 2082 21319 2032 2013 2037 3200 2043 2002 10865 2055 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1021 1012 1019 2243 6661 9130 10474 1037 2270 2152 2082 2038 2042 5496 1997 11424 6593 11796 3064 7025 2046 2037 2493 1012 9382 1010 1996 2082 2038 2042 2158 16616 2336 11268 7971 1996 5499 4861 1997 4752 1010 24443 25709 1996 2274 13766 1997 7025 1010 2004 2092 2004 4252 2493 2008 1996 7486 4752 2003 6428 2084 1037 3017 2030 5181 1012 2023 2003 2035 2429 2000 9870 6406 1999 2976 2457 2023 2627 9317 1012 1996 9870 2001 6406 2006 6852 1997 2198 1998 9606 3536 2007 1996 2726 2062 2375 2415 1998 1996 2895 2003 2108 2579 2114 2474 19534 2152 2082 1999 5374 1012 2429 2000 2198 3536 1996 2082 21319 2032 2013 2037 3200 2043 2002 10865 2055 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] ( cnn ) the mistaken us - led coalition bombing of a syrian military position saturday may have happened because the personnel weren ' t wearing military uniforms and didn ' t have standard military weapons , several us military officials told cnn . officials said they now think the personnel bombed may have been syrian military prisoners , according to several us defense officials . that ' s a working theory of how us , british , danish and australian aircraft may have incorrectly assessed intelligence and targeted the site that killed more than 60 syrian personnel near dei ##r e ##zzo ##r in eastern syria . the uk ministry of defence is saying it used drones in the strike . officials emphasized there are [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] ( cnn ) the mistaken us - led coalition bombing of a syrian military position saturday may have happened because the personnel weren ' t wearing military uniforms and didn ' t have standard military weapons , several us military officials told cnn . officials said they now think the personnel bombed may have been syrian military prisoners , according to several us defense officials . that ' s a working theory of how us , british , danish and australian aircraft may have incorrectly assessed intelligence and targeted the site that killed more than 60 syrian personnel near dei ##r e ##zzo ##r in eastern syria . the uk ministry of defence is saying it used drones in the strike . officials emphasized there are [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1006 13229 1007 1996 13534 2149 1011 2419 6056 8647 1997 1037 9042 2510 2597 5095 2089 2031 3047 2138 1996 5073 4694 1005 1056 4147 2510 11408 1998 2134 1005 1056 2031 3115 2510 4255 1010 2195 2149 2510 4584 2409 13229 1012 4584 2056 2027 2085 2228 1996 5073 18897 2089 2031 2042 9042 2510 5895 1010 2429 2000 2195 2149 3639 4584 1012 2008 1005 1055 1037 2551 3399 1997 2129 2149 1010 2329 1010 5695 1998 2827 2948 2089 2031 19721 14155 4454 1998 9416 1996 2609 2008 2730 2062 2084 3438 9042 5073 2379 14866 2099 1041 12036 2099 1999 2789 7795 1012 1996 2866 3757 1997 4721 2003 3038 2009 2109 24633 1999 1996 4894 1012 4584 13155 2045 2024 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1006 13229 1007 1996 13534 2149 1011 2419 6056 8647 1997 1037 9042 2510 2597 5095 2089 2031 3047 2138 1996 5073 4694 1005 1056 4147 2510 11408 1998 2134 1005 1056 2031 3115 2510 4255 1010 2195 2149 2510 4584 2409 13229 1012 4584 2056 2027 2085 2228 1996 5073 18897 2089 2031 2042 9042 2510 5895 1010 2429 2000 2195 2149 3639 4584 1012 2008 1005 1055 1037 2551 3399 1997 2129 2149 1010 2329 1010 5695 1998 2827 2948 2089 2031 19721 14155 4454 1998 9416 1996 2609 2008 2730 2062 2084 3438 9042 5073 2379 14866 2099 1041 12036 2099 1999 2789 7795 1012 1996 2866 3757 1997 4721 2003 3038 2009 2109 24633 1999 1996 4894 1012 4584 13155 2045 2024 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_Ftx1toYkFc"
      },
      "source": [
        "features =  sp.csr_matrix(feature_np, dtype=float).tolil()\n",
        "adj =  pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/DL_final/news_news_bf_adjacency_matrix.csv\", header=None).values\n",
        "y_train, y_val, y_test, train_mask, val_mask, test_mask =  getYs()\n",
        "\n",
        "features=sp.csr_matrix(features)\n",
        "\n",
        "features, sparse_matrix = preprocess_features(features)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-SPbqDjfFBj"
      },
      "source": [
        "y_train  = np.asarray(y_train)\n",
        "y_val = np.asarray(y_val)\n",
        "y_test = np.asarray(y_test)\n",
        "train_mask  = np.asarray(train_mask)\n",
        "val_mask  = np.asarray(val_mask)\n",
        "test_mask  = np.asarray(test_mask)\n",
        "label = np.asarray(label)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjkY5SFgjzcp",
        "outputId": "f151144a-c015-4a46-9f00-4f8b7f342262"
      },
      "source": [
        "# np.isnan(y_train.data).any()\n",
        "# np.isnan(y_val.data).any()\n",
        "np.isnan(y_test.data).any()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXS2-Ig7j1xf",
        "outputId": "1ec79d9f-7012-4efd-e1fc-9a31c9a82b51"
      },
      "source": [
        "train_mask.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(182,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua1lEpZtXPGi"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import time\n",
        "import tensorflow as tf \n",
        "from spektral.layers import GCNConv\n",
        "# from  utils import *\n",
        "# from models import GCN, MLP\n",
        "\n",
        "# Set random seed\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "epochs = 200\n",
        "dropout = 0.2\n",
        "weight_decay = 5e-4\n",
        "early_stopping = 10\n",
        "learning_rate = 1e-2\n",
        "num_nodes = features.shape[0]\n",
        "# num_nodes = adj.shape[0]\n",
        "l2_reg = 5e-4 / 2\n",
        "num_classes = 2\n",
        "# 'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label = label_encoder.fit_transform(label)\n",
        "label = tf.keras.utils.to_categorical(label)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7icSaxpqSpF",
        "outputId": "a25423e8-4364-4c5a-cfa2-f03ada03fd04"
      },
      "source": [
        "features.shape[1]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "129"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOeuJmf8r1Pu"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCs0MBEQm-ZE",
        "outputId": "2fb5b1b6-124a-4ec6-929f-12c090412058"
      },
      "source": [
        "adj_matrix = GCNConv.preprocess(adj).astype('f4')\n",
        "x_inputs = Input(shape = features.shape[1])\n",
        "a_inputs = Input((num_nodes,), sparse=True, dtype=tf.float32)\n",
        "do_1 = Dropout(dropout)(x_inputs)\n",
        "gc_1 = GCNConv(8,\n",
        "               activation='relu',\n",
        "               kernel_regularizer=l2(l2_reg),\n",
        "               use_bias=False)([do_1, a_inputs])\n",
        "# do_2 = Dropout(dropout)(gc_1)\n",
        "# gc_2 = GCNConv(8,\n",
        "#                activation='relu',\n",
        "#                kernel_regularizer=l2(l2_reg),\n",
        "#                use_bias=False)([do_2, a_inputs])\n",
        "\n",
        "output =  Dense(units=num_classes, activation='softmax')(gc_1)\n",
        "model = Model(inputs=[x_inputs, a_inputs], outputs=output)\n",
        "optimizer = Adam(lr=learning_rate)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              weighted_metrics=['acc'])\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 129)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 129)          0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            [(None, 182)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "gcn_conv_6 (GCNConv)            (None, 8)            1032        dropout_6[0][0]                  \n",
            "                                                                 input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 2)            18          gcn_conv_6[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 1,050\n",
            "Trainable params: 1,050\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VccjWJRLsZh5"
      },
      "source": [
        "tbCallBack_GCN = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir='./Tensorboard_GCN_cora',\n",
        ")\n",
        "callback_GCN = [tbCallBack_GCN]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu1k0tsejv9a",
        "outputId": "fe3db0f6-fda7-4ad2-ac4e-49dd8a073014"
      },
      "source": [
        "print('adj shape', type(adj))\n",
        "\n",
        "print('features shape', type(features))\n",
        "print('y_train shape', type(y_train))\n",
        "\n",
        "print('y_val shape', type(y_val))\n",
        "print('y_test shape', type(y_test))\n",
        "print('train_mask shape', type(train_mask))\n",
        "print('val_mask shape', type(val_mask))\n",
        "print('test_mask shape', type(test_mask)) \n",
        "print('label', type(label))\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adj shape <class 'numpy.ndarray'>\n",
            "features shape <class 'numpy.matrix'>\n",
            "y_train shape <class 'numpy.ndarray'>\n",
            "y_val shape <class 'numpy.ndarray'>\n",
            "y_test shape <class 'numpy.ndarray'>\n",
            "train_mask shape <class 'numpy.ndarray'>\n",
            "val_mask shape <class 'numpy.ndarray'>\n",
            "test_mask shape <class 'numpy.ndarray'>\n",
            "label <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfpR8YaKAZyP",
        "outputId": "aa051cd5-3a2e-4302-8bb4-8efc99a8c2ce"
      },
      "source": [
        "print(np.isnan(y_train).any())\n",
        "print(np.isnan(y_val).any())\n",
        "print(np.isnan(y_test).any())\n",
        "print(np.isnan(train_mask).any())\n",
        "print(np.isnan(val_mask).any())\n",
        "print(np.isnan(test_mask).any())\n",
        "print(np.isnan(label).any())\n",
        "print(np.isnan(features).any())\n",
        "print(np.isnan(adj_matrix).any())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC5lbhtM8FAi",
        "outputId": "75ef2be4-6693-4332-e023-f9ebc384515d"
      },
      "source": [
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_val.shape)\n",
        "\n",
        "\n",
        "print(label.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(182, 2)\n",
            "(182, 2)\n",
            "(182, 2)\n",
            "(182, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v-rtV7Gso6J",
        "outputId": "56337d1f-f9ee-4976-b5e3-49666947fe7f"
      },
      "source": [
        "# Train model\n",
        "validation_data = ([features, adj_matrix], label, val_mask)\n",
        "model.fit([features, adj_matrix],\n",
        "          label,\n",
        "          sample_weight=train_mask,\n",
        "          epochs=epochs,\n",
        "          batch_size=num_nodes,\n",
        "          validation_data=validation_data,\n",
        "          shuffle=False,\n",
        "          )\n",
        "\n",
        "\n",
        "# callbacks=[\n",
        "#               EarlyStopping(patience=early_stopping,  restore_best_weights=True),\n",
        "#               tbCallBack_GCN\n",
        "#           ]\n",
        "# history = model.fit(\n",
        "#     features, \n",
        "#     y_train, \n",
        "#     sample_weight=tf.cast(train_mask, tf.float32), # This will be used in loss calculations\n",
        "#     validation_data=(features, y_val, val_mask),\n",
        "#     epochs=epochs, \n",
        "#     batch_size=num_nodes, # This is unusual in ML - since our adjacency matrix is the whole graph, we want to feed in the whole node_state array in each training step\n",
        "#     verbose=1,\n",
        "#     shuffle=False # Do not shuffle the order of our input data, since its order matches up to the adjacency matrix\n",
        "# )\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 0s 144ms/step - loss: nan - acc: 0.4771 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 27ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 27ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 27ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: nan - acc: 0.5229 - val_loss: nan - val_acc: 0.5278\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc01fac50b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mrDJeVF6ADI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}