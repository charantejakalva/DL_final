{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fake_news_detection2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzncODGgVu4a",
        "outputId": "eab27ad3-adcc-491f-8737-4012b96274d3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95rM3srFhFjF",
        "outputId": "b31ab14f-6fc6-4a58-9600-5faf41f562a0"
      },
      "source": [
        "!pip install --upgrade tensorflow-hub"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlJgORzLmuLO",
        "outputId": "796cb485-8445-4bf6-a6a4-ca0450855462"
      },
      "source": [
        "!pip install spektral"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spektral\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/b2/7b51a6b2719a085fa70ab5ef523d0ca6a68bf1751c02d3487c8e0e6c11a3/spektral-1.0.2-py3-none-any.whl (111kB)\n",
            "\r\u001b[K     |███                             | 10kB 15.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 20kB 20.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 30kB 16.7MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 40kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 51kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 61kB 10.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 71kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 81kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from spektral) (2.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from spektral) (0.17.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from spektral) (1.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from spektral) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from spektral) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from spektral) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from spektral) (2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from spektral) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from spektral) (4.41.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from spektral) (4.2.6)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (3.12.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.33.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.12.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (2.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (0.3.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (2.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (0.35.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->spektral) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->spektral) (2.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->spektral) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->spektral) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->spektral) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->spektral) (2.10)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->spektral) (4.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow>=2.1.0->spektral) (50.3.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (3.3.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (3.4.0)\n",
            "Installing collected packages: spektral\n",
            "Successfully installed spektral-1.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waVF_0MNW-dv"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import scipy.sparse as sp\n",
        "import pandas as pd \n",
        "import random\n",
        "import json\n",
        "import os\n",
        "\n",
        "# from feature_matrix import FeatureMatrix\n",
        "\n",
        "import pandas as pd \n",
        "import tensorflow_hub as hub\n",
        "from bert import run_classifier\n",
        "from bert import tokenization\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba_5jRy6XBxP"
      },
      "source": [
        "##Code referenced from: \n",
        "##Website Title: BERT in Keras with Tensorflow hub\n",
        "##URL : https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\")\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                              tokenization_info[\"do_lower_case\"]])\n",
        "\n",
        "  return tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZgYEexvaC0u"
      },
      "source": [
        "    def getFeatures( dataset=\"BuzzFeed\"):\n",
        "        feature_df =  FM.get_feature_matrix(dataset)\n",
        "        label = feature_df['label'].tolist()\n",
        "        label_comp = [0 if each else 1 for each in label]\n",
        "        global label_zip\n",
        "        label_zip = list(zip(label_comp, label))\n",
        "        feature_df.drop(['label'], axis=1)\n",
        "        feature_np = feature_df.values\n",
        "\n",
        "        return sp.csr_matrix(feature_np, dtype=float).tolil()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZakw4OzXF0Q"
      },
      "source": [
        "def getYs():\n",
        "\n",
        "  random.seed(1)\n",
        "  yTrain =  label_zip[:]\n",
        "  yVal =  label_zip[:]\n",
        "  yTest =  label_zip[:]\n",
        "  train_mask = [False] * len(yTrain)\n",
        "  val_mask = [False] * len(yTrain)\n",
        "  test_mask = [False] * len(yTrain)\n",
        "  n = len(yTrain)\n",
        "\n",
        "  set_of_records_range = set(range(n))\n",
        "\n",
        "  train_range = set(random.sample(set_of_records_range, k=int(n * 0.6)))\n",
        "  set_of_records_range = set_of_records_range - train_range\n",
        "\n",
        "  val_range = set(random.sample(set_of_records_range, k=int(n * 0.2)))\n",
        "  set_of_records_range = set_of_records_range - train_range\n",
        "\n",
        "  test_range = set(random.sample(set_of_records_range, k=int(n * 0.2)))\n",
        "\n",
        "  for i in train_range:\n",
        "      yVal[i] = (0,0)\n",
        "      yTest[i] = (0,0)\n",
        "      train_mask[i] = True\n",
        "  for i in val_range:\n",
        "      yTrain[i] = (0,0)\n",
        "      yTest[i] = (0,0)\n",
        "      val_mask[i] = True\n",
        "  for i in test_range:\n",
        "      yVal[i] = (0,0)\n",
        "      yTrain[i] = (0,0)\n",
        "      test_mask[i] = True\n",
        "\n",
        "  return yTrain, yVal, yTest, train_mask, val_mask, test_mask\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_p_V2wJ_mcp"
      },
      "source": [
        "def sparse_to_tuple(sparse_mx):\n",
        "  \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "  def to_tuple(mx):\n",
        "    if not sp.isspmatrix_coo(mx):\n",
        "        mx = mx.tocoo()\n",
        "    coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "    values = mx.data\n",
        "    shape = mx.shape\n",
        "    return coords, values, shape\n",
        "\n",
        "  if isinstance(sparse_mx, list):\n",
        "    for i in range(len(sparse_mx)):\n",
        "        sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "  else:\n",
        "    sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "  return sparse_mx"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy2gbWs_f1vI"
      },
      "source": [
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1, dtype=float).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return features.todense(), sparse_to_tuple(features)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8olrwshX3qR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5909d16-f1a8-448c-8e94-df4571982b2f"
      },
      "source": [
        "base_path = \"/content/drive/MyDrive/Colab_Notebooks/DL_final/\"\n",
        "folder = \"BuzzFeed\"\n",
        "data_list = []\n",
        "for subfolder in [\"FakeNewsContent\", \"RealNewsContent\"]:\n",
        "  print(\"Getting data from subfolder: \", subfolder)\n",
        "  file_array = [f for f in os.listdir(base_path + folder + \"/\" + subfolder) if f.endswith('.json')]\n",
        "  file_array.sort() # file is sorted list\n",
        "  file_array = [os.path.join(base_path + folder + \"/\" + subfolder, name) for name in file_array]\n",
        "\n",
        "  for file in file_array:\n",
        "    # print(\"file: \", file)\n",
        "    # print(\"path: \",base_path + folder + \"/\" + subfolder)\n",
        "    with open(file, 'r') as json_file:\n",
        "      data = json.load(json_file)\n",
        "      if file.split(\"/\")[-2] == \"FakeNewsContent\":\n",
        "          data_list.append([data['text'], 1])\n",
        "      else:\n",
        "          data_list.append([data['text'], 0])\n",
        "\n",
        "print(\"Creating data frame\")\n",
        "data_frame = pd.DataFrame(data_list, columns=[\"text\", \"label\"])\n",
        "data_frame = data_frame.sample(frac=1)\n",
        "\n",
        "input = data_frame.apply(lambda x: run_classifier.InputExample(guid=None, \n",
        "                                                                       text_a=x['text'], text_b=None, label=x['label']), axis=1)\n",
        "print(data_frame)\n",
        "print(\"extracting features\")\n",
        "tokenizer = create_tokenizer_from_hub_module()\n",
        "features = run_classifier.convert_examples_to_features(input, [0, 1], 128, tokenizer)\n",
        "\n",
        "train_features_list = []\n",
        "for item in features:\n",
        "  temp = item.input_ids\n",
        "  temp.append(item.label_id)\n",
        "  train_features_list.append(temp)\n",
        "column_names = [\"feature\" + str(i) for i in range(128)]\n",
        "column_names.append(\"label\")\n",
        "features_frame = pd.DataFrame(train_features_list, columns=column_names)  ##extracted features data frame\n",
        "# /content/drive/MyDrive/Colab_Notebooks/DL_final/BuzzFeed/FakeNewsContent/BuzzFeed_Fake_3-Webpage.json\n",
        "\n",
        "label = features_frame['label'].tolist()\n",
        "label_comp = [0 if each else 1 for each in label]\n",
        "global label_zip\n",
        "label_zip = list(zip(label_comp, label))\n",
        "features_frame.drop(['label'], axis=1)\n",
        "feature_np = features_frame.values"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting data from subfolder:  FakeNewsContent\n",
            "Getting data from subfolder:  RealNewsContent\n",
            "Creating data frame\n",
            "                                                  text  label\n",
            "98   Peaceful protesters crowded Charlotte's first ...      0\n",
            "141  Kellyanne Conway praised Donald Trump for show...      0\n",
            "156  Under the Radar Blog Archives Select Date… Jul...      0\n",
            "92   Less than a day after protests over the police...      0\n",
            "20   A Daisy of a Rerun\\n\\nI am of an age which inc...      1\n",
            "..                                                 ...    ...\n",
            "114  Obama wears hat, breaking ‘Politics 101’ rule ...      0\n",
            "128  7.5k SHARES Facebook Twitter\\n\\nA public high ...      0\n",
            "56   71k SHARES SHARE THIS STORY\\n\\nNew Jersey’s de...      1\n",
            "17   What would possess anyone to take control of t...      1\n",
            "19   In every state there is a law governing the me...      1\n",
            "\n",
            "[182 rows x 2 columns]\n",
            "extracting features\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 182\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 182\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] peaceful protesters crowded charlotte ' s first city council meeting since the cop shooting of keith lam ##ont scott , with over 50 people speaking out against police violence . but none of them stood out monday quite as much as a young girl . z ##ian ##na ol ##ip ##han ##t , her hair done up in braid ##s and tears streak ##ing her face , brought into focus the stress police shooting ##s place on children , as well as the pain the black community of charlotte has suffered in the wake of scott ' s death last week . “ i ’ ve been born and raised in charlotte . and i never felt this way until now and i can ’ t [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] peaceful protesters crowded charlotte ' s first city council meeting since the cop shooting of keith lam ##ont scott , with over 50 people speaking out against police violence . but none of them stood out monday quite as much as a young girl . z ##ian ##na ol ##ip ##han ##t , her hair done up in braid ##s and tears streak ##ing her face , brought into focus the stress police shooting ##s place on children , as well as the pain the black community of charlotte has suffered in the wake of scott ' s death last week . “ i ’ ve been born and raised in charlotte . and i never felt this way until now and i can ’ t [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 9379 13337 10789 5904 1005 1055 2034 2103 2473 3116 2144 1996 8872 5008 1997 6766 16983 12162 3660 1010 2007 2058 2753 2111 4092 2041 2114 2610 4808 1012 2021 3904 1997 2068 2768 2041 6928 3243 2004 2172 2004 1037 2402 2611 1012 1062 2937 2532 19330 11514 4819 2102 1010 2014 2606 2589 2039 1999 24148 2015 1998 4000 9039 2075 2014 2227 1010 2716 2046 3579 1996 6911 2610 5008 2015 2173 2006 2336 1010 2004 2092 2004 1996 3255 1996 2304 2451 1997 5904 2038 4265 1999 1996 5256 1997 3660 1005 1055 2331 2197 2733 1012 1523 1045 1521 2310 2042 2141 1998 2992 1999 5904 1012 1998 1045 2196 2371 2023 2126 2127 2085 1998 1045 2064 1521 1056 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 9379 13337 10789 5904 1005 1055 2034 2103 2473 3116 2144 1996 8872 5008 1997 6766 16983 12162 3660 1010 2007 2058 2753 2111 4092 2041 2114 2610 4808 1012 2021 3904 1997 2068 2768 2041 6928 3243 2004 2172 2004 1037 2402 2611 1012 1062 2937 2532 19330 11514 4819 2102 1010 2014 2606 2589 2039 1999 24148 2015 1998 4000 9039 2075 2014 2227 1010 2716 2046 3579 1996 6911 2610 5008 2015 2173 2006 2336 1010 2004 2092 2004 1996 3255 1996 2304 2451 1997 5904 2038 4265 1999 1996 5256 1997 3660 1005 1055 2331 2197 2733 1012 1523 1045 1521 2310 2042 2141 1998 2992 1999 5904 1012 1998 1045 2196 2371 2023 2126 2127 2085 1998 1045 2064 1521 1056 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] kelly ##anne conway praised donald trump for showing restraint at the debate . | ap photo trump camp : ‘ donald trump is guilty of answering the question asked ’ donald trump ’ s campaign manager kelly ##anne conway praised her candidate for showing restraint at the tail end of monday night ’ s debate when hillary clinton attacked his history of di ##sr ##es ##pe ##ct ##ful and der ##oga ##tory remarks about women . “ i have to say , certainly as a woman , i appreciated the restraint at the end . i am not sure i would have been able to exercise it myself , but restraint is a virtue and it ’ s a presidential virtue , ” conway told ms [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] kelly ##anne conway praised donald trump for showing restraint at the debate . | ap photo trump camp : ‘ donald trump is guilty of answering the question asked ’ donald trump ’ s campaign manager kelly ##anne conway praised her candidate for showing restraint at the tail end of monday night ’ s debate when hillary clinton attacked his history of di ##sr ##es ##pe ##ct ##ful and der ##oga ##tory remarks about women . “ i have to say , certainly as a woman , i appreciated the restraint at the end . i am not sure i would have been able to exercise it myself , but restraint is a virtue and it ’ s a presidential virtue , ” conway told ms [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 5163 20147 14783 5868 6221 8398 2005 4760 19355 2012 1996 5981 1012 1064 9706 6302 8398 3409 1024 1520 6221 8398 2003 5905 1997 10739 1996 3160 2356 1521 6221 8398 1521 1055 3049 3208 5163 20147 14783 5868 2014 4018 2005 4760 19355 2012 1996 5725 2203 1997 6928 2305 1521 1055 5981 2043 18520 7207 4457 2010 2381 1997 4487 21338 2229 5051 6593 3993 1998 4315 18170 7062 12629 2055 2308 1012 1523 1045 2031 2000 2360 1010 5121 2004 1037 2450 1010 1045 12315 1996 19355 2012 1996 2203 1012 1045 2572 2025 2469 1045 2052 2031 2042 2583 2000 6912 2009 2870 1010 2021 19355 2003 1037 11870 1998 2009 1521 1055 1037 4883 11870 1010 1524 14783 2409 5796 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 5163 20147 14783 5868 6221 8398 2005 4760 19355 2012 1996 5981 1012 1064 9706 6302 8398 3409 1024 1520 6221 8398 2003 5905 1997 10739 1996 3160 2356 1521 6221 8398 1521 1055 3049 3208 5163 20147 14783 5868 2014 4018 2005 4760 19355 2012 1996 5725 2203 1997 6928 2305 1521 1055 5981 2043 18520 7207 4457 2010 2381 1997 4487 21338 2229 5051 6593 3993 1998 4315 18170 7062 12629 2055 2308 1012 1523 1045 2031 2000 2360 1010 5121 2004 1037 2450 1010 1045 12315 1996 19355 2012 1996 2203 1012 1045 2572 2025 2469 1045 2052 2031 2042 2583 2000 6912 2009 2870 1010 2021 19355 2003 1037 11870 1998 2009 1521 1055 1037 4883 11870 1010 1524 14783 2409 5796 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] under the radar blog archives select date … july , 2017 june , 2017 may , 2017 april , 2017 march , 2017 february , 2017 january , 2017 december , 2016 november , 2016 october , 2016 september , 2016 august , 2016 ahmad khan ra ##ham ##i is taken into custody after a shootout with police monday in linden , n . j . | nicola ##us c ##zar ##neck ##i / boston herald via ap u . s . attorney sees no rush to ar ##rai ##gn ra ##ham ##i in federal court federal prosecutors are pushing back against a demand by defense lawyers that new york and new jersey bombing suspect ahmad ra ##ham ##i be ar ##rai ##gned immediately on federal [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] under the radar blog archives select date … july , 2017 june , 2017 may , 2017 april , 2017 march , 2017 february , 2017 january , 2017 december , 2016 november , 2016 october , 2016 september , 2016 august , 2016 ahmad khan ra ##ham ##i is taken into custody after a shootout with police monday in linden , n . j . | nicola ##us c ##zar ##neck ##i / boston herald via ap u . s . attorney sees no rush to ar ##rai ##gn ra ##ham ##i in federal court federal prosecutors are pushing back against a demand by defense lawyers that new york and new jersey bombing suspect ahmad ra ##ham ##i be ar ##rai ##gned immediately on federal [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2104 1996 7217 9927 8264 7276 3058 1529 2251 1010 2418 2238 1010 2418 2089 1010 2418 2258 1010 2418 2233 1010 2418 2337 1010 2418 2254 1010 2418 2285 1010 2355 2281 1010 2355 2255 1010 2355 2244 1010 2355 2257 1010 2355 10781 4967 10958 3511 2072 2003 2579 2046 9968 2044 1037 18297 2007 2610 6928 1999 22066 1010 1050 1012 1046 1012 1064 17388 2271 1039 9057 18278 2072 1013 3731 9536 3081 9706 1057 1012 1055 1012 4905 5927 2053 5481 2000 12098 14995 16206 10958 3511 2072 1999 2976 2457 2976 19608 2024 6183 2067 2114 1037 5157 2011 3639 9559 2008 2047 2259 1998 2047 3933 8647 8343 10781 10958 3511 2072 2022 12098 14995 19225 3202 2006 2976 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2104 1996 7217 9927 8264 7276 3058 1529 2251 1010 2418 2238 1010 2418 2089 1010 2418 2258 1010 2418 2233 1010 2418 2337 1010 2418 2254 1010 2418 2285 1010 2355 2281 1010 2355 2255 1010 2355 2244 1010 2355 2257 1010 2355 10781 4967 10958 3511 2072 2003 2579 2046 9968 2044 1037 18297 2007 2610 6928 1999 22066 1010 1050 1012 1046 1012 1064 17388 2271 1039 9057 18278 2072 1013 3731 9536 3081 9706 1057 1012 1055 1012 4905 5927 2053 5481 2000 12098 14995 16206 10958 3511 2072 1999 2976 2457 2976 19608 2024 6183 2067 2114 1037 5157 2011 3639 9559 2008 2047 2259 1998 2047 3933 8647 8343 10781 10958 3511 2072 2022 12098 14995 19225 3202 2006 2976 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] less than a day after protests over the police killing of an african - american man turned violent in north carolina , republican presidential nominee donald trump placed some of the blame for the turmoil on the impact of drugs . \" if you ' re not aware , drugs are a very , very big factor in what you ' re watching on television at night , \" said trump in a speech to the shale insight 2016 conference in pittsburgh today . on wednesday night , charlotte experienced its second night of protests after the death of keith lam ##ont scott on tuesday afternoon . officers in riot gear confronted demonstrators in a downtown commercial area and employed tear gas to control crowds . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] less than a day after protests over the police killing of an african - american man turned violent in north carolina , republican presidential nominee donald trump placed some of the blame for the turmoil on the impact of drugs . \" if you ' re not aware , drugs are a very , very big factor in what you ' re watching on television at night , \" said trump in a speech to the shale insight 2016 conference in pittsburgh today . on wednesday night , charlotte experienced its second night of protests after the death of keith lam ##ont scott on tuesday afternoon . officers in riot gear confronted demonstrators in a downtown commercial area and employed tear gas to control crowds . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2625 2084 1037 2154 2044 8090 2058 1996 2610 4288 1997 2019 3060 1011 2137 2158 2357 6355 1999 2167 3792 1010 3951 4883 9773 6221 8398 2872 2070 1997 1996 7499 2005 1996 17930 2006 1996 4254 1997 5850 1012 1000 2065 2017 1005 2128 2025 5204 1010 5850 2024 1037 2200 1010 2200 2502 5387 1999 2054 2017 1005 2128 3666 2006 2547 2012 2305 1010 1000 2056 8398 1999 1037 4613 2000 1996 18488 12369 2355 3034 1999 6278 2651 1012 2006 9317 2305 1010 5904 5281 2049 2117 2305 1997 8090 2044 1996 2331 1997 6766 16983 12162 3660 2006 9857 5027 1012 3738 1999 11421 6718 12892 28337 1999 1037 5116 3293 2181 1998 4846 7697 3806 2000 2491 12783 1012 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2625 2084 1037 2154 2044 8090 2058 1996 2610 4288 1997 2019 3060 1011 2137 2158 2357 6355 1999 2167 3792 1010 3951 4883 9773 6221 8398 2872 2070 1997 1996 7499 2005 1996 17930 2006 1996 4254 1997 5850 1012 1000 2065 2017 1005 2128 2025 5204 1010 5850 2024 1037 2200 1010 2200 2502 5387 1999 2054 2017 1005 2128 3666 2006 2547 2012 2305 1010 1000 2056 8398 1999 1037 4613 2000 1996 18488 12369 2355 3034 1999 6278 2651 1012 2006 9317 2305 1010 5904 5281 2049 2117 2305 1997 8090 2044 1996 2331 1997 6766 16983 12162 3660 2006 9857 5027 1012 3738 1999 11421 6718 12892 28337 1999 1037 5116 3293 2181 1998 4846 7697 3806 2000 2491 12783 1012 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] a daisy of a re ##run i am of an age which includes probably the first major political distortion in presidential campaigning . that being the tv advertisement of an innocent little girl , sitting in a field picking daisy petals one by one , while a mushroom cloud arises in a most sinister fashion behind her . the year was 1964 and the incumbent president lyndon b johnson “ approved ” that message . only problem was that he followed the course in vietnam which he was trying to assign onto his supposedly “ war mon ##ger ##ing ” opponent , barry gold ##water . today , as our history books find difficulty in rev ##isi ##ting johnson ’ s dec ##eit ##ful war policies [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] a daisy of a re ##run i am of an age which includes probably the first major political distortion in presidential campaigning . that being the tv advertisement of an innocent little girl , sitting in a field picking daisy petals one by one , while a mushroom cloud arises in a most sinister fashion behind her . the year was 1964 and the incumbent president lyndon b johnson “ approved ” that message . only problem was that he followed the course in vietnam which he was trying to assign onto his supposedly “ war mon ##ger ##ing ” opponent , barry gold ##water . today , as our history books find difficulty in rev ##isi ##ting johnson ’ s dec ##eit ##ful war policies [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1037 10409 1997 1037 2128 15532 1045 2572 1997 2019 2287 2029 2950 2763 1996 2034 2350 2576 20870 1999 4883 18524 1012 2008 2108 1996 2694 15147 1997 2019 7036 2210 2611 1010 3564 1999 1037 2492 8130 10409 15829 2028 2011 2028 1010 2096 1037 18565 6112 18653 1999 1037 2087 16491 4827 2369 2014 1012 1996 2095 2001 3546 1998 1996 7703 2343 23037 1038 3779 1523 4844 1524 2008 4471 1012 2069 3291 2001 2008 2002 2628 1996 2607 1999 5148 2029 2002 2001 2667 2000 23911 3031 2010 10743 1523 2162 12256 4590 2075 1524 7116 1010 6287 2751 5880 1012 2651 1010 2004 2256 2381 2808 2424 7669 1999 7065 17417 3436 3779 1521 1055 11703 20175 3993 2162 6043 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1037 10409 1997 1037 2128 15532 1045 2572 1997 2019 2287 2029 2950 2763 1996 2034 2350 2576 20870 1999 4883 18524 1012 2008 2108 1996 2694 15147 1997 2019 7036 2210 2611 1010 3564 1999 1037 2492 8130 10409 15829 2028 2011 2028 1010 2096 1037 18565 6112 18653 1999 1037 2087 16491 4827 2369 2014 1012 1996 2095 2001 3546 1998 1996 7703 2343 23037 1038 3779 1523 4844 1524 2008 4471 1012 2069 3291 2001 2008 2002 2628 1996 2607 1999 5148 2029 2002 2001 2667 2000 23911 3031 2010 10743 1523 2162 12256 4590 2075 1524 7116 1010 6287 2751 5880 1012 2651 1010 2004 2256 2381 2808 2424 7669 1999 7065 17417 3436 3779 1521 1055 11703 20175 3993 2162 6043 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_Ftx1toYkFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91ccc501-e9ed-40ab-8cb0-1b618f617aa2"
      },
      "source": [
        "features =  sp.csr_matrix(feature_np, dtype=float).tolil()\n",
        "adj =  pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/DL_final/news_news_bf_adjacency_matrix.csv\", header=None).values\n",
        "y_train, y_val, y_test, train_mask, val_mask, test_mask =  getYs()\n",
        "\n",
        "features=sp.csr_matrix(features)\n",
        "\n",
        "print(features)\n",
        "features, sparse_matrix = preprocess_features(features)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label = label_encoder.fit_transform(label)\n",
        "label = tf.keras.utils.to_categorical(label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 0)\t101.0\n",
            "  (0, 1)\t9379.0\n",
            "  (0, 2)\t13337.0\n",
            "  (0, 3)\t10789.0\n",
            "  (0, 4)\t5904.0\n",
            "  (0, 5)\t1005.0\n",
            "  (0, 6)\t1055.0\n",
            "  (0, 7)\t2034.0\n",
            "  (0, 8)\t2103.0\n",
            "  (0, 9)\t2473.0\n",
            "  (0, 10)\t3116.0\n",
            "  (0, 11)\t2144.0\n",
            "  (0, 12)\t1996.0\n",
            "  (0, 13)\t8872.0\n",
            "  (0, 14)\t5008.0\n",
            "  (0, 15)\t1997.0\n",
            "  (0, 16)\t6766.0\n",
            "  (0, 17)\t16983.0\n",
            "  (0, 18)\t12162.0\n",
            "  (0, 19)\t3660.0\n",
            "  (0, 20)\t1010.0\n",
            "  (0, 21)\t2007.0\n",
            "  (0, 22)\t2058.0\n",
            "  (0, 23)\t2753.0\n",
            "  (0, 24)\t2111.0\n",
            "  :\t:\n",
            "  (181, 104)\t1999.0\n",
            "  (181, 105)\t1996.0\n",
            "  (181, 106)\t2142.0\n",
            "  (181, 107)\t2163.0\n",
            "  (181, 108)\t1010.0\n",
            "  (181, 109)\t1998.0\n",
            "  (181, 110)\t2097.0\n",
            "  (181, 111)\t2644.0\n",
            "  (181, 112)\t2012.0\n",
            "  (181, 113)\t2498.0\n",
            "  (181, 114)\t1010.0\n",
            "  (181, 115)\t2164.0\n",
            "  (181, 116)\t14303.0\n",
            "  (181, 117)\t9861.0\n",
            "  (181, 118)\t1010.0\n",
            "  (181, 119)\t10428.0\n",
            "  (181, 120)\t28652.0\n",
            "  (181, 121)\t1010.0\n",
            "  (181, 122)\t1998.0\n",
            "  (181, 123)\t2151.0\n",
            "  (181, 124)\t2060.0\n",
            "  (181, 125)\t6206.0\n",
            "  (181, 126)\t2965.0\n",
            "  (181, 127)\t102.0\n",
            "  (181, 128)\t1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUvS3KfsJ17x",
        "outputId": "cdbec8f0-39c1-4e9b-a937-fb9ba8fb7a49"
      },
      "source": [
        "print(label)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-SPbqDjfFBj"
      },
      "source": [
        "y_train  = np.asarray(y_train)\n",
        "y_val = np.asarray(y_val)\n",
        "y_test = np.asarray(y_test)\n",
        "train_mask  = np.asarray(train_mask)\n",
        "val_mask  = np.asarray(val_mask)\n",
        "test_mask  = np.asarray(test_mask)\n",
        "label = np.asarray(label)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjkY5SFgjzcp",
        "outputId": "c58bdc9f-737f-4eb4-f71b-7cf854194704"
      },
      "source": [
        "# np.isnan(y_train.data).any()\n",
        "# np.isnan(y_val.data).any()\n",
        "np.isnan(y_test.data).any()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXS2-Ig7j1xf",
        "outputId": "32d2704b-1e8c-40a0-f770-a3cf194bbdf2"
      },
      "source": [
        "train_mask.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(182,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua1lEpZtXPGi"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import time\n",
        "import tensorflow as tf \n",
        "from spektral.layers import GCNConv\n",
        "# from  utils import *\n",
        "# from models import GCN, MLP\n",
        "\n",
        "# Set random seed\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "epochs = 200\n",
        "dropout = 0.5\n",
        "weight_decay = 2e-5\n",
        "early_stopping = 10\n",
        "learning_rate = 2e-5\n",
        "num_nodes = features.shape[0]\n",
        "# num_nodes = adj.shape[0]\n",
        "l2_reg = 5e-4 / 2\n",
        "num_classes = 2\n",
        "# 'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7icSaxpqSpF",
        "outputId": "b4310231-2733-48db-b863-557f5e14c27d"
      },
      "source": [
        "features.shape[1]"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "129"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDjnJPaKNkma"
      },
      "source": [
        "##GCN1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOeuJmf8r1Pu"
      },
      "source": [
        "adj_matrix = GCNConv.preprocess(adj).astype('f4')\n",
        "adj_matrix[np.isinf(adj_matrix)] = 1"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCs0MBEQm-ZE",
        "outputId": "3cc2a8a4-72c4-410c-d0b8-b24aa6cd88b8"
      },
      "source": [
        "\n",
        "x_inputs = Input(shape = features.shape[1])\n",
        "a_inputs = Input((num_nodes,), sparse=True, dtype=tf.float32)\n",
        "do_1 = Dropout(dropout)(x_inputs)\n",
        "gc_1 = GCNConv(64,\n",
        "               activation='relu',\n",
        "               kernel_regularizer=l2(l2_reg),\n",
        "               use_bias=False)([do_1, a_inputs])\n",
        "do_2 = Dropout(dropout)(gc_1)\n",
        "gc_2 = GCNConv(64,\n",
        "               activation='relu',\n",
        "               kernel_regularizer=l2(l2_reg),\n",
        "               use_bias=False)([do_2, a_inputs])\n",
        "\n",
        "output =  Dense(units=num_classes, activation='softmax')(gc_2)\n",
        "model = Model(inputs=[x_inputs, a_inputs], outputs=output)\n",
        "optimizer = SGD(lr=learning_rate)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              weighted_metrics=['acc'])\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_15\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_15 (InputLayer)           [(None, 129)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 129)          0           input_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_16 (InputLayer)           [(None, 182)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "gcn_conv_12 (GCNConv)           (None, 64)           8256        dropout_12[0][0]                 \n",
            "                                                                 input_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 64)           0           gcn_conv_12[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "gcn_conv_13 (GCNConv)           (None, 64)           4096        dropout_13[0][0]                 \n",
            "                                                                 input_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 2)            130         gcn_conv_13[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 12,482\n",
            "Trainable params: 12,482\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VccjWJRLsZh5"
      },
      "source": [
        "tbCallBack_GCN = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir='./Tensorboard_GCN_cora',\n",
        ")\n",
        "callback_GCN = [tbCallBack_GCN]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW-QwN7tJ-AG",
        "outputId": "e92f4ffd-f94e-4f42-8f87-e011c40effa0"
      },
      "source": [
        "\n",
        "\n",
        "print(adj_matrix)\n",
        "# x[numpy.isneginf(x)] = 0\n",
        "adj_matrix[np.isinf(adj_matrix)] = 1\n",
        "print(adj_matrix)\n",
        "# print(max(adj_matrix[0]))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.04651163 0.         0.         ... 0.01495371 0.         0.        ]\n",
            " [0.         0.03333334 0.016265   ... 0.         0.         0.        ]\n",
            " [0.         0.016265   0.03174603 ... 0.01235416 0.         0.        ]\n",
            " ...\n",
            " [0.01495371 0.         0.01235416 ... 0.01923077 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         1.         ... 0.         0.         0.        ]]\n",
            "[[0.04651163 0.         0.         ... 0.01495371 0.         0.        ]\n",
            " [0.         0.03333334 0.016265   ... 0.         0.         0.        ]\n",
            " [0.         0.016265   0.03174603 ... 0.01235416 0.         0.        ]\n",
            " ...\n",
            " [0.01495371 0.         0.01235416 ... 0.01923077 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         1.         ... 0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu1k0tsejv9a",
        "outputId": "abd46ffc-8fa8-4d8f-c3cc-b31ecc1eac33"
      },
      "source": [
        "print('adj shape', type(adj))\n",
        "\n",
        "print('features shape', type(features))\n",
        "print('y_train shape', type(y_train))\n",
        "\n",
        "print('y_val shape', type(y_val))\n",
        "print('y_test shape', type(y_test))\n",
        "print('train_mask shape', type(train_mask))\n",
        "print('val_mask shape', type(val_mask))\n",
        "print('test_mask shape', type(test_mask)) \n",
        "print('label', type(label))\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adj shape <class 'numpy.ndarray'>\n",
            "features shape <class 'numpy.matrix'>\n",
            "y_train shape <class 'numpy.ndarray'>\n",
            "y_val shape <class 'numpy.ndarray'>\n",
            "y_test shape <class 'numpy.ndarray'>\n",
            "train_mask shape <class 'numpy.ndarray'>\n",
            "val_mask shape <class 'numpy.ndarray'>\n",
            "test_mask shape <class 'numpy.ndarray'>\n",
            "label <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfpR8YaKAZyP",
        "outputId": "abbdada9-1cc8-4b7a-e1d8-bd8312a13091"
      },
      "source": [
        "print(np.isnan(y_train).any())\n",
        "print(np.isnan(y_val).any())\n",
        "print(np.isnan(y_test).any())\n",
        "print(np.isnan(train_mask).any())\n",
        "print(np.isnan(val_mask).any())\n",
        "print(np.isnan(test_mask).any())\n",
        "print(np.isnan(label).any())\n",
        "print(np.isnan(features).any())\n",
        "print(np.isnan(adj_matrix).any())"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC5lbhtM8FAi",
        "outputId": "3752068e-52f7-4bc7-dce8-5931787a4273"
      },
      "source": [
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_val.shape)\n",
        "\n",
        "\n",
        "print(label.shape)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(182, 2)\n",
            "(182, 2)\n",
            "(182, 2)\n",
            "(182, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v-rtV7Gso6J",
        "outputId": "35d912e4-de34-42b7-ef8c-57a1af0ad59e"
      },
      "source": [
        "# Train model\n",
        "validation_data = ([features, adj_matrix], label, val_mask)\n",
        "model.fit([features, adj_matrix],\n",
        "          label,\n",
        "          sample_weight=train_mask,\n",
        "          epochs=epochs,\n",
        "          batch_size=num_nodes,\n",
        "          validation_data=validation_data,\n",
        "          shuffle=False,\n",
        "          )\n",
        "\n",
        "\n",
        "# callbacks=[\n",
        "#               EarlyStopping(patience=early_stopping,  restore_best_weights=True),\n",
        "#               tbCallBack_GCN\n",
        "#           ]\n",
        "# history = model.fit(\n",
        "#     features, \n",
        "#     y_train, \n",
        "#     sample_weight=tf.cast(train_mask, tf.float32), # This will be used in loss calculations\n",
        "#     validation_data=(features, y_val, val_mask),\n",
        "#     epochs=epochs, \n",
        "#     batch_size=num_nodes, # This is unusual in ML - since our adjacency matrix is the whole graph, we want to feed in the whole node_state array in each training step\n",
        "#     verbose=1,\n",
        "#     shuffle=False # Do not shuffle the order of our input data, since its order matches up to the adjacency matrix\n",
        "# )\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490738778663408543475957760.0000 - val_acc: 0.4722\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490757668129340022056812544.0000 - val_acc: 0.4722\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490757668129340022056812544.0000 - val_acc: 0.4722\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4528 - acc: 0.5138 - val_loss: 169490757668129340022056812544.0000 - val_acc: 0.4722\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490757668129340022056812544.0000 - val_acc: 0.4722\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490757668129340022056812544.0000 - val_acc: 0.4722\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490757668129340022056812544.0000 - val_acc: 0.4722\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490757668129340022056812544.0000 - val_acc: 0.4722\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4525 - acc: 0.5138 - val_loss: 169490757668129340022056812544.0000 - val_acc: 0.4722\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490757668129340022056812544.0000 - val_acc: 0.4722\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490757668129340022056812544.0000 - val_acc: 0.4722\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490757668129340022056812544.0000 - val_acc: 0.4722\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490757668129340022056812544.0000 - val_acc: 0.4722\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490757668129340022056812544.0000 - val_acc: 0.4722\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490757668129340022056812544.0000 - val_acc: 0.4722\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490757668129340022056812544.0000 - val_acc: 0.4722\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490776557595271500637667328.0000 - val_acc: 0.4722\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490776557595271500637667328.0000 - val_acc: 0.4722\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490776557595271500637667328.0000 - val_acc: 0.4722\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490776557595271500637667328.0000 - val_acc: 0.4722\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490776557595271500637667328.0000 - val_acc: 0.4722\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490776557595271500637667328.0000 - val_acc: 0.4722\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490795447061202979218522112.0000 - val_acc: 0.4722\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490795447061202979218522112.0000 - val_acc: 0.4722\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4525 - acc: 0.5138 - val_loss: 169490795447061202979218522112.0000 - val_acc: 0.4722\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490795447061202979218522112.0000 - val_acc: 0.4722\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490795447061202979218522112.0000 - val_acc: 0.4722\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490795447061202979218522112.0000 - val_acc: 0.4722\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490795447061202979218522112.0000 - val_acc: 0.4722\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490795447061202979218522112.0000 - val_acc: 0.4722\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490795447061202979218522112.0000 - val_acc: 0.4722\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490814336527134457799376896.0000 - val_acc: 0.4722\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490814336527134457799376896.0000 - val_acc: 0.4722\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490814336527134457799376896.0000 - val_acc: 0.4722\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490795447061202979218522112.0000 - val_acc: 0.4722\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4526 - acc: 0.5229 - val_loss: 169490814336527134457799376896.0000 - val_acc: 0.4722\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490814336527134457799376896.0000 - val_acc: 0.4722\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490814336527134457799376896.0000 - val_acc: 0.4722\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490814336527134457799376896.0000 - val_acc: 0.4722\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490833225993065936380231680.0000 - val_acc: 0.4722\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490833225993065936380231680.0000 - val_acc: 0.4722\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490833225993065936380231680.0000 - val_acc: 0.4722\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490833225993065936380231680.0000 - val_acc: 0.4722\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490833225993065936380231680.0000 - val_acc: 0.4722\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490833225993065936380231680.0000 - val_acc: 0.4722\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490833225993065936380231680.0000 - val_acc: 0.4722\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4528 - acc: 0.5138 - val_loss: 169490833225993065936380231680.0000 - val_acc: 0.4722\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490833225993065936380231680.0000 - val_acc: 0.4722\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490833225993065936380231680.0000 - val_acc: 0.4722\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490833225993065936380231680.0000 - val_acc: 0.4722\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4525 - acc: 0.5138 - val_loss: 169490833225993065936380231680.0000 - val_acc: 0.4722\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4525 - acc: 0.5138 - val_loss: 169490833225993065936380231680.0000 - val_acc: 0.4722\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490833225993065936380231680.0000 - val_acc: 0.4722\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490871004924928893541941248.0000 - val_acc: 0.4722\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4528 - acc: 0.5138 - val_loss: 169490852115458997414961086464.0000 - val_acc: 0.4722\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490871004924928893541941248.0000 - val_acc: 0.4722\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490871004924928893541941248.0000 - val_acc: 0.4722\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490871004924928893541941248.0000 - val_acc: 0.4722\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490871004924928893541941248.0000 - val_acc: 0.4722\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490871004924928893541941248.0000 - val_acc: 0.4722\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4525 - acc: 0.5229 - val_loss: 169490871004924928893541941248.0000 - val_acc: 0.4722\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490871004924928893541941248.0000 - val_acc: 0.4722\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490871004924928893541941248.0000 - val_acc: 0.4722\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490871004924928893541941248.0000 - val_acc: 0.4722\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490871004924928893541941248.0000 - val_acc: 0.4722\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490871004924928893541941248.0000 - val_acc: 0.4722\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490871004924928893541941248.0000 - val_acc: 0.4722\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490871004924928893541941248.0000 - val_acc: 0.4722\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4528 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4525 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490889894390860372122796032.0000 - val_acc: 0.4722\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490908783856791850703650816.0000 - val_acc: 0.4722\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4525 - acc: 0.5138 - val_loss: 169490927673322723329284505600.0000 - val_acc: 0.4722\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490927673322723329284505600.0000 - val_acc: 0.4722\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490927673322723329284505600.0000 - val_acc: 0.4722\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490927673322723329284505600.0000 - val_acc: 0.4722\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4528 - acc: 0.5138 - val_loss: 169490927673322723329284505600.0000 - val_acc: 0.4722\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490927673322723329284505600.0000 - val_acc: 0.4722\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490927673322723329284505600.0000 - val_acc: 0.4722\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490927673322723329284505600.0000 - val_acc: 0.4722\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490927673322723329284505600.0000 - val_acc: 0.4722\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4526 - acc: 0.5229 - val_loss: 169490927673322723329284505600.0000 - val_acc: 0.4722\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490927673322723329284505600.0000 - val_acc: 0.4722\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490927673322723329284505600.0000 - val_acc: 0.4722\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490927673322723329284505600.0000 - val_acc: 0.4722\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490927673322723329284505600.0000 - val_acc: 0.4722\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4525 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4525 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4525 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4525 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4528 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490946562788654807865360384.0000 - val_acc: 0.4722\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490965452254586286446215168.0000 - val_acc: 0.4722\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490984341720517765027069952.0000 - val_acc: 0.4722\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490984341720517765027069952.0000 - val_acc: 0.4722\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490984341720517765027069952.0000 - val_acc: 0.4722\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4525 - acc: 0.5138 - val_loss: 169490984341720517765027069952.0000 - val_acc: 0.4722\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490984341720517765027069952.0000 - val_acc: 0.4722\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490984341720517765027069952.0000 - val_acc: 0.4722\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169490984341720517765027069952.0000 - val_acc: 0.4722\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169490984341720517765027069952.0000 - val_acc: 0.4722\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4528 - acc: 0.5138 - val_loss: 169491003231186449243607924736.0000 - val_acc: 0.4722\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491003231186449243607924736.0000 - val_acc: 0.4722\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491003231186449243607924736.0000 - val_acc: 0.4722\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491003231186449243607924736.0000 - val_acc: 0.4722\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4528 - acc: 0.5138 - val_loss: 169491003231186449243607924736.0000 - val_acc: 0.4722\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169491003231186449243607924736.0000 - val_acc: 0.4722\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169491003231186449243607924736.0000 - val_acc: 0.4722\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491003231186449243607924736.0000 - val_acc: 0.4722\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169491003231186449243607924736.0000 - val_acc: 0.4722\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491003231186449243607924736.0000 - val_acc: 0.4722\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491003231186449243607924736.0000 - val_acc: 0.4722\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169491003231186449243607924736.0000 - val_acc: 0.4722\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491022120652380722188779520.0000 - val_acc: 0.4722\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491022120652380722188779520.0000 - val_acc: 0.4722\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169491022120652380722188779520.0000 - val_acc: 0.4722\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4525 - acc: 0.5229 - val_loss: 169491022120652380722188779520.0000 - val_acc: 0.4722\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491022120652380722188779520.0000 - val_acc: 0.4722\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491022120652380722188779520.0000 - val_acc: 0.4722\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169491022120652380722188779520.0000 - val_acc: 0.4722\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491022120652380722188779520.0000 - val_acc: 0.4722\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491022120652380722188779520.0000 - val_acc: 0.4722\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491022120652380722188779520.0000 - val_acc: 0.4722\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169491022120652380722188779520.0000 - val_acc: 0.4722\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491022120652380722188779520.0000 - val_acc: 0.4722\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169491022120652380722188779520.0000 - val_acc: 0.4722\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491041010118312200769634304.0000 - val_acc: 0.4722\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169491059899584243679350489088.0000 - val_acc: 0.4722\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491059899584243679350489088.0000 - val_acc: 0.4722\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491059899584243679350489088.0000 - val_acc: 0.4722\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491059899584243679350489088.0000 - val_acc: 0.4722\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491059899584243679350489088.0000 - val_acc: 0.4722\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491059899584243679350489088.0000 - val_acc: 0.4722\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169491059899584243679350489088.0000 - val_acc: 0.4722\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491059899584243679350489088.0000 - val_acc: 0.4722\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491059899584243679350489088.0000 - val_acc: 0.4722\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169491059899584243679350489088.0000 - val_acc: 0.4722\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4525 - acc: 0.5138 - val_loss: 169491078789050175157931343872.0000 - val_acc: 0.4722\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169491078789050175157931343872.0000 - val_acc: 0.4722\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169491078789050175157931343872.0000 - val_acc: 0.4722\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169491078789050175157931343872.0000 - val_acc: 0.4722\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169491078789050175157931343872.0000 - val_acc: 0.4722\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169491078789050175157931343872.0000 - val_acc: 0.4722\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491078789050175157931343872.0000 - val_acc: 0.4722\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491078789050175157931343872.0000 - val_acc: 0.4722\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491078789050175157931343872.0000 - val_acc: 0.4722\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491078789050175157931343872.0000 - val_acc: 0.4722\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4527 - acc: 0.5138 - val_loss: 169491078789050175157931343872.0000 - val_acc: 0.4722\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.5138 - val_loss: 169491078789050175157931343872.0000 - val_acc: 0.4722\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1934e163c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mrDJeVF6ADI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MB7RsKeNpaI"
      },
      "source": [
        "##GCN2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBmyucfzNq1s"
      },
      "source": [
        "from  utils import *\n",
        "from models import GCN, MLP\n",
        "\n",
        "flags = tf.compat.v1.flags\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_string('f', '', 'kernel')\n",
        "# flags.DEFINE_string('dataset', 'cora', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
        "# flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
        "# flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
        "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
        "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
        "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPQs342zQFex",
        "outputId": "fb16798f-fe1c-4261-c1a8-7fcb0b1118b0"
      },
      "source": [
        "print(features[2])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00017975 0.00374449 0.00355228 0.01284408 0.01766706 0.01470743\n",
            "  0.01294908 0.00544232 0.00272116 0.0040061  0.0017975  0.00430331\n",
            "  0.00398296 0.0017975  0.00430331 0.00371779 0.0017975  0.00430331\n",
            "  0.00401856 0.0017975  0.00430331 0.00397407 0.0017975  0.00430331\n",
            "  0.00415915 0.0017975  0.00430331 0.00401144 0.0017975  0.00430331\n",
            "  0.00406661 0.0017975  0.00419119 0.00405949 0.0017975  0.00419119\n",
            "  0.00401322 0.0017975  0.00419119 0.00399364 0.0017975  0.00419119\n",
            "  0.00401678 0.0017975  0.00419119 0.01918693 0.00883976 0.01950193\n",
            "  0.00624852 0.00368753 0.00356474 0.00458984 0.00364126 0.01774003\n",
            "  0.0036377  0.00184555 0.03256314 0.00357185 0.00464501 0.01232975\n",
            "  0.00355762 0.03927082 0.0017975  0.00186868 0.00180105 0.00186156\n",
            "  0.00180105 0.0018936  0.03094539 0.00404169 0.00184911 0.01611873\n",
            "  0.03252932 0.00368753 0.00180283 0.00664005 0.0169712  0.00548325\n",
            "  0.01727375 0.00188114 0.00180105 0.00187758 0.00180105 0.00872942\n",
            "  0.01054827 0.00365372 0.00975453 0.0035594  0.02153079 0.02668658\n",
            "  0.02884179 0.01950193 0.00624852 0.00368753 0.00355762 0.00529638\n",
            "  0.00437272 0.00529638 0.03489632 0.00360211 0.01100387 0.00367864\n",
            "  0.00376228 0.00184555 0.0091779  0.00357897 0.00647632 0.01701214\n",
            "  0.00357363 0.00364304 0.00402034 0.00355584 0.00364304 0.00699955\n",
            "  0.01538905 0.01484802 0.01918693 0.01950193 0.00624852 0.00368753\n",
            "  0.00359855 0.02153079 0.02668658 0.0342147  0.00569859 0.00357007\n",
            "  0.00529638 0.00018153 0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "EaDwOcs6O_85",
        "outputId": "d6cda874-e79e-4381-bb7a-c392aeae7e30"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "model_func = GCN\n",
        "\n",
        "support = [preprocess_adj(adj)]\n",
        "num_supports = 1\n",
        "# Define placeholders\n",
        "placeholders = {\n",
        "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
        "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
        "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
        "    'labels_mask': tf.placeholder(tf.int32),\n",
        "    'dropout': tf.placeholder_with_default(0.2, shape=()),\n",
        "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
        "}\n",
        "# Create model\n",
        "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
        "\n",
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "\n",
        "# Define model evaluation function\n",
        "def evaluate(features, support, labels, mask, placeholders):\n",
        "    t_test = time.time()\n",
        "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
        "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
        "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-bab51a4213e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m placeholders = {\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m'support'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_supports\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;34m'features'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;34m'labels'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m'labels_mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msparse_placeholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   3177\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3179\u001b[0;31m       \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3180\u001b[0m       \u001b[0mdense_shape_default\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value_as_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    885\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2_behavior\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqHkMcanOVX5"
      },
      "source": [
        "# Init variables\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "cost_val = []\n",
        "\n",
        "# Train model\n",
        "for epoch in range(FLAGS.epochs):\n",
        "\n",
        "    t = time.time()\n",
        "    # Construct feed dictionary\n",
        "    print('before feed')\n",
        "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
        "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
        "\n",
        "    # Training step\n",
        "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
        "\n",
        "    # Validation\n",
        "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
        "    cost_val.append(cost)\n",
        "\n",
        "    # Print results\n",
        "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
        "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
        "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
        "        print(\"Early stopping...\")\n",
        "        break\n",
        "\n",
        "print(\"Optimization Finished!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_h6NaJHPF6V"
      },
      "source": [
        "# Testing\n",
        "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
        "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
        "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}