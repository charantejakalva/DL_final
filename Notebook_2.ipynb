{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Notebook_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hV4t2RaaIHD",
        "outputId": "92d9c43a-3679-404e-c614-fde280ddd204"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gNqocrl-iR-",
        "outputId": "f412ae5b-6620-42f9-bba7-76acb4694d34"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAAs2VvI-rq9",
        "outputId": "ee53b089-fd90-4c65-adc8-6a0a4b7f8a13"
      },
      "source": [
        "!ls '/content/drive/MyDrive/Colab_Notebooks/DL_final/'\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BuzzFeed\t       feature_matrix_pf.csv\t\t  UserFeature.mat\n",
            "BuzzFeedNewsUser.txt   news_news_bf_adjacency_matrix.csv  User.txt\n",
            "BuzzFeedUserUser.txt   news_news_pf_adjacency_matrix.csv\n",
            "feature_matrix_bf.csv  News.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGzwLDX8BScF",
        "outputId": "6513f9ed-58d3-45f8-db72-a8c071d867f4"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzqOUINPaC0t",
        "outputId": "a1a76ae8-0a04-4eab-cb65-a45318d00941"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import scipy.sparse as sp\n",
        "import pandas as pd \n",
        "import random\n",
        "import json\n",
        "import os\n",
        "\n",
        "from feature_matrix import FeatureMatrix\n",
        "\n",
        "import pandas as pd \n",
        "import tensorflow_hub as hub\n",
        "from bert import run_classifier\n",
        "from bert import tokenization\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92T1ds1zaC0u"
      },
      "source": [
        "FM = FeatureMatrix(base_path = \"/content/drive/MyDrive/Colab_Notebooks/DL_final//\")\n",
        "label_zip = None"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcNDwKB-aC0u"
      },
      "source": [
        " def getAdj( dataset=\"BuzzFeed\"):\n",
        "        if dataset == \"BuzzFeed\":\n",
        "            adj_np = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/DL_final/news_news_bf_adjacency_matrix.csv\", header=None).values\n",
        "        else:\n",
        "            adj_np = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/DL_final/news_news_pf_adjacency_matrix.csv\", header=None).values\n",
        "\n",
        "        return sp.csr_matrix(adj_np, dtype=int)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZgYEexvaC0u"
      },
      "source": [
        "    def getFeatures( dataset=\"BuzzFeed\"):\n",
        "        feature_df =  FM.get_feature_matrix(dataset)\n",
        "        label = feature_df['label'].tolist()\n",
        "        label_comp = [0 if each else 1 for each in label]\n",
        "        global label_zip\n",
        "        label_zip = list(zip(label_comp, label))\n",
        "        feature_df.drop(['label'], axis=1)\n",
        "        feature_np = feature_df.values\n",
        "\n",
        "        return sp.csr_matrix(feature_np, dtype=float).tolil()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5HwPAnPaC0u"
      },
      "source": [
        "    def getYs(  dataset=\"BuzzFeed\"):\n",
        "        if dataset == \"BuzzFeed\":\n",
        "            random.seed(1)\n",
        "        else:\n",
        "            random.seed(1)\n",
        "        yTrain =  label_zip[:]\n",
        "        yVal =  label_zip[:]\n",
        "        yTest =  label_zip[:]\n",
        "        train_mask = [False] * len(yTrain)\n",
        "        val_mask = [False] * len(yTrain)\n",
        "        test_mask = [False] * len(yTrain)\n",
        "        n = len(yTrain)\n",
        "\n",
        "        set_of_records_range = set(range(n))\n",
        "\n",
        "        train_range = set(random.sample(set_of_records_range, k=int(n * 0.6)))\n",
        "        set_of_records_range = set_of_records_range - train_range\n",
        "\n",
        "        val_range = set(random.sample(set_of_records_range, k=int(n * 0.2)))\n",
        "        set_of_records_range = set_of_records_range - train_range\n",
        "\n",
        "        test_range = set(random.sample(set_of_records_range, k=int(n * 0.2)))\n",
        "\n",
        "        for i in train_range:\n",
        "            yVal[i] = (0,0)\n",
        "            yTest[i] = (0,0)\n",
        "            train_mask[i] = True\n",
        "        for i in val_range:\n",
        "            yTrain[i] = (0,0)\n",
        "            yTest[i] = (0,0)\n",
        "            val_mask[i] = True\n",
        "        for i in test_range:\n",
        "            yVal[i] = (0,0)\n",
        "            yTrain[i] = (0,0)\n",
        "            test_mask[i] = True\n",
        "\n",
        "        return yTrain, yVal, yTest, train_mask, val_mask, test_mask\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMzRtZ8saC0u"
      },
      "source": [
        "    def getComps(  dataset=\"BuzzFeed\"):\n",
        "        print(dataset)\n",
        "        adj =  getAdj(dataset)\n",
        "        features =  getFeatures(dataset)\n",
        "\n",
        "        yTrain, yVal, yTest, train_mask, val_mask, test_mask =  getYs(dataset)\n",
        "\n",
        "        return adj, features, yTrain, yVal, yTest, train_mask, val_mask, test_mask\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG12H5OpaC0u"
      },
      "source": [
        "def sparse_to_tuple(sparse_mx):\n",
        "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjFlJY85aC0u"
      },
      "source": [
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1, dtype=float).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return features.todense(), sparse_to_tuple(features)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H2st7PUaC0u"
      },
      "source": [
        "def adj_to_bias(adj, sizes, nhood=1):\n",
        "    nb_graphs = adj.shape[0]\n",
        "    mt = np.empty(adj.shape)\n",
        "    for g in range(nb_graphs):\n",
        "        mt[g] = np.eye(adj.shape[1])\n",
        "        for _ in range(nhood):\n",
        "            mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))\n",
        "        for i in range(sizes[g]):\n",
        "            for j in range(sizes[g]):\n",
        "                if mt[g][i][j] > 0.0:\n",
        "                    mt[g][i][j] = 1.0\n",
        "    return -1e9 * (1.0 - mt)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_Zlg_4RaC0u",
        "outputId": "5ed9b2d7-520e-43c4-8e09-620936be0ccc"
      },
      "source": [
        "\n",
        "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = getComps(dataset='BuzzFeed')\n",
        "features, spars = preprocess_features(features)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BuzzFeed\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/feature_matrix.py:49: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/feature_matrix.py:49: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 182\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 182\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 61 . 3 ##k shares facebook twitter update : buzz ##fe ##ed has deemed that our description of the crime and the victims is false . we have utterly de ##bu ##nk ##ed buzz ##fe ##ed ’ s claims . you can view our follow ##up article here . ~ ~ ~ you can ’ t make this stuff up . people used to tell crazy stories , but have no way to prove if the story was true or not . then along came video . this is by far one of the cr ##azi ##est things i ’ ve ever seen . these guys are out in the middle of the street and it looks like there was supposed to be a fight of [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 61 . 3 ##k shares facebook twitter update : buzz ##fe ##ed has deemed that our description of the crime and the victims is false . we have utterly de ##bu ##nk ##ed buzz ##fe ##ed ’ s claims . you can view our follow ##up article here . ~ ~ ~ you can ’ t make this stuff up . people used to tell crazy stories , but have no way to prove if the story was true or not . then along came video . this is by far one of the cr ##azi ##est things i ’ ve ever seen . these guys are out in the middle of the street and it looks like there was supposed to be a fight of [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6079 1012 1017 2243 6661 9130 10474 10651 1024 12610 7959 2098 2038 8357 2008 2256 6412 1997 1996 4126 1998 1996 5694 2003 6270 1012 2057 2031 12580 2139 8569 8950 2098 12610 7959 2098 1521 1055 4447 1012 2017 2064 3193 2256 3582 6279 3720 2182 1012 1066 1066 1066 2017 2064 1521 1056 2191 2023 4933 2039 1012 2111 2109 2000 2425 4689 3441 1010 2021 2031 2053 2126 2000 6011 2065 1996 2466 2001 2995 2030 2025 1012 2059 2247 2234 2678 1012 2023 2003 2011 2521 2028 1997 1996 13675 16103 4355 2477 1045 1521 2310 2412 2464 1012 2122 4364 2024 2041 1999 1996 2690 1997 1996 2395 1998 2009 3504 2066 2045 2001 4011 2000 2022 1037 2954 1997 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6079 1012 1017 2243 6661 9130 10474 10651 1024 12610 7959 2098 2038 8357 2008 2256 6412 1997 1996 4126 1998 1996 5694 2003 6270 1012 2057 2031 12580 2139 8569 8950 2098 12610 7959 2098 1521 1055 4447 1012 2017 2064 3193 2256 3582 6279 3720 2182 1012 1066 1066 1066 2017 2064 1521 1056 2191 2023 4933 2039 1012 2111 2109 2000 2425 4689 3441 1010 2021 2031 2053 2126 2000 6011 2065 1996 2466 2001 2995 2030 2025 1012 2059 2247 2234 2678 1012 2023 2003 2011 2521 2028 1997 1996 13675 16103 4355 2477 1045 1521 2310 2412 2464 1012 2122 4364 2024 2041 1999 1996 2690 1997 1996 2395 1998 2009 3504 2066 2045 2001 4011 2000 2022 1037 2954 1997 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] the man arrested monday in connection with the new york city bombing sued his local police force over anti - muslim discrimination claims . ahmad khan ra ##ham ##i filed the lawsuit against cops in elizabeth , n . j . , where he was residing before he planted bombs in the chelsea neighborhood of manhattan , at a train station in elizabeth , and on the route of a 5 ##k marine charity run on the jersey shore . he claimed police were per ##se ##cuting him for being a muslim and subject ##ing him and his family to “ selective enforcement ” based on islam , reports the daily mail . ra ##ham ##i worked at a chicken restaurant called first american , owned [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] the man arrested monday in connection with the new york city bombing sued his local police force over anti - muslim discrimination claims . ahmad khan ra ##ham ##i filed the lawsuit against cops in elizabeth , n . j . , where he was residing before he planted bombs in the chelsea neighborhood of manhattan , at a train station in elizabeth , and on the route of a 5 ##k marine charity run on the jersey shore . he claimed police were per ##se ##cuting him for being a muslim and subject ##ing him and his family to “ selective enforcement ” based on islam , reports the daily mail . ra ##ham ##i worked at a chicken restaurant called first american , owned [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1996 2158 4727 6928 1999 4434 2007 1996 2047 2259 2103 8647 12923 2010 2334 2610 2486 2058 3424 1011 5152 9147 4447 1012 10781 4967 10958 3511 2072 6406 1996 9870 2114 10558 1999 3870 1010 1050 1012 1046 1012 1010 2073 2002 2001 7154 2077 2002 8461 9767 1999 1996 9295 5101 1997 7128 1010 2012 1037 3345 2276 1999 3870 1010 1998 2006 1996 2799 1997 1037 1019 2243 3884 5952 2448 2006 1996 3933 5370 1012 2002 3555 2610 2020 2566 3366 29163 2032 2005 2108 1037 5152 1998 3395 2075 2032 1998 2010 2155 2000 1523 13228 7285 1524 2241 2006 7025 1010 4311 1996 3679 5653 1012 10958 3511 2072 2499 2012 1037 7975 4825 2170 2034 2137 1010 3079 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1996 2158 4727 6928 1999 4434 2007 1996 2047 2259 2103 8647 12923 2010 2334 2610 2486 2058 3424 1011 5152 9147 4447 1012 10781 4967 10958 3511 2072 6406 1996 9870 2114 10558 1999 3870 1010 1050 1012 1046 1012 1010 2073 2002 2001 7154 2077 2002 8461 9767 1999 1996 9295 5101 1997 7128 1010 2012 1037 3345 2276 1999 3870 1010 1998 2006 1996 2799 1997 1037 1019 2243 3884 5952 2448 2006 1996 3933 5370 1012 2002 3555 2610 2020 2566 3366 29163 2032 2005 2108 1037 5152 1998 3395 2075 2032 1998 2010 2155 2000 1523 13228 7285 1524 2241 2006 7025 1010 4311 1996 3679 5653 1012 10958 3511 2072 2499 2012 1037 7975 4825 2170 2034 2137 1010 3079 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] exposed : so ##ros funding fake veterans ’ pac to take trump down a couple months ago , discover the networks revealed a list of 187 groups who were funded by so ##ros . they have naturally been attacking the presidential candidate who is generally against their values like open borders , amnesty , giving illegal ##s voting rights , muslim migration and social justice . so , why then is so ##ros caught again ? now a new organization is going after trump and sure enough , they ’ re funded by so ##ros too ! it ’ s called ‘ common defense pac ’ and their mantra is : “ as veterans , we swore to protect the rights of every american . we [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] exposed : so ##ros funding fake veterans ’ pac to take trump down a couple months ago , discover the networks revealed a list of 187 groups who were funded by so ##ros . they have naturally been attacking the presidential candidate who is generally against their values like open borders , amnesty , giving illegal ##s voting rights , muslim migration and social justice . so , why then is so ##ros caught again ? now a new organization is going after trump and sure enough , they ’ re funded by so ##ros too ! it ’ s called ‘ common defense pac ’ and their mantra is : “ as veterans , we swore to protect the rights of every american . we [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6086 1024 2061 7352 4804 8275 8244 1521 14397 2000 2202 8398 2091 1037 3232 2706 3283 1010 7523 1996 6125 3936 1037 2862 1997 19446 2967 2040 2020 6787 2011 2061 7352 1012 2027 2031 8100 2042 7866 1996 4883 4018 2040 2003 3227 2114 2037 5300 2066 2330 6645 1010 16154 1010 3228 6206 2015 6830 2916 1010 5152 9230 1998 2591 3425 1012 2061 1010 2339 2059 2003 2061 7352 3236 2153 1029 2085 1037 2047 3029 2003 2183 2044 8398 1998 2469 2438 1010 2027 1521 2128 6787 2011 2061 7352 2205 999 2009 1521 1055 2170 1520 2691 3639 14397 1521 1998 2037 25951 2003 1024 1523 2004 8244 1010 2057 12860 2000 4047 1996 2916 1997 2296 2137 1012 2057 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6086 1024 2061 7352 4804 8275 8244 1521 14397 2000 2202 8398 2091 1037 3232 2706 3283 1010 7523 1996 6125 3936 1037 2862 1997 19446 2967 2040 2020 6787 2011 2061 7352 1012 2027 2031 8100 2042 7866 1996 4883 4018 2040 2003 3227 2114 2037 5300 2066 2330 6645 1010 16154 1010 3228 6206 2015 6830 2916 1010 5152 9230 1998 2591 3425 1012 2061 1010 2339 2059 2003 2061 7352 3236 2153 1029 2085 1037 2047 3029 2003 2183 2044 8398 1998 2469 2438 1010 2027 1521 2128 6787 2011 2061 7352 2205 999 2009 1521 1055 2170 1520 2691 3639 14397 1521 1998 2037 25951 2003 1024 1523 2004 8244 1010 2057 12860 2000 4047 1996 2916 1997 2296 2137 1012 2057 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] this is a true story . general stanley mcc ##hr ##yst ##al was the commander of us forces in afghanistan and he had frequent disagreements on the conduct of the war with his commander - in - chief . at one point mcc ##hr ##yst ##al was called into the oval office and he knew his army career was about to be over . when former u . s . military commander in afghanistan , stanley mcc ##hr ##yst ##al , was called into the oval office by barack obama , he knew things weren ’ t going to go well when the president accused him of not supporting him in his political role as president . “ it ’ s not my job to support [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] this is a true story . general stanley mcc ##hr ##yst ##al was the commander of us forces in afghanistan and he had frequent disagreements on the conduct of the war with his commander - in - chief . at one point mcc ##hr ##yst ##al was called into the oval office and he knew his army career was about to be over . when former u . s . military commander in afghanistan , stanley mcc ##hr ##yst ##al , was called into the oval office by barack obama , he knew things weren ’ t going to go well when the president accused him of not supporting him in his political role as president . “ it ’ s not my job to support [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2023 2003 1037 2995 2466 1012 2236 6156 23680 8093 27268 2389 2001 1996 3474 1997 2149 2749 1999 7041 1998 2002 2018 6976 23145 2006 1996 6204 1997 1996 2162 2007 2010 3474 1011 1999 1011 2708 1012 2012 2028 2391 23680 8093 27268 2389 2001 2170 2046 1996 9242 2436 1998 2002 2354 2010 2390 2476 2001 2055 2000 2022 2058 1012 2043 2280 1057 1012 1055 1012 2510 3474 1999 7041 1010 6156 23680 8093 27268 2389 1010 2001 2170 2046 1996 9242 2436 2011 13857 8112 1010 2002 2354 2477 4694 1521 1056 2183 2000 2175 2092 2043 1996 2343 5496 2032 1997 2025 4637 2032 1999 2010 2576 2535 2004 2343 1012 1523 2009 1521 1055 2025 2026 3105 2000 2490 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2023 2003 1037 2995 2466 1012 2236 6156 23680 8093 27268 2389 2001 1996 3474 1997 2149 2749 1999 7041 1998 2002 2018 6976 23145 2006 1996 6204 1997 1996 2162 2007 2010 3474 1011 1999 1011 2708 1012 2012 2028 2391 23680 8093 27268 2389 2001 2170 2046 1996 9242 2436 1998 2002 2354 2010 2390 2476 2001 2055 2000 2022 2058 1012 2043 2280 1057 1012 1055 1012 2510 3474 1999 7041 1010 6156 23680 8093 27268 2389 1010 2001 2170 2046 1996 9242 2436 2011 13857 8112 1010 2002 2354 2477 4694 1521 1056 2183 2000 2175 2092 2043 1996 2343 5496 2032 1997 2025 4637 2032 1999 2010 2576 2535 2004 2343 1012 1523 2009 1521 1055 2025 2026 3105 2000 2490 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] \" he didn ' t ask her about a lot of things she should have been asked about , \" donald trump said about lester holt . | get ##ty post - debate , trump team hits moderator holt for ' some hostile questions ' donald trump and his campaign complained tuesday morning that debate moderator lester holt targeted him for unfair questioning while neglect ##ing to ask hillary clinton about any of her various scandals . “ they were leaving all of her little good ##ies out . they didn ' t ask her about , you know , much , ” trump said on fox news ’ s “ fox and friends . ” “ but i was asked about my tax returns , [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] \" he didn ' t ask her about a lot of things she should have been asked about , \" donald trump said about lester holt . | get ##ty post - debate , trump team hits moderator holt for ' some hostile questions ' donald trump and his campaign complained tuesday morning that debate moderator lester holt targeted him for unfair questioning while neglect ##ing to ask hillary clinton about any of her various scandals . “ they were leaving all of her little good ##ies out . they didn ' t ask her about , you know , much , ” trump said on fox news ’ s “ fox and friends . ” “ but i was asked about my tax returns , [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1000 2002 2134 1005 1056 3198 2014 2055 1037 2843 1997 2477 2016 2323 2031 2042 2356 2055 1010 1000 6221 8398 2056 2055 14131 12621 1012 1064 2131 3723 2695 1011 5981 1010 8398 2136 4978 29420 12621 2005 1005 2070 10420 3980 1005 6221 8398 1998 2010 3049 10865 9857 2851 2008 5981 29420 14131 12621 9416 2032 2005 15571 11242 2096 19046 2075 2000 3198 18520 7207 2055 2151 1997 2014 2536 29609 1012 1523 2027 2020 2975 2035 1997 2014 2210 2204 3111 2041 1012 2027 2134 1005 1056 3198 2014 2055 1010 2017 2113 1010 2172 1010 1524 8398 2056 2006 4419 2739 1521 1055 1523 4419 1998 2814 1012 1524 1523 2021 1045 2001 2356 2055 2026 4171 5651 1010 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1000 2002 2134 1005 1056 3198 2014 2055 1037 2843 1997 2477 2016 2323 2031 2042 2356 2055 1010 1000 6221 8398 2056 2055 14131 12621 1012 1064 2131 3723 2695 1011 5981 1010 8398 2136 4978 29420 12621 2005 1005 2070 10420 3980 1005 6221 8398 1998 2010 3049 10865 9857 2851 2008 5981 29420 14131 12621 9416 2032 2005 15571 11242 2096 19046 2075 2000 3198 18520 7207 2055 2151 1997 2014 2536 29609 1012 1523 2027 2020 2975 2035 1997 2014 2210 2204 3111 2041 1012 2027 2134 1005 1056 3198 2014 2055 1010 2017 2113 1010 2172 1010 1524 8398 2056 2006 4419 2739 1521 1055 1523 4419 1998 2814 1012 1524 1523 2021 1045 2001 2356 2055 2026 4171 5651 1010 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLxYjuNzEAHh"
      },
      "source": [
        "features=sp.csr_matrix(features)\n",
        "\n",
        "y_train  = np.asarray(y_train)\n",
        "y_val = np.asarray(y_val)\n",
        "y_test = np.asarray(y_test)\n",
        "train_mask  = np.asarray(train_mask)\n",
        "val_mask  = np.asarray(val_mask)\n",
        "test_mask  = np.asarray(test_mask)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLqa550GHPj5"
      },
      "source": [
        "from scipy import sparse\n",
        "\n",
        "sparse.save_npz(\"features.npz\", features)\n",
        "sparse.save_npz(\"adj.npz\", adj)\n",
        "# your_matrix_back = sparse.load_npz(\"yourmatrix.npz\")\n",
        "with open('array.npy', 'wb') as f:\n",
        "    np.save(f, y_train )\n",
        "    np.save(f,  y_val)\n",
        "    np.save(f, y_test )\n",
        "    np.save(f, train_mask )\n",
        "    np.save(f,  val_mask)\n",
        "    np.save(f, test_mask )"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inu7acNWMyNm"
      },
      "source": [
        "features = scipy.sparse.load_npz(\"features.npz\")\n",
        "adj = scipy.sparse.load_npz(\"adj.npz\")\n",
        "\n",
        "with open('array.npy', 'rb') as f:\n",
        "    y_train = np.load(f)\n",
        "    y_val = np.load(f)\n",
        "    y_test = np.load(f)\n",
        "    train_mask = np.load(f)\n",
        "    val_mask = np.load(f)\n",
        "    test_mask = np.load(f)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twKeogQQDuX_",
        "outputId": "abe17ce0-4811-402e-8f7a-84c550a45b16"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import tensorflow as tf \n",
        "from  utils import *\n",
        "from models import GCN, MLP\n",
        "\n",
        "# Set random seed\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "# Settings\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_string('f', '', 'kernel')\n",
        "# flags.DEFINE_string('dataset', 'cora', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
        "# flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
        "# flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
        "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
        "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
        "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
        "\n",
        "# Load data\n",
        "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = getComps(dataset='BuzzFeed')\n",
        "features=sp.csr_matrix(features)\n",
        "# Some preprocessing\n",
        "features = preprocess_features(features)\n",
        "# if FLAGS.model == 'gcn':\n",
        "support = [preprocess_adj(adj)]\n",
        "num_supports = 1\n",
        "model_func = GCN\n",
        "# elif FLAGS.model == 'gcn_cheby':\n",
        "#     support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
        "#     num_supports = 1 + FLAGS.max_degree\n",
        "#     model_func = GCN\n",
        "# elif FLAGS.model == 'dense':\n",
        "#     support = [preprocess_adj(adj)]  # Not used\n",
        "#     num_supports = 1\n",
        "#     model_func = MLP\n",
        "# else:\n",
        "#     raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
        "\n",
        "y_train  = np.asarray(y_train)\n",
        "y_val = np.asarray(y_val)\n",
        "y_test = np.asarray(y_test)\n",
        "train_mask  = np.asarray(train_mask)\n",
        "val_mask  = np.asarray(val_mask)\n",
        "test_mask  = np.asarray(test_mask)\n",
        "\n",
        "# Define placeholders\n",
        "placeholders = {\n",
        "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
        "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
        "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
        "    'labels_mask': tf.placeholder(tf.int32),\n",
        "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
        "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
        "}\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BuzzFeed\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/feature_matrix.py:49: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/feature_matrix.py:49: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 182\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 182\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] friday morning around 4 : 00 , three men – some of whom were armed – barge ##d in through one of the doors of an atlanta resident ’ s house . the resident ’ s co - worker was over for work - related reasons , and when she heard the commotion , she sprung into action , wielding a handgun . the whole episode was caught on surveillance camera that was set up inside the house . the resident ’ s co - worker un ##loaded all the rounds from her gun into the intruder ##s ’ direction , sending all three of them out the door . one of the intruder ##s even jumped through a glass door . two intruder ##s are [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] friday morning around 4 : 00 , three men – some of whom were armed – barge ##d in through one of the doors of an atlanta resident ’ s house . the resident ’ s co - worker was over for work - related reasons , and when she heard the commotion , she sprung into action , wielding a handgun . the whole episode was caught on surveillance camera that was set up inside the house . the resident ’ s co - worker un ##loaded all the rounds from her gun into the intruder ##s ’ direction , sending all three of them out the door . one of the intruder ##s even jumped through a glass door . two intruder ##s are [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 5958 2851 2105 1018 1024 4002 1010 2093 2273 1516 2070 1997 3183 2020 4273 1516 19398 2094 1999 2083 2028 1997 1996 4303 1997 2019 5865 6319 1521 1055 2160 1012 1996 6319 1521 1055 2522 1011 7309 2001 2058 2005 2147 1011 3141 4436 1010 1998 2043 2016 2657 1996 23960 1010 2016 22057 2046 2895 1010 26974 1037 28497 1012 1996 2878 2792 2001 3236 2006 9867 4950 2008 2001 2275 2039 2503 1996 2160 1012 1996 6319 1521 1055 2522 1011 7309 4895 17468 2035 1996 6241 2013 2014 3282 2046 1996 22841 2015 1521 3257 1010 6016 2035 2093 1997 2068 2041 1996 2341 1012 2028 1997 1996 22841 2015 2130 5598 2083 1037 3221 2341 1012 2048 22841 2015 2024 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 5958 2851 2105 1018 1024 4002 1010 2093 2273 1516 2070 1997 3183 2020 4273 1516 19398 2094 1999 2083 2028 1997 1996 4303 1997 2019 5865 6319 1521 1055 2160 1012 1996 6319 1521 1055 2522 1011 7309 2001 2058 2005 2147 1011 3141 4436 1010 1998 2043 2016 2657 1996 23960 1010 2016 22057 2046 2895 1010 26974 1037 28497 1012 1996 2878 2792 2001 3236 2006 9867 4950 2008 2001 2275 2039 2503 1996 2160 1012 1996 6319 1521 1055 2522 1011 7309 4895 17468 2035 1996 6241 2013 2014 3282 2046 1996 22841 2015 1521 3257 1010 6016 2035 2093 1997 2068 2041 1996 2341 1012 2028 1997 1996 22841 2015 2130 5598 2083 1037 3221 2341 1012 2048 22841 2015 2024 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] story highlights a protest ##er pie ##d sacramento ' s mayor in the face at a charity event wednesday the two sc ##uf ##fle ##d afterward , and the protest ##er was taken to a hospital for stitches ( cnn ) kevin johnson - - the nba star - turned - sacramento mayor - - was pie ##d in the face by a man at a charity dinner wednesday night at the high school he once attended . but the pie - throw ##er appeared to get the worst of it . sean thompson , 32 , approached the mayor , pulled a pie out of the bag and shoving it in his face - - setting off a short sc ##uf ##fle , in which [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] story highlights a protest ##er pie ##d sacramento ' s mayor in the face at a charity event wednesday the two sc ##uf ##fle ##d afterward , and the protest ##er was taken to a hospital for stitches ( cnn ) kevin johnson - - the nba star - turned - sacramento mayor - - was pie ##d in the face by a man at a charity dinner wednesday night at the high school he once attended . but the pie - throw ##er appeared to get the worst of it . sean thompson , 32 , approached the mayor , pulled a pie out of the bag and shoving it in his face - - setting off a short sc ##uf ##fle , in which [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2466 11637 1037 6186 2121 11345 2094 11932 1005 1055 3664 1999 1996 2227 2012 1037 5952 2724 9317 1996 2048 8040 16093 21031 2094 9707 1010 1998 1996 6186 2121 2001 2579 2000 1037 2902 2005 25343 1006 13229 1007 4901 3779 1011 1011 1996 6452 2732 1011 2357 1011 11932 3664 1011 1011 2001 11345 2094 1999 1996 2227 2011 1037 2158 2012 1037 5952 4596 9317 2305 2012 1996 2152 2082 2002 2320 3230 1012 2021 1996 11345 1011 5466 2121 2596 2000 2131 1996 5409 1997 2009 1012 5977 5953 1010 3590 1010 5411 1996 3664 1010 2766 1037 11345 2041 1997 1996 4524 1998 15866 2009 1999 2010 2227 1011 1011 4292 2125 1037 2460 8040 16093 21031 1010 1999 2029 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2466 11637 1037 6186 2121 11345 2094 11932 1005 1055 3664 1999 1996 2227 2012 1037 5952 2724 9317 1996 2048 8040 16093 21031 2094 9707 1010 1998 1996 6186 2121 2001 2579 2000 1037 2902 2005 25343 1006 13229 1007 4901 3779 1011 1011 1996 6452 2732 1011 2357 1011 11932 3664 1011 1011 2001 11345 2094 1999 1996 2227 2011 1037 2158 2012 1037 5952 4596 9317 2305 2012 1996 2152 2082 2002 2320 3230 1012 2021 1996 11345 1011 5466 2121 2596 2000 2131 1996 5409 1997 2009 1012 5977 5953 1010 3590 1010 5411 1996 3664 1010 2766 1037 11345 2041 1997 1996 4524 1998 15866 2009 1999 2010 2227 1011 1011 4292 2125 1037 2460 8040 16093 21031 1010 1999 2029 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 0 shares facebook twitter bernard sans ##ari ##c ##q , former president of the haitian senate , issued a b ##list ##ering statement condemning the clinton foundation , which has been posted at donald trump ’ s campaign website . sans ##ari ##c ##q ’ s statement says : sadly , when an earthquake rocked the nation of haiti in 2010 , corruption moved in faster than the help so desperately needed . today , the people of haiti are still suffering despite the billions of dollars that have flowed into the clinton foundation . the clinton ##s exploited this terrible disaster to steal billions of dollars from the sick and starving people of haiti . the world trusted the clinton ##s to help the haitian [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 0 shares facebook twitter bernard sans ##ari ##c ##q , former president of the haitian senate , issued a b ##list ##ering statement condemning the clinton foundation , which has been posted at donald trump ’ s campaign website . sans ##ari ##c ##q ’ s statement says : sadly , when an earthquake rocked the nation of haiti in 2010 , corruption moved in faster than the help so desperately needed . today , the people of haiti are still suffering despite the billions of dollars that have flowed into the clinton foundation . the clinton ##s exploited this terrible disaster to steal billions of dollars from the sick and starving people of haiti . the world trusted the clinton ##s to help the haitian [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1014 6661 9130 10474 6795 20344 8486 2278 4160 1010 2280 2343 1997 1996 21404 4001 1010 3843 1037 1038 9863 7999 4861 28525 1996 7207 3192 1010 2029 2038 2042 6866 2012 6221 8398 1521 1055 3049 4037 1012 20344 8486 2278 4160 1521 1055 4861 2758 1024 13718 1010 2043 2019 8372 14215 1996 3842 1997 12867 1999 2230 1010 7897 2333 1999 5514 2084 1996 2393 2061 9652 2734 1012 2651 1010 1996 2111 1997 12867 2024 2145 6114 2750 1996 25501 1997 6363 2008 2031 13230 2046 1996 7207 3192 1012 1996 7207 2015 18516 2023 6659 7071 2000 8954 25501 1997 6363 2013 1996 5305 1998 18025 2111 1997 12867 1012 1996 2088 9480 1996 7207 2015 2000 2393 1996 21404 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1014 6661 9130 10474 6795 20344 8486 2278 4160 1010 2280 2343 1997 1996 21404 4001 1010 3843 1037 1038 9863 7999 4861 28525 1996 7207 3192 1010 2029 2038 2042 6866 2012 6221 8398 1521 1055 3049 4037 1012 20344 8486 2278 4160 1521 1055 4861 2758 1024 13718 1010 2043 2019 8372 14215 1996 3842 1997 12867 1999 2230 1010 7897 2333 1999 5514 2084 1996 2393 2061 9652 2734 1012 2651 1010 1996 2111 1997 12867 2024 2145 6114 2750 1996 25501 1997 6363 2008 2031 13230 2046 1996 7207 3192 1012 1996 7207 2015 18516 2023 6659 7071 2000 8954 25501 1997 6363 2013 1996 5305 1998 18025 2111 1997 12867 1012 1996 2088 9480 1996 7207 2015 2000 2393 1996 21404 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] the democrats are using an intimidation tactic which they are prone to use inc ##ess ##antly . if you question the vera ##city of a black president ’ s birth certificate you are racist . that is the outright claim of lynn sweet washington bureau chief for the chicago sun - times made on fox news . consider the slant ##ed claim that if a president is black and his birth certificate is called into question the conclusion is you must be a racist . what is wrong with this obvious non - se ##qui ##tur ? first of all , there is evidence the birth certificate presented was altered . this is discount ##ed by the claims that whoever presented this must be racist . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] the democrats are using an intimidation tactic which they are prone to use inc ##ess ##antly . if you question the vera ##city of a black president ’ s birth certificate you are racist . that is the outright claim of lynn sweet washington bureau chief for the chicago sun - times made on fox news . consider the slant ##ed claim that if a president is black and his birth certificate is called into question the conclusion is you must be a racist . what is wrong with this obvious non - se ##qui ##tur ? first of all , there is evidence the birth certificate presented was altered . this is discount ##ed by the claims that whoever presented this must be racist . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1996 8037 2024 2478 2019 28973 19717 2029 2027 2024 13047 2000 2224 4297 7971 15706 1012 2065 2017 3160 1996 12297 12972 1997 1037 2304 2343 1521 1055 4182 8196 2017 2024 16939 1012 2008 2003 1996 13848 4366 1997 9399 4086 2899 4879 2708 2005 1996 3190 3103 1011 2335 2081 2006 4419 2739 1012 5136 1996 27474 2098 4366 2008 2065 1037 2343 2003 2304 1998 2010 4182 8196 2003 2170 2046 3160 1996 7091 2003 2017 2442 2022 1037 16939 1012 2054 2003 3308 2007 2023 5793 2512 1011 7367 15549 20689 1029 2034 1997 2035 1010 2045 2003 3350 1996 4182 8196 3591 2001 8776 1012 2023 2003 19575 2098 2011 1996 4447 2008 9444 3591 2023 2442 2022 16939 1012 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1996 8037 2024 2478 2019 28973 19717 2029 2027 2024 13047 2000 2224 4297 7971 15706 1012 2065 2017 3160 1996 12297 12972 1997 1037 2304 2343 1521 1055 4182 8196 2017 2024 16939 1012 2008 2003 1996 13848 4366 1997 9399 4086 2899 4879 2708 2005 1996 3190 3103 1011 2335 2081 2006 4419 2739 1012 5136 1996 27474 2098 4366 2008 2065 1037 2343 2003 2304 1998 2010 4182 8196 2003 2170 2046 3160 1996 7091 2003 2017 2442 2022 1037 16939 1012 2054 2003 3308 2007 2023 5793 2512 1011 7367 15549 20689 1029 2034 1997 2035 1010 2045 2003 3350 1996 4182 8196 3591 2001 8776 1012 2023 2003 19575 2098 2011 1996 4447 2008 9444 3591 2023 2442 2022 16939 1012 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] breaking : pipe bombs found in new jersey train station first a pipe bomb exploded on the jersey shore , then new york was next . now , pipe bombs have been found in a new jersey train station . according to the new york daily news , “ authorities discovered three pipe bombs and two smaller devices at a train station in elizabeth . ” new jersey : elizabeth mayor says a bag with wires and pipes was found in a trash can near nj ##t station , fbi on scene . https : / / t . co / q ##60 ##l ##ggs ##52 ##o — ko ##l ##ha ##ola ##m ( @ ko ##l ##ha ##ola ##m ) september 19 , 2016 since [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] breaking : pipe bombs found in new jersey train station first a pipe bomb exploded on the jersey shore , then new york was next . now , pipe bombs have been found in a new jersey train station . according to the new york daily news , “ authorities discovered three pipe bombs and two smaller devices at a train station in elizabeth . ” new jersey : elizabeth mayor says a bag with wires and pipes was found in a trash can near nj ##t station , fbi on scene . https : / / t . co / q ##60 ##l ##ggs ##52 ##o — ko ##l ##ha ##ola ##m ( @ ko ##l ##ha ##ola ##m ) september 19 , 2016 since [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4911 1024 8667 9767 2179 1999 2047 3933 3345 2276 2034 1037 8667 5968 9913 2006 1996 3933 5370 1010 2059 2047 2259 2001 2279 1012 2085 1010 8667 9767 2031 2042 2179 1999 1037 2047 3933 3345 2276 1012 2429 2000 1996 2047 2259 3679 2739 1010 1523 4614 3603 2093 8667 9767 1998 2048 3760 5733 2012 1037 3345 2276 1999 3870 1012 1524 2047 3933 1024 3870 3664 2758 1037 4524 2007 14666 1998 12432 2001 2179 1999 1037 11669 2064 2379 19193 2102 2276 1010 8495 2006 3496 1012 16770 1024 1013 1013 1056 1012 2522 1013 1053 16086 2140 21314 25746 2080 1517 12849 2140 3270 6030 2213 1006 1030 12849 2140 3270 6030 2213 1007 2244 2539 1010 2355 2144 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4911 1024 8667 9767 2179 1999 2047 3933 3345 2276 2034 1037 8667 5968 9913 2006 1996 3933 5370 1010 2059 2047 2259 2001 2279 1012 2085 1010 8667 9767 2031 2042 2179 1999 1037 2047 3933 3345 2276 1012 2429 2000 1996 2047 2259 3679 2739 1010 1523 4614 3603 2093 8667 9767 1998 2048 3760 5733 2012 1037 3345 2276 1999 3870 1012 1524 2047 3933 1024 3870 3664 2758 1037 4524 2007 14666 1998 12432 2001 2179 1999 1037 11669 2064 2379 19193 2102 2276 1010 8495 2006 3496 1012 16770 1024 1013 1013 1056 1012 2522 1013 1053 16086 2140 21314 25746 2080 1517 12849 2140 3270 6030 2213 1006 1030 12849 2140 3270 6030 2213 1007 2244 2539 1010 2355 2144 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "/content/utils.py:126: RuntimeWarning: invalid value encountered in power\n",
            "  d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmxwMSiIYPkg"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d9BAW9aOgnD",
        "outputId": "dc3412c5-95f6-4fd5-a542-b0baaaf7422f"
      },
      "source": [
        "    print('adj shape', type(adj))\n",
        "    print('features shape', type(features))\n",
        "    print('y_train shape', type(y_train))\n",
        "    print('y_val shape', type(y_val))\n",
        "    print('y_test shape', type(y_test))\n",
        "    print('train_mask shape', type(train_mask))\n",
        "    print('val_mask shape', type(val_mask))\n",
        "    print('test_mask shape', type(test_mask)) "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adj shape <class 'scipy.sparse.csr.csr_matrix'>\n",
            "features shape <class 'tuple'>\n",
            "y_train shape <class 'numpy.ndarray'>\n",
            "y_val shape <class 'numpy.ndarray'>\n",
            "y_test shape <class 'numpy.ndarray'>\n",
            "train_mask shape <class 'numpy.ndarray'>\n",
            "val_mask shape <class 'numpy.ndarray'>\n",
            "test_mask shape <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J4pE-cCbeKN",
        "outputId": "04a7cbc7-d752-4cd8-dbf3-b1fc88e6c0d3"
      },
      "source": [
        "train_mask.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(182,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn574ijFOxCI"
      },
      "source": [
        "adj shape <class 'scipy.sparse.csr.csr_matrix'>\n",
        "features shape <class 'scipy.sparse.lil.lil_matrix'>\n",
        "y_train shape <class 'numpy.ndarray'>\n",
        "y_val shape <class 'numpy.ndarray'>\n",
        "y_test shape <class 'numpy.ndarray'>\n",
        "train_mask shape <class 'numpy.ndarray'>\n",
        "val_mask shape <class 'numpy.ndarray'>\n",
        "test_mask shape <class 'numpy.ndarray'>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wcJj2F1G2PW",
        "outputId": "3ac35e3b-f61b-4509-f6fe-52d87fb5d55c"
      },
      "source": [
        "\n",
        "# Create model\n",
        "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
        "\n",
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "\n",
        "\n",
        "# Define model evaluation function\n",
        "def evaluate(features, support, labels, mask, placeholders):\n",
        "    t_test = time.time()\n",
        "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
        "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
        "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
        "\n",
        "\n",
        "# Init variables\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "cost_val = []\n",
        "\n",
        "# Train model\n",
        "for epoch in range(FLAGS.epochs):\n",
        "\n",
        "    t = time.time()\n",
        "    # Construct feed dictionary\n",
        "    print('before feed')\n",
        "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
        "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
        "\n",
        "    # Training step\n",
        "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
        "\n",
        "    # Validation\n",
        "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
        "    cost_val.append(cost)\n",
        "\n",
        "    # Print results\n",
        "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
        "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
        "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
        "        print(\"Early stopping...\")\n",
        "        break\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "\n",
        "# Testing\n",
        "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
        "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
        "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/layers.py:170: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/layers.py:170: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "before feed\n",
            "Epoch: 0001 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.30413\n",
            "before feed\n",
            "Epoch: 0002 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01034\n",
            "before feed\n",
            "Epoch: 0003 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01057\n",
            "before feed\n",
            "Epoch: 0004 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01009\n",
            "before feed\n",
            "Epoch: 0005 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01023\n",
            "before feed\n",
            "Epoch: 0006 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01063\n",
            "before feed\n",
            "Epoch: 0007 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00961\n",
            "before feed\n",
            "Epoch: 0008 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00915\n",
            "before feed\n",
            "Epoch: 0009 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00991\n",
            "before feed\n",
            "Epoch: 0010 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00949\n",
            "before feed\n",
            "Epoch: 0011 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00939\n",
            "before feed\n",
            "Epoch: 0012 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01016\n",
            "before feed\n",
            "Epoch: 0013 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00977\n",
            "before feed\n",
            "Epoch: 0014 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00929\n",
            "before feed\n",
            "Epoch: 0015 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00962\n",
            "before feed\n",
            "Epoch: 0016 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00957\n",
            "before feed\n",
            "Epoch: 0017 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01002\n",
            "before feed\n",
            "Epoch: 0018 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00945\n",
            "before feed\n",
            "Epoch: 0019 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00988\n",
            "before feed\n",
            "Epoch: 0020 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01398\n",
            "before feed\n",
            "Epoch: 0021 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00901\n",
            "before feed\n",
            "Epoch: 0022 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00992\n",
            "before feed\n",
            "Epoch: 0023 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01030\n",
            "before feed\n",
            "Epoch: 0024 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00938\n",
            "before feed\n",
            "Epoch: 0025 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01002\n",
            "before feed\n",
            "Epoch: 0026 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01017\n",
            "before feed\n",
            "Epoch: 0027 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01484\n",
            "before feed\n",
            "Epoch: 0028 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01056\n",
            "before feed\n",
            "Epoch: 0029 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00936\n",
            "before feed\n",
            "Epoch: 0030 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00978\n",
            "before feed\n",
            "Epoch: 0031 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00983\n",
            "before feed\n",
            "Epoch: 0032 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01162\n",
            "before feed\n",
            "Epoch: 0033 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00994\n",
            "before feed\n",
            "Epoch: 0034 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01013\n",
            "before feed\n",
            "Epoch: 0035 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01093\n",
            "before feed\n",
            "Epoch: 0036 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01268\n",
            "before feed\n",
            "Epoch: 0037 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01062\n",
            "before feed\n",
            "Epoch: 0038 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01672\n",
            "before feed\n",
            "Epoch: 0039 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01162\n",
            "before feed\n",
            "Epoch: 0040 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01119\n",
            "before feed\n",
            "Epoch: 0041 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01051\n",
            "before feed\n",
            "Epoch: 0042 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01127\n",
            "before feed\n",
            "Epoch: 0043 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01139\n",
            "before feed\n",
            "Epoch: 0044 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00995\n",
            "before feed\n",
            "Epoch: 0045 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00992\n",
            "before feed\n",
            "Epoch: 0046 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01034\n",
            "before feed\n",
            "Epoch: 0047 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00939\n",
            "before feed\n",
            "Epoch: 0048 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00947\n",
            "before feed\n",
            "Epoch: 0049 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00916\n",
            "before feed\n",
            "Epoch: 0050 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01027\n",
            "before feed\n",
            "Epoch: 0051 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00949\n",
            "before feed\n",
            "Epoch: 0052 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00929\n",
            "before feed\n",
            "Epoch: 0053 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00941\n",
            "before feed\n",
            "Epoch: 0054 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01012\n",
            "before feed\n",
            "Epoch: 0055 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00976\n",
            "before feed\n",
            "Epoch: 0056 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00952\n",
            "before feed\n",
            "Epoch: 0057 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01245\n",
            "before feed\n",
            "Epoch: 0058 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00952\n",
            "before feed\n",
            "Epoch: 0059 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00965\n",
            "before feed\n",
            "Epoch: 0060 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01090\n",
            "before feed\n",
            "Epoch: 0061 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00994\n",
            "before feed\n",
            "Epoch: 0062 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01022\n",
            "before feed\n",
            "Epoch: 0063 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01113\n",
            "before feed\n",
            "Epoch: 0064 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01215\n",
            "before feed\n",
            "Epoch: 0065 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01497\n",
            "before feed\n",
            "Epoch: 0066 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01489\n",
            "before feed\n",
            "Epoch: 0067 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01182\n",
            "before feed\n",
            "Epoch: 0068 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01111\n",
            "before feed\n",
            "Epoch: 0069 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01578\n",
            "before feed\n",
            "Epoch: 0070 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01020\n",
            "before feed\n",
            "Epoch: 0071 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01052\n",
            "before feed\n",
            "Epoch: 0072 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01011\n",
            "before feed\n",
            "Epoch: 0073 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01040\n",
            "before feed\n",
            "Epoch: 0074 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01267\n",
            "before feed\n",
            "Epoch: 0075 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01115\n",
            "before feed\n",
            "Epoch: 0076 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00993\n",
            "before feed\n",
            "Epoch: 0077 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01114\n",
            "before feed\n",
            "Epoch: 0078 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01095\n",
            "before feed\n",
            "Epoch: 0079 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00998\n",
            "before feed\n",
            "Epoch: 0080 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00963\n",
            "before feed\n",
            "Epoch: 0081 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00949\n",
            "before feed\n",
            "Epoch: 0082 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01013\n",
            "before feed\n",
            "Epoch: 0083 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00990\n",
            "before feed\n",
            "Epoch: 0084 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00945\n",
            "before feed\n",
            "Epoch: 0085 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00956\n",
            "before feed\n",
            "Epoch: 0086 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00993\n",
            "before feed\n",
            "Epoch: 0087 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01010\n",
            "before feed\n",
            "Epoch: 0088 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00991\n",
            "before feed\n",
            "Epoch: 0089 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00992\n",
            "before feed\n",
            "Epoch: 0090 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01033\n",
            "before feed\n",
            "Epoch: 0091 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00870\n",
            "before feed\n",
            "Epoch: 0092 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00972\n",
            "before feed\n",
            "Epoch: 0093 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01360\n",
            "before feed\n",
            "Epoch: 0094 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01071\n",
            "before feed\n",
            "Epoch: 0095 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01012\n",
            "before feed\n",
            "Epoch: 0096 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01350\n",
            "before feed\n",
            "Epoch: 0097 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00992\n",
            "before feed\n",
            "Epoch: 0098 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00958\n",
            "before feed\n",
            "Epoch: 0099 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00990\n",
            "before feed\n",
            "Epoch: 0100 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01012\n",
            "before feed\n",
            "Epoch: 0101 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00979\n",
            "before feed\n",
            "Epoch: 0102 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01067\n",
            "before feed\n",
            "Epoch: 0103 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00973\n",
            "before feed\n",
            "Epoch: 0104 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00944\n",
            "before feed\n",
            "Epoch: 0105 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00953\n",
            "before feed\n",
            "Epoch: 0106 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00907\n",
            "before feed\n",
            "Epoch: 0107 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00928\n",
            "before feed\n",
            "Epoch: 0108 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00987\n",
            "before feed\n",
            "Epoch: 0109 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01002\n",
            "before feed\n",
            "Epoch: 0110 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00935\n",
            "before feed\n",
            "Epoch: 0111 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00934\n",
            "before feed\n",
            "Epoch: 0112 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01302\n",
            "before feed\n",
            "Epoch: 0113 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00989\n",
            "before feed\n",
            "Epoch: 0114 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01004\n",
            "before feed\n",
            "Epoch: 0115 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00934\n",
            "before feed\n",
            "Epoch: 0116 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00942\n",
            "before feed\n",
            "Epoch: 0117 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00955\n",
            "before feed\n",
            "Epoch: 0118 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00927\n",
            "before feed\n",
            "Epoch: 0119 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00922\n",
            "before feed\n",
            "Epoch: 0120 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00972\n",
            "before feed\n",
            "Epoch: 0121 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00977\n",
            "before feed\n",
            "Epoch: 0122 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00962\n",
            "before feed\n",
            "Epoch: 0123 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00982\n",
            "before feed\n",
            "Epoch: 0124 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00910\n",
            "before feed\n",
            "Epoch: 0125 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00932\n",
            "before feed\n",
            "Epoch: 0126 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00981\n",
            "before feed\n",
            "Epoch: 0127 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00975\n",
            "before feed\n",
            "Epoch: 0128 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00903\n",
            "before feed\n",
            "Epoch: 0129 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00964\n",
            "before feed\n",
            "Epoch: 0130 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01007\n",
            "before feed\n",
            "Epoch: 0131 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00996\n",
            "before feed\n",
            "Epoch: 0132 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01186\n",
            "before feed\n",
            "Epoch: 0133 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00965\n",
            "before feed\n",
            "Epoch: 0134 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01000\n",
            "before feed\n",
            "Epoch: 0135 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00935\n",
            "before feed\n",
            "Epoch: 0136 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00953\n",
            "before feed\n",
            "Epoch: 0137 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01009\n",
            "before feed\n",
            "Epoch: 0138 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00957\n",
            "before feed\n",
            "Epoch: 0139 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00986\n",
            "before feed\n",
            "Epoch: 0140 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00928\n",
            "before feed\n",
            "Epoch: 0141 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00930\n",
            "before feed\n",
            "Epoch: 0142 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01042\n",
            "before feed\n",
            "Epoch: 0143 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01227\n",
            "before feed\n",
            "Epoch: 0144 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00960\n",
            "before feed\n",
            "Epoch: 0145 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00962\n",
            "before feed\n",
            "Epoch: 0146 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00957\n",
            "before feed\n",
            "Epoch: 0147 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00951\n",
            "before feed\n",
            "Epoch: 0148 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00986\n",
            "before feed\n",
            "Epoch: 0149 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01026\n",
            "before feed\n",
            "Epoch: 0150 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00945\n",
            "before feed\n",
            "Epoch: 0151 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00924\n",
            "before feed\n",
            "Epoch: 0152 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01201\n",
            "before feed\n",
            "Epoch: 0153 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00916\n",
            "before feed\n",
            "Epoch: 0154 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01018\n",
            "before feed\n",
            "Epoch: 0155 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00987\n",
            "before feed\n",
            "Epoch: 0156 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01003\n",
            "before feed\n",
            "Epoch: 0157 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00998\n",
            "before feed\n",
            "Epoch: 0158 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00938\n",
            "before feed\n",
            "Epoch: 0159 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00945\n",
            "before feed\n",
            "Epoch: 0160 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00899\n",
            "before feed\n",
            "Epoch: 0161 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01690\n",
            "before feed\n",
            "Epoch: 0162 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01056\n",
            "before feed\n",
            "Epoch: 0163 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00978\n",
            "before feed\n",
            "Epoch: 0164 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01210\n",
            "before feed\n",
            "Epoch: 0165 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00985\n",
            "before feed\n",
            "Epoch: 0166 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00920\n",
            "before feed\n",
            "Epoch: 0167 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01556\n",
            "before feed\n",
            "Epoch: 0168 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01115\n",
            "before feed\n",
            "Epoch: 0169 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01148\n",
            "before feed\n",
            "Epoch: 0170 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01292\n",
            "before feed\n",
            "Epoch: 0171 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01100\n",
            "before feed\n",
            "Epoch: 0172 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00996\n",
            "before feed\n",
            "Epoch: 0173 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01032\n",
            "before feed\n",
            "Epoch: 0174 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00996\n",
            "before feed\n",
            "Epoch: 0175 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00957\n",
            "before feed\n",
            "Epoch: 0176 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00986\n",
            "before feed\n",
            "Epoch: 0177 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00979\n",
            "before feed\n",
            "Epoch: 0178 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00983\n",
            "before feed\n",
            "Epoch: 0179 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00969\n",
            "before feed\n",
            "Epoch: 0180 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00941\n",
            "before feed\n",
            "Epoch: 0181 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01358\n",
            "before feed\n",
            "Epoch: 0182 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00863\n",
            "before feed\n",
            "Epoch: 0183 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00950\n",
            "before feed\n",
            "Epoch: 0184 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01014\n",
            "before feed\n",
            "Epoch: 0185 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00984\n",
            "before feed\n",
            "Epoch: 0186 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00984\n",
            "before feed\n",
            "Epoch: 0187 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01013\n",
            "before feed\n",
            "Epoch: 0188 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00974\n",
            "before feed\n",
            "Epoch: 0189 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01253\n",
            "before feed\n",
            "Epoch: 0190 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01005\n",
            "before feed\n",
            "Epoch: 0191 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00936\n",
            "before feed\n",
            "Epoch: 0192 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01030\n",
            "before feed\n",
            "Epoch: 0193 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00968\n",
            "before feed\n",
            "Epoch: 0194 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00994\n",
            "before feed\n",
            "Epoch: 0195 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01021\n",
            "before feed\n",
            "Epoch: 0196 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00961\n",
            "before feed\n",
            "Epoch: 0197 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00954\n",
            "before feed\n",
            "Epoch: 0198 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01019\n",
            "before feed\n",
            "Epoch: 0199 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01191\n",
            "before feed\n",
            "Epoch: 0200 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00960\n",
            "Optimization Finished!\n",
            "Test set results: cost= nan accuracy= 0.80556 time= 0.00493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txjJqIaKFgIQ"
      },
      "source": [
        "feed_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt52Uy3TCBAx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS3SBY4fOadG"
      },
      "source": [
        "![Logo](https://uploads-ssl.webflow.com/5a72b3a09e04c7000168f4de/5ce6005b22f44fde8ced717c_MD%20Horizontal.png)\n",
        "\n",
        "\n",
        "# Octavian.ai machine learning on graphs course\n",
        "\n",
        "Welcome to our summer course on graph ML.\n",
        "\n",
        "This course is primarily exercise based - you'll learn through reading and writing code, and answering the questions throughout these exercises.\n",
        "\n",
        "[Join our Discord](https://discord.gg/a2Z82Te) to chat with fellow enthusiasts about this exercise and give us feedback to direct the next one.\n",
        "\n",
        "## Exercise 2, graph convolutional networks\n",
        "In this exercise, you will learn how to classify nodes in a graph. We'll do this by creating a graph network that passes messages along the edges of the graph.\n",
        "\n",
        "This technique is very versatile and with creativity can be applied to a wide range of graph problems.\n",
        "\n",
        "### Dataset\n",
        "\n",
        "We'll work with the popular [Cora](https://relational.fit.cvut.cz/dataset/CORA) dataset. Using a well known dataset makes this exercise easier as there are lots of existing solutions to look at if you run into trouble. Also, we know a solution is possible, which is not always the case in ML research.\n",
        "\n",
        "Cora is a academic paper citation graph. Its nodes are papers and the edges are citations between them. Each paper also comes with a set of mentioned topics, which we will use to help increase the network's classificiation accuracy.\n",
        "\n",
        "Each node has a classification label, which we will train our network to output:\n",
        "*\t\tCase_Based\n",
        "*\t\tGenetic_Algorithms\n",
        "*\t\tNeural_Networks\n",
        "*\t\tProbabilistic_Methods\n",
        "*\t\tReinforcement_Learning\n",
        "*\t\tRule_Learning\n",
        "*\t\tTheory\n",
        "\n",
        "Some statistics from the Relational Dataset Repository:\n",
        ">The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.\n",
        "\n",
        "### Introduction to Graph Convolutional Networks (GCN)\n",
        "\n",
        "A graph convolutional network (GCN) is a machine learning technique for graphs. In a GCN each node has an initial state and directed edge weights:\n",
        "\n",
        "![Illustration of initial graph state](https://uploads-ssl.webflow.com/5a72b3a09e04c7000168f4de/5d4f208e2108a9472bcd5f70_Graph%20illustrations%20(1).png)\n",
        "\n",
        "This initial state could be some known information about a node (in our case, the keywords of the paper), or it could be a fixed/learned/random value.\n",
        "\n",
        "Then a number (determined by the engineer) of GCN layers are executed. In each GCN layer:\n",
        "- Propagate each node's state to its neighbors along the graph's edges (a weighted sum by edge weight)\n",
        "- Then apply the same dense layer *W* to every node, with an optional activation function *σ*\n",
        "\n",
        "![illustration of propagation](https://uploads-ssl.webflow.com/5a72b3a09e04c7000168f4de/5d4f208b8ab9f5fc43094ed8_Graph%20illustrations.png)\n",
        "\n",
        "These two steps can be thought of as the eqivalent of a dense layer in a standard feedforward network, you get to design the architecture by making decisions such as:\n",
        "- How many of these GCN layers to stack\n",
        "- What size should the output node state be after each layer?\n",
        "- What activation function should each GCN layer use?\n",
        "- What regularisation should be applied?\n",
        "\n",
        "The result of this network is a state for each node in the graph. \n",
        "\n",
        "To use these node states as node classifications (as we shall in this exercise), softmax can be used as the final activation function. [This generates a probability-distribution-like vector for each node](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax).\n",
        "\n",
        "### Theoretical background\n",
        "\n",
        "Thomas Kipf has published [really excellent articles](https://tkipf.github.io/graph-convolutional-networks/) about this area of technology, and this tutorial is based off of his basic network structure [outlined here](https://tkipf.github.io/graph-convolutional-networks/).\n",
        "\n",
        "You're encouraged to read Thomas's articles to get the full background on this technique. This exercise focuses on the application of it, as opposed to the background and theory.\n",
        "\n",
        "This area of technology is still in its infancy; The capabilties of GCN have not been fully charted. Whilst working on this exercise, embrace a healthy relish for research and the unknown!\n",
        "\n",
        "### Exercise structure\n",
        "\n",
        "In this exercise you will create a fully functioning graph convolutional network. \n",
        "\n",
        "The exercise is a series of empty functions that you will fill out according to the instructions. There are then a series of unit tests to verify that your code works according to plan.\n",
        "\n",
        "# The exercise\n",
        "\n",
        "## Library setup\n",
        "\n",
        "We'll write our code in Tensorflow 2.0 and Keras. We'll also use Numpy and Scipy for some of the initial data manipulation. Let's load up all the libraries we'll need:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PTFYh33GYD2",
        "outputId": "49729bf6-2f58-4250-b6fc-ee4a47af25a4"
      },
      "source": [
        "# Import all the libraries we need\n",
        "!pip uninstall -q -y tensorflow\n",
        "!pip uninstall -q -y grpcio\n",
        " \n",
        "!pip install -q tensorflow\n",
        "!pip install -q grpcio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 320.4MB 55kB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6qCoY_vG4ia"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY9e6RkuGlbJ",
        "outputId": "e0f4ffe0-c3f4-44dc-b0a3-1030f476adfc"
      },
      "source": [
        "# %tensorflow_version 2.x"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESFlVsylONbD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d928ca-3e3c-4c4c-e2dc-2c054a645fa0"
      },
      "source": [
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        " \n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, backend as bk\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras import Model\n",
        " \n",
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        " \n",
        "import matplotlib.pyplot as plt \n",
        " \n",
        "import collections\n",
        "from collections import namedtuple\n",
        "import unittest\n",
        "import os\n",
        "import sys\n",
        " \n",
        "print(\"TensorFlow version: \", tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version:  2.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0QUSDq5pgUB"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "We're going to use the data as prepared in Thomas Kipf's TensorFlow GCN codebase. It's been neatly pickled so that we can easily load it from disk. The following code will download the data, the TensorFlow codebase, and load the data into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUQDQ_HTuoIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "551fd83b-b476-41bd-9585-154dfd6feb82"
      },
      "source": [
        "# Thanks to Thomas Kipf for sharing this and also for all his work\n",
        "# # researching and publicizing GCNs\n",
        "# !git clone https://github.com/tkipf/gcn.git\n",
        "\n",
        "# # Add the GCN repo to the import path\n",
        "# sys.path.append('/content/gcn')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'gcn' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4TdEMH3xrgU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80a4739-e499-4896-c31e-1b74ad13c10a"
      },
      "source": [
        "# !ls gcn/gcn/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t     inits.py\tmetrics.py  train.py\n",
            "__init__.py  layers.py\tmodels.py   utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVF8ADPAOCax"
      },
      "source": [
        "from scipy import sparse\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcsSfXzmNaVL"
      },
      "source": [
        "features = sparse.load_npz(\"features.npz\")\n",
        "adj = sparse.load_npz(\"adj.npz\")\n",
        "\n",
        "with open('array.npy', 'rb') as f:\n",
        "    y_train = np.load(f)\n",
        "    y_val = np.load(f)\n",
        "    y_test = np.load(f)\n",
        "    train_mask = np.load(f)\n",
        "    val_mask = np.load(f)\n",
        "    test_mask = np.load(f)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs-m-oE7Em8G"
      },
      "source": [
        "features=sp.lil_matrix(features)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKjWq_p1piBj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9250362e-04f8-435a-c1aa-f141c4f60562"
      },
      "source": [
        "# Load the data\n",
        "# %cd -q /content/gcn/gcn\n",
        "#from gcn.utils import load_data\n",
        "#adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
        "#adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data('cora')\n",
        "print(\"Loaded\",len(y_train),\"nodes\")\n",
        "print()\n",
        "print(\"-- Data format --\")\n",
        "print(\"Adj:       \", adj.shape,             type(adj), \"number of indices\", len(adj.indices))\n",
        "print(\"y_train:   \", y_train.shape, \"\\t\",   type(y_train))\n",
        "print(\"train_mask:\", train_mask.shape,\"\\t\", type(train_mask))\n",
        "print(\"features:\", features.shape,\"\\t\", type(features))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 182 nodes\n",
            "\n",
            "-- Data format --\n",
            "Adj:        (182, 182) <class 'scipy.sparse.csr.csr_matrix'> number of indices 9906\n",
            "y_train:    (182, 2) \t <class 'numpy.ndarray'>\n",
            "train_mask: (182,) \t <class 'numpy.ndarray'>\n",
            "features: (182, 129) \t <class 'scipy.sparse.lil.lil_matrix'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwV4hsJFd_1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6198146b-a93a-463b-a8a1-d7bbfed93f59"
      },
      "source": [
        "print(train_mask)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True  True  True  True  True  True  True  True False False False  True\n",
            " False  True False False  True False False False  True  True  True False\n",
            "  True  True  True False False  True  True  True False False  True False\n",
            "  True False False  True False False False False False False  True  True\n",
            "  True False  True  True False  True False  True  True False  True  True\n",
            "  True  True  True  True  True  True  True False  True  True  True False\n",
            " False False  True  True False  True  True False False  True False False\n",
            "  True  True  True False  True  True False False  True False  True  True\n",
            " False  True False  True  True False  True False False  True  True False\n",
            "  True  True  True False  True False  True  True False  True  True  True\n",
            "  True  True False False  True  True  True False  True  True False  True\n",
            " False  True  True  True False  True  True  True  True  True False  True\n",
            " False  True  True  True  True False  True  True False False  True  True\n",
            "  True  True  True False False False False False False False  True  True\n",
            "  True  True False  True  True False  True False  True  True  True False\n",
            " False False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Iwdivy7-8CD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dbf4fb7-a5e4-4013-b614-335f3d5e5c47"
      },
      "source": [
        "np.unique(test_mask,return_counts=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([False,  True]), array([146,  36]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU_V6Zupr1TZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e433d4b8-de31-450d-bdca-644ef504cf01"
      },
      "source": [
        "y_val[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wMowHJEE_IF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06289ccc-0320-4459-8e76-e566debfcac5"
      },
      "source": [
        "np.unique(train_mask,return_counts=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([False,  True]), array([ 73, 109]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbGX3KEWryO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf07441-8790-496e-df48-7ed9b7695437"
      },
      "source": [
        "adj.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(182, 182)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DWlpTWACRGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c1a2303-9a50-4218-bfba-ed01511563a7"
      },
      "source": [
        "y_train[138:150,]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1],\n",
              "       [1, 0],\n",
              "       [0, 1],\n",
              "       [0, 1],\n",
              "       [0, 0],\n",
              "       [1, 0],\n",
              "       [1, 0],\n",
              "       [0, 1],\n",
              "       [0, 1],\n",
              "       [0, 1],\n",
              "       [0, 1],\n",
              "       [0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40VPNWyaB75E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fdf6217-ae38-46c2-cf02-a3606cd0191a"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(182, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH_uxt4PCrS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e73a129a-6877-4326-cc11-219b38135dd6"
      },
      "source": [
        "np.any(y_train,1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True,  True,  True,  True,  True, False,\n",
              "       False, False,  True,  True,  True, False,  True,  True, False,\n",
              "       False, False,  True,  True,  True, False,  True,  True,  True,\n",
              "       False,  True,  True,  True,  True, False, False,  True,  True,\n",
              "        True, False, False,  True, False, False, False, False, False,\n",
              "       False,  True,  True,  True, False,  True,  True, False,  True,\n",
              "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True, False,  True,  True,  True, False,\n",
              "       False, False,  True,  True, False,  True,  True,  True,  True,\n",
              "        True, False,  True,  True,  True,  True, False,  True,  True,\n",
              "       False,  True,  True, False,  True,  True,  True,  True, False,\n",
              "        True,  True, False,  True, False, False,  True,  True, False,\n",
              "        True,  True,  True, False,  True, False,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
              "        True, False,  True,  True, False,  True, False,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
              "        True,  True,  True,  True,  True, False,  True,  True, False,\n",
              "       False,  True,  True,  True,  True,  True, False, False,  True,\n",
              "        True, False,  True, False,  True,  True,  True,  True, False,\n",
              "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
              "       False, False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl_l--35r20G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f728ea8c-da03-4d90-910d-ab2846fc8b04"
      },
      "source": [
        "np.unique(np.nonzero(y_train)[1], return_counts=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), array([61, 67]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnrBXlIMV61w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b05fdc1-5bdd-4526-b50e-b7de80105fb1"
      },
      "source": [
        "g7features = sp.lil_matrix(np.random.randint(0,2,(2708,1433)).astype(np.float32))\n",
        "print(\"features:\", features.shape,\"\\t\", type(features))\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features: (182, 129) \t <class 'scipy.sparse.lil.lil_matrix'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx00AWImwKxl"
      },
      "source": [
        "## Data preparation\n",
        "\n",
        "The adjacency matrix is a representation of all the edges in the graph. For each node *i* and *j* in the graph, if they have an edge from *i* to *j* then `adj[i][j] == 1.0`, else `0.0`. Here's an example:\n",
        "\n",
        "![Adjacency matrix format](https://uploads-ssl.webflow.com/5a72b3a09e04c7000168f4de/5d508c94a4a3707d208a52d3_Matrix%20illustration.png)\n",
        "\n",
        "We need to do a little preparation of the adjacency matrix so it'll work well with our GCN layers.\n",
        "\n",
        "First, we need to add self-edges. This allows each node to propagate state back to itself, which allows nodes to retain information.\n",
        "\n",
        "Secondly, the adjacency matrix needs to be [normalised](). This ensures that when node states are propagated during the GCN layer, the size of the result is the average of the neighbors (instead of an ever-increasing sum, depending on the number of incoming edges a node has).\n",
        "\n",
        "The [degree matrix](https://en.wikipedia.org/wiki/Degree_matrix) measures how many edges each node has. We'll essentially divide the adjacency matrix by that so it does not grow the size of the node states each iteration.\n",
        "\n",
        "We'll use the following symmetric normalization technique, which has been noted for its [useful dynamics](https://arxiv.org/abs/1609.02907):\n",
        "\n",
        "<img src=\"https://uploads-ssl.webflow.com/5a72b3a09e04c7000168f4de/5d508ecc3691c96989e3d4f0_ex2%20sym%20normalize.png\" width=\"400px\"/>\n",
        "\n",
        "Note that the adjacency matrix is [sparse](https://en.wikipedia.org/wiki/Sparse_matrix): instead of storing every value in a *2708 × 2708 × sizeof(float)* memory matrix (hint: that's a lot of memory!), just the non-zero values are stored as a list. Its internal structure is a [list of tuples](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html), each with a numeric value and a coordinate of where in the matrix that value appears.\n",
        "\n",
        "Storing it sparsely greatly reduces the memory footprint of the matrix, making it easier to store, move and process. It does have one downside however: TensorFlow's library function support for sparse matrices is still quite limited.\n",
        "\n",
        "Numpy and Scipy have a richer set of functions for manipulating sparse matrices, so we'll use those to normalize the adjacency matrix prior to feeding it into Tensorflow-Keras.\n",
        "\n",
        "The following functions will be helpful:\n",
        "- `adj.sum(axis)` to get the sum along an axis (hint: the initial adjacency matrix has 1s where there are edges, so summing it along an axis generates the degrees as a vector - which you could diagonalize into a matrix)\n",
        "- `np.power(vector, power)` to square / square-root a matrix\n",
        "- `m[np.isinf(m)] = 0.0` lets you trip infinite values from a matrix, which could occur if you square-root zero\n",
        "- `sp.diags(m)` to get the [diagonal](https://en.wikipedia.org/wiki/Diagonal_matrix) of a matrix\n",
        "- `m.dot(n)` to multiply two matrices together\n",
        "- `sp.eye(m.shape[0])` to get an [identity matrix](https://en.wikipedia.org/wiki/Identity_matrix) the same size as a square matrix `m` - useful for adding self-edges\n",
        "- `m.astype(dtype)` To cast your result to the desired type\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZnYb_CHZAqX"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FEg5eu51c3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c4a357-b0ea-4ed2-9caa-c9fc73f2e8ac"
      },
      "source": [
        "# Takes a scipy csr matrix, returns a csr matrix\n",
        "def sym_normalize_matrix(adj, dtype=np.float32):\n",
        "  '''\n",
        "  Parameters:\n",
        "    adj: The matrix to normalize\n",
        "    dtype: The desired output dtype (e.g. the type of the values in the sparse matrix)\n",
        "  '''\n",
        "  # Apply the matrix normalization D^(-1/2) x A x D^(-1/2) where D is the degree and A the adjacency\n",
        "  \n",
        "  # --- WRITE CODE HERE ---\n",
        "  D = np.array(np.power(adj.sum(1).transpose(),-1/2))[0]\n",
        "  for i in range(len(D)):\n",
        "    if np.isinf(D[i]):\n",
        "      D[i] = 0.0\n",
        "  D = sp.diags(D)\n",
        "  return D.dot(adj).dot(D).astype(np.float32)\n",
        "  \n",
        "\n",
        "# Takes a scipy csr matrix, returns a csr matrix\n",
        "def prepare_adj_matrix(adj):\n",
        "  # Add self-edges to adj, then apply sym_normaliize_matrix to the result\n",
        "  \n",
        "  # --- WRITE CODE HERE ---\n",
        "  return sym_normalize_matrix(sp.eye(adj.shape[0]) + adj)\n",
        "  \n",
        "prepared_adj = prepare_adj_matrix(adj)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Tests to validate your code\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class TestAdjNormalization(unittest.TestCase):\n",
        "\n",
        "  def assert_csr_close(self, result, expected):\n",
        "    np.testing.assert_allclose(result.indices, expected.indices, err_msg=\"Indices mismatch\")\n",
        "    np.testing.assert_allclose(result.data,  expected.data, err_msg=\"Values mismatch\")\n",
        "    self.assertEqual(result.shape, expected.shape, \"Shape mismatch\")\n",
        "  \n",
        "  def test_prepare(self):\n",
        "    with unittest.mock.patch('__main__.sym_normalize_matrix') as mock_norm:\n",
        "      test_value = sp.csr.csr_matrix(np.array([[0,1],[0,0]], np.float32))\n",
        "      test_value_exp = sp.csr.csr_matrix(np.array([[1,1],[0,1]], np.float32))\n",
        "\n",
        "      result = prepare_adj_matrix(test_value)\n",
        "\n",
        "      assert mock_norm.call_args is not None, \"sym_normalize_matrix should be called\"\n",
        "      args, kwargs = mock_norm.call_args\n",
        "      self.assert_csr_close(args[0], test_value_exp)\n",
        "\n",
        "      print(\"test_prepare success!\")\n",
        "\n",
        "  def test_normalization(self):\n",
        "    \n",
        "    adj = sp.csr_matrix(np.array([\n",
        "        [1.0, 1.0, 0.0],\n",
        "        [1.0, 1.0, 1.0],\n",
        "        [0.0, 0.0, 1.0]\n",
        "    ]))\n",
        "       \n",
        "    result = sym_normalize_matrix(adj, np.float32)\n",
        "    \n",
        "    expected_indices = np.array([[0, 0], [1, 0],      [0, 1],     [1, 1],     [2, 1],     [2, 2]])\n",
        "    expected_values  = np.array([ 0.5,    0.40824829,  0.40824829, 0.33333333, 0.57735027, 1.   ])\n",
        "    expected_shape   = (3, 3)\n",
        "\n",
        "    expected = sp.csr_matrix(np.array([\n",
        "      [0.5, 0.40824829, 0.0],\n",
        "      [0.40824829, 0.33333333, 0.57735027],\n",
        "      [0.0, 0.0, 1.0]\n",
        "    ]))\n",
        "    \n",
        "    self.assert_csr_close(result, expected)\n",
        "    assert result.dtype == np.float32, \"Result of sym_normalize_matrix should have dtype float32, got \" + str(result.dtype)\n",
        "    \n",
        "    print(\"test_normalization success!\")\n",
        "\n",
        "TestAdjNormalization().test_prepare()\n",
        "TestAdjNormalization().test_normalization()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_prepare success!\n",
            "test_normalization success!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in power\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGUjyl7ZuqRV"
      },
      "source": [
        "## Keras layer\n",
        "\n",
        "Now for the exciting part, let's build a GCN layer in Keras that we can use to construct a graph convolutional network.\n",
        "\n",
        "The layer takes the *node_state* as the incoming tensor, then transforms that into a new *node_state*. It performs the following operations (many akin to a standard Dense layer):\n",
        "\n",
        "1.   [Dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)\n",
        "2.   Graph convolution (i.e. multiplying the node state by the weights matrix)\n",
        "3.   Graph propagation (i.e. multiplying the adjacency matrix by the node state)\n",
        "4.   [Activation function](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)\n",
        "\n",
        "#### Graph convolution\n",
        "This is convolution in the sense that the same parameters are being applied to each node state. This is in the form of a shared matrix, which transforms each node state just as a [dense layer](https://towardsdatascience.com/building-neural-network-from-scratch-9c88535bf8e9) would transform the activations in a feed-forward network.\n",
        "\n",
        "#### Implementation details\n",
        "The layer takes the adjacency matrix (the sparse matrix representing graph connectivity) as a parameter in its constructor. The adjacency matrix does not change during training or testing as our graph is static.\n",
        "\n",
        "I've provided the main scaffold for the layer, initialising the weights and constructing the object.\n",
        "\n",
        "#### Your work is to fill out the **call** method of the class *GCNLayer*. I've included comments, and test cases to verify your implementation.\n",
        "\n",
        "Useful functions:\n",
        "- `tf.sparse.sparse_dense_matmul` to multiply a sparse tensor by a dense tensor\n",
        "- `tf.matmul` to multiply a dense tensor by a dense tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9SuJ4FCw0Ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3205e4b2-415a-4747-bafd-7db65d1ae2d7"
      },
      "source": [
        "class GCNLayer(keras.layers.Layer):\n",
        " \n",
        "  def __init__(self, adjacency, units, activation=tf.identity, dropout=0.0, l2=0.0, dtype=tf.float32, name=None):\n",
        "    '''\n",
        "    Params:\n",
        "      Adjacency: a tf.SparseTensor adjacency matrix\n",
        "      Units: The number of output units per node state\n",
        "      Activation: The activation function to apply to the node states\n",
        "      Dropout: The amount of dropout (0.0 being none, 1.0 being all units) to apply\n",
        "      l2: The amount of L2 regularisation to apply\n",
        "      dtype: The type of values in the tensors this layer will transform\n",
        "      name: The name of this layer \n",
        "    '''\n",
        "    super(GCNLayer, self).__init__(dtype=dtype, name=name)\n",
        "    \n",
        "    self.adjacency = adjacency\n",
        "    self.units = units\n",
        "    self.activation = activation\n",
        "    self.dropout = dropout\n",
        "    self.l2 = l2\n",
        "    \n",
        "    assert isinstance(adjacency, tf.SparseTensor), \"Adjacency matrix should be a SparseTensor\"\n",
        "    assert adjacency.dtype == self.dtype, \"Adjacency matrix not expected dtype, got \" + str(adjacency.dtype) + \" expected \" + str(self.dtype)\n",
        "    \n",
        "  def build(self, input_shape):\n",
        "    '''\n",
        "    This method is called during the initial compilation of our model. Its \n",
        "    primary job is to initialize the weights for this layer.\n",
        " \n",
        "    Params:\n",
        "      input_shape: this is the shape of the input to the layer, in our case an \n",
        "                   array of (NUMBER_NODES, NODE_STATE_SIZE)\n",
        " \n",
        "    We build one weight, w, which will be applied to each node_state. We initialize\n",
        "    it from the uniform distribution, scaled by the size of the matrix. We apply\n",
        "    l2 loss to regularize the matrix\n",
        "    '''\n",
        "    self.w = self.add_weight( \n",
        "      shape=(input_shape[1], self.units),\n",
        "      dtype=self.dtype,\n",
        "      initializer='glorot_uniform',\n",
        "      regularizer=keras.regularizers.l2(self.l2)\n",
        "    )\n",
        "  \n",
        "    \n",
        "  def call(self, node_state):\n",
        "    '''\n",
        "    This method is called to apply the layer to an incoming tensor. This is the\n",
        "    real meat of the model.\n",
        " \n",
        "    Params:\n",
        "      node_state: The tf.Tensor of node states.  Shape (NUMBER_NODES, NODE_STATE_SIZE)\n",
        " \n",
        "    Returns: The transformed node state tf.Tensor\n",
        "    '''\n",
        "    \n",
        "    assert isinstance(node_state, tf.Tensor), \"Layer input should be a Tensor, got \" + str(type(node_state))\n",
        "    assert node_state.dtype == self.dtype, \"Input to layer \" + str(self.name) + \" wrong dtype, got \" + str(node_state.dtype) + \" expected \" + str(self.dtype)\n",
        "    tf.debugging.check_numerics(node_state, \"Input to layer \" + str(self.name) + \" has numerical instability\")\n",
        " \n",
        "    # Apply dropout to the node_state, using the self.dropout as the factor\n",
        "    \n",
        "    # --- WRITE CODE HERE ---\n",
        "    node_state = tf.nn.dropout(node_state, rate=self.dropout)\n",
        "    #node_state = layers.Dropout(node_state,rate=self.dropout)\n",
        "    # Apply the node convolution: This means to matrix multiply each node_state by our learned parameters `self.w`\n",
        "    \n",
        "    # --- WRITE CODE HERE ---\n",
        "    node_state = tf.matmul(node_state, self.w)\n",
        " \n",
        "    # Apply the graph propagation: This means to multiply the\n",
        "    # normalized adjacency matrix by the node state\n",
        "    # You can do this as a single sparse_dense_matmul()\n",
        "    \n",
        "    # --- WRITE CODE HERE ---\n",
        "    node_state = tf.sparse.sparse_dense_matmul(self.adjacency, node_state)\n",
        " \n",
        "    # Apply the activation function `self.activation`\n",
        "    \n",
        "    # --- WRITE CODE HERE ---\n",
        "    node_state = self.activation(node_state)\n",
        " \n",
        "    tf.debugging.check_numerics(node_state, \"Output of layer \" + str(self.name) + \" has numerical instability\")\n",
        " \n",
        "    return node_state"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_propagate_node_state Success!\n",
            "test_apply_convolution Success!\n",
            "test_layer Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYRaDxMY-9hH"
      },
      "source": [
        "## Keras model\n",
        "\n",
        "Now we have a working GCN layer, let's put it to work in a Keras model. Keras provides a simple interface for doing what we want to do, the Sequential model\n",
        "format. It stacks multiple layers linearly, passing the output of one as the input to the next.\n",
        "\n",
        "We're going to build the following network architecture - two layers, with the following parameters:\n",
        "\n",
        "1.   Output units = 16, activation = relu\n",
        "2.   Output units = number of different labels (7), activation = softmax\n",
        "\n",
        "We'll give each layer our prepared adjacency matrix from earlier.\n",
        "\n",
        "I've provided hyper-parameters for [L2](https://developers.google.com/machine-learning/glossary/#L2_regularization) and [dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ragAHoJ3_8Ts"
      },
      "source": [
        "L2_FACTOR = 5e-1\n",
        "DROPOUT_FACTOR = 0.1\n",
        "\n",
        "NODE_COUNT   = adj.shape[0]\n",
        "NUM_CLASSES  = 2\n",
        "CLASS_LABELS = list(range(NUM_CLASSES))\n",
        "\n",
        "# Transform our Scipy CSR matrix into a tensorflow SparseTensor\n",
        "coo = prepared_adj.tocoo()\n",
        "indices = np.array(list(zip(coo.row, coo.col)))\n",
        "tf_adj = tf.SparseTensor(indices=indices, values=tf.cast(prepared_adj.data, tf.float32), dense_shape=prepared_adj.shape)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# The model\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  # --- WRITE CODE HERE ---\n",
        "  GCNLayer(tf_adj, units=64, activation=keras.backend.relu, dropout=DROPOUT_FACTOR),\n",
        "  GCNLayer(tf_adj, units=64, activation=keras.backend.relu, dropout=DROPOUT_FACTOR),\n",
        "  #GCNLayer(tf_adj, units=32, activation=keras.backend.relu, dropout=DROPOUT_FACTOR),\n",
        "  keras.layers.Dense(units=NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "# ------------------------------------------------------------------------------\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr2nUPxkAxiB"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Finally, let's train the model. Keras makes this simple for us, with one call to `compile` then `fit`.\n",
        "\n",
        "I've provided the optimizer and loss functions - they're a fairly common setup, using [Adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) as the optimizer and [categorical cross-entropy](https://gombru.github.io/2018/05/23/cross_entropy_loss/) for the loss calculation.\n",
        "\n",
        "I've provided a helper class to monitor accuracy for us, which I've wired up in the metrics (because of how our data is structured, the normal Keras metrics won't correctly measure accuracy. We need to apply our label mask in both the loss and accuracy calculations, the custom metric below will do this).\n",
        "\n",
        "I've also provided a few graphs - you should try adding more graphs to see the inner workings of the model. Graphing is a valuable research skill."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYw5XEthB9rr"
      },
      "source": [
        "# Metric to measure accuracy of our model, optionally by class.\n",
        "\n",
        "# This is needed as the way we're treating our data doesn't fit the Keras\n",
        "# metrics. In a normal ML flow you have seperate lists of examples (e.g. input \n",
        "# and expected label) for training, testing and validation. You feed one of these\n",
        "# lists in to your training/evaluation loop and measure the model's loss and\n",
        "# accuracy on those examples.\n",
        "\n",
        "# Because our network is a graph, the input to the network is a tensor of node states\n",
        "# and we need to pass the entire set of node states in for the adjacency matrix\n",
        "# to match the shape of the node state matrix. We cannot just pass in the training\n",
        "# nodes and their labels.\n",
        "\n",
        "# Therefore instead we mask the labels output by the network to just the training\n",
        "# or testing set of labels, and measure their accuracy. Keras's sample_weight\n",
        "# mechanism doesn't get applied to accuracy metrics or during testing, therefore\n",
        "# I've implemented our own metric.\n",
        "\n",
        "# This metric has one additional optional feature: It will calculate accuracy for \n",
        "# a single class.\n",
        "\n",
        "# It's important to watch the accuracy by each class label to check the network\n",
        "# is discriminating between them. If a network is struggling to train, one common\n",
        "# failure case is it predicts the same class for all labels as an easy way to \n",
        "# decrease loss. This often presents itself as a train accuracy of 100%/NUM_CLASSES.\n",
        "\n",
        "# By watching individual class acurracies, we can see if the network is learning\n",
        "# to predict each class, or sacrificing some/all classes for one class.\n",
        "\n",
        "class AccuracyByClass(keras.metrics.Metric):\n",
        "  def __init__(self, name, dtype, class_label=None, sample_weight=None, y_true=None):\n",
        "    ''' \n",
        "    Parameters:\n",
        "      name: The name of this metric\n",
        "      dtype: The type of the data being measured\n",
        "      class_label: (Optional) If you supply this, the accuracy will be measured for just that class. Otherwise overall accuracy is measured\n",
        "      sample_weight: The mask you want to apply to the labels\n",
        "      y_true: (Optional) The correct values for the labels output by the network\n",
        "\n",
        "    '''\n",
        "    super().__init__(name, dtype)\n",
        "    \n",
        "    self.class_label = class_label\n",
        "    self.sample_weight = tf.cast(sample_weight, dtype)\n",
        "    self.y_true = y_true\n",
        "\n",
        "    self.correct = tf.Variable(initial_value=0.0, dtype=self._dtype, trainable=False, name='correct_'+str(class_label))\n",
        "    self.total   = tf.Variable(initial_value=0.0, dtype=self._dtype, trainable=False, name='total_'+str(class_label)) \n",
        "    \n",
        "  def reset_states(self):\n",
        "    self.correct.assign(0)\n",
        "    self.total.assign(0)\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    # Note that Keras doesn't pass sample_weight into its metrics\n",
        "    # during testing\n",
        "\n",
        "    if self.y_true is not None:\n",
        "      y_true = self.y_true\n",
        "\n",
        "    y_true_classes = tf.argmax(y_true, axis=-1)\n",
        "    y_pred_classes = tf.argmax(y_pred, axis=-1)\n",
        "    correct = tf.equal(y_pred_classes, y_true_classes)\n",
        "\n",
        "    # Create mask\n",
        "    if self.class_label is not None:\n",
        "      mask = tf.cast(tf.equal(y_true_classes, self.class_label), self._dtype)\n",
        "    else:\n",
        "      mask = tf.ones(tf.shape(y_true_classes), self._dtype, 'mask_class_true_ones')\n",
        "    \n",
        "    sample_weight = self.sample_weight\n",
        "\n",
        "    if sample_weight is not None:\n",
        "      mask *= sample_weight\n",
        "\n",
        "    # Apply mask\n",
        "    masked_total_count = tf.reduce_sum(mask)\n",
        "    self.total.assign_add(masked_total_count)\n",
        "\n",
        "    masked_correct = mask * tf.cast(correct, self._dtype)\n",
        "    masked_correct_count = tf.reduce_sum(masked_correct)\n",
        "    self.correct.assign_add(masked_correct_count)\n",
        "  \n",
        "    return self.result()\n",
        "      \n",
        "\n",
        "  def result(self):\n",
        "    return tf.math.divide_no_nan(self.correct, self.total)\n",
        "  "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imjm5o2XBsHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d5d1a1-cbbc-493d-bad0-0a8ec12d032f"
      },
      "source": [
        "EPOCH_COUNT   = 200 # Determined by experiment\n",
        "LEARNING_RATE = 0.01 # Learning rate determined through experimentation\n",
        "\n",
        "# Loss function and optimizer\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE) \n",
        "\n",
        "metrics = [\n",
        "  AccuracyByClass(\"train_accuracy\", tf.float32, sample_weight=train_mask, y_true=y_train),\n",
        "  AccuracyByClass(\"test_accuracy\",  tf.float32, sample_weight=test_mask,  y_true=y_test),\n",
        "  AccuracyByClass(\"val_accuracy\",   tf.float32, sample_weight=val_mask,   y_true=y_val)\n",
        "]\n",
        "\n",
        "# for label in CLASS_LABELS:\n",
        "#   metrics.append(AccuracyByClass(\"train_class_acc_\"+str(label), tf.float32, label, sample_weight=train_mask, y_true=y_train))\n",
        "#   metrics.append(AccuracyByClass(\"test_class_acc_\"+str(label),  tf.float32, label, sample_weight=test_mask,  y_true=y_test))\n",
        "\n",
        "# Fix the random seeds prior to compiling and training the model - this helps make\n",
        "# results reproduceable\n",
        "np.random.seed(13)\n",
        "tf.random.set_seed(13)\n",
        "\n",
        "# Generate weights, setup optimizer and loss function:\n",
        "model.compile(optimizer, loss=loss_fn, metrics=metrics)\n",
        "\n",
        "initial_state = features.todense()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    initial_state, \n",
        "    y_train, \n",
        "    sample_weight=tf.cast(train_mask, tf.float32), # This will be used in loss calculations\n",
        "    validation_data=(initial_state, y_val, val_mask),\n",
        "    epochs=EPOCH_COUNT, \n",
        "    batch_size=NODE_COUNT, # This is unusual in ML - since our adjacency matrix is the whole graph, we want to feed in the whole node_state array in each training step\n",
        "    verbose=1,\n",
        "    shuffle=False # Do not shuffle the order of our input data, since its order matches up to the adjacency matrix\n",
        ")\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0691 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0691 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0691 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0691 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0691 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0691 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0691 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0691 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0691 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0691 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0691 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0691 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0691 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4145 - train_accuracy: 0.5229 - test_accuracy: 0.2500 - val_accuracy: 0.2222 - val_loss: 0.0692 - val_train_accuracy: 0.5229 - val_test_accuracy: 0.2500 - val_val_accuracy: 0.2222\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gcn_layer_11 (GCNLayer)      (182, 64)                 8256      \n",
            "_________________________________________________________________\n",
            "gcn_layer_12 (GCNLayer)      (182, 64)                 4096      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (182, 2)                  130       \n",
            "=================================================================\n",
            "Total params: 12,482\n",
            "Trainable params: 12,482\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1Ymv8M6EUx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9048f432-37e8-4d25-e23c-9b6e25a6cdf7"
      },
      "source": [
        "np.nonzero(initial_state)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  0,   0,   0, ..., 181, 181, 181]),\n",
              " array([  0,   1,   2, ..., 126, 127, 128]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqikApUyF_HE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "outputId": "aaf6d46f-6fa6-47a2-cfae-ed9fad97f1bc"
      },
      "source": [
        "# Display graphs of how the model performed\n",
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "\n",
        "def smooth(key):\n",
        "  return gaussian_filter1d(history.history[key], sigma=2)\n",
        "\n",
        "print(\"Final loss: \", history.history['loss'][-1])\n",
        "print(\"Final training accuracy: \", round(history.history['train_accuracy'][-1]*100), \"%\")\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [15, 10]\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title(\"Training loss\")\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(smooth('train_accuracy'))\n",
        "plt.title(\"Training accuracy\")\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(smooth('val_accuracy'))\n",
        "plt.title(\"Validation accuracy\")\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "for label in CLASS_LABELS:\n",
        "  plt.plot(smooth(\"train_class_acc_\"+str(label)))\n",
        "plt.title(\"Training accuracy by class\")\n",
        "plt.ylabel('Class accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['class_'+str(label) for label in CLASS_LABELS], loc='lower right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final loss:  0.4144962728023529\n",
            "Final training accuracy:  52 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x720 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyPF1j5JGUrt"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxJyhEQfGO-u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "47420960-00e9-4d00-9f16-beaa96e510ea"
      },
      "source": [
        "# Apply the trained model to our test data (that is, the held-out node labels)\n",
        "# and measure how it performs\n",
        "results = model.evaluate(\n",
        "    initial_state, \n",
        "    y_test, \n",
        "    steps=1, \n",
        "    batch_size=NODE_COUNT, \n",
        "    verbose=0)\n",
        "\n",
        "# Get test metrics\n",
        "results_dict = dict(zip(model.metrics_names, results))\n",
        "for name in model.metrics_names:\n",
        "  if \"test\" not in name:\n",
        "    del results_dict[name]\n",
        "\n",
        "# Add train accuracy from the earlier training history\n",
        "results_dict[\"train_accuracy\"] = history.history['train_accuracy'][-1]\n",
        "\n",
        "results_keys = list(results_dict.keys())\n",
        "results_values = [results_dict[key] for key in results_keys]\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [15, 6]\n",
        "\n",
        "# Display a bar chart\n",
        "y_pos = np.arange(len(results_dict))\n",
        "plt.barh(y_pos, results_values, align='center', alpha=0.5)\n",
        "plt.yticks(y_pos, results_keys)\n",
        "plt.ylabel('Percentage')\n",
        "plt.title('Evaluation')\n",
        "plt.show()\n",
        "\n",
        "results_values"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.25, 0.0, 1.0, 0.5229358077049255]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjlBnqU30QiX"
      },
      "source": [
        "## Exercise questions and next steps\n",
        "\n",
        "\n",
        "[Add your answers and discoveries to the answer document](https://docs.google.com/document/d/1QdAEOYnJ5AwFczNQZk5ZZo1Ng7_shn33GzQPdumnsvI/edit?usp=sharing)\n",
        "\n",
        "\n",
        "- What train and test accuracy did you get?\n",
        "\n",
        "- What modifications did you try to the network? How did they perform?\n",
        "\n",
        "- Are there other graphs you find it useful to produce?\n",
        "\n",
        "- What is the theoretical capabilities of this network? Do we need other methods of graph machine learning?\n",
        "\n",
        "- Find an example of a Graph Convolutional Network being used in industry, link to it here and provide a summary\n",
        "\n",
        "- How would you scale this network to a larger graph? What challenges might you encounter?\n",
        "\n",
        "- Imagine you’re applying this method to Twitter’s tweet-reply graph. It’s constantly changing. How could you apply this method (which currently is written for a static graph)?\n",
        "\n",
        "- How could you apply this to language (e.g. how could you treat language as a graph?)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bwy5rkD3j5vC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}