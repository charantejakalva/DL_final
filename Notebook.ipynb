{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hV4t2RaaIHD",
        "outputId": "de4ea3ce-d89a-4011-ca0c-3f373b63f036"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gNqocrl-iR-",
        "outputId": "71af2d5f-5659-40e8-b4c9-efdb1f48afd6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAAs2VvI-rq9",
        "outputId": "97aed9f9-d125-459a-a8c5-ec51800b4141"
      },
      "source": [
        "!ls '/content/drive/MyDrive/Colab_Notebooks/DL_final/'\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BuzzFeed\t       feature_matrix_pf.csv\t\t  UserFeature.mat\n",
            "BuzzFeedNewsUser.txt   news_news_bf_adjacency_matrix.csv  User.txt\n",
            "BuzzFeedUserUser.txt   news_news_pf_adjacency_matrix.csv\n",
            "feature_matrix_bf.csv  News.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGzwLDX8BScF",
        "outputId": "c58d702b-1f7b-4081-e6c1-9e1bb1f162be"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzqOUINPaC0t",
        "outputId": "48157b29-626c-4260-f179-9d1aad0b02f7"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import scipy.sparse as sp\n",
        "import pandas as pd \n",
        "import random\n",
        "import json\n",
        "import os\n",
        "\n",
        "from feature_matrix import FeatureMatrix\n",
        "\n",
        "import pandas as pd \n",
        "import tensorflow_hub as hub\n",
        "from bert import run_classifier\n",
        "from bert import tokenization\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92T1ds1zaC0u"
      },
      "source": [
        "FM = FeatureMatrix(base_path = \"/content/drive/MyDrive/Colab_Notebooks/DL_final//\")\n",
        "label_zip = None"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcNDwKB-aC0u"
      },
      "source": [
        " def getAdj( dataset=\"BuzzFeed\"):\n",
        "        if dataset == \"BuzzFeed\":\n",
        "            adj_np = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/DL_final/news_news_bf_adjacency_matrix.csv\", header=None).values\n",
        "        else:\n",
        "            adj_np = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/DL_final/news_news_pf_adjacency_matrix.csv\", header=None).values\n",
        "\n",
        "        return sp.csr_matrix(adj_np, dtype=int)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZgYEexvaC0u"
      },
      "source": [
        "    def getFeatures( dataset=\"BuzzFeed\"):\n",
        "        feature_df =  FM.get_feature_matrix(dataset)\n",
        "        label = feature_df['label'].tolist()\n",
        "        label_comp = [0 if each else 1 for each in label]\n",
        "        global label_zip\n",
        "        label_zip = list(zip(label_comp, label))\n",
        "        feature_df.drop(['label'], axis=1)\n",
        "        feature_np = feature_df.values\n",
        "\n",
        "        return sp.csr_matrix(feature_np, dtype=float).tolil()\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5HwPAnPaC0u"
      },
      "source": [
        "    def getYs(  dataset=\"BuzzFeed\"):\n",
        "        if dataset == \"BuzzFeed\":\n",
        "            random.seed(1)\n",
        "        else:\n",
        "            random.seed(1)\n",
        "        yTrain =  label_zip[:]\n",
        "        yVal =  label_zip[:]\n",
        "        yTest =  label_zip[:]\n",
        "        train_mask = [False] * len(yTrain)\n",
        "        val_mask = [False] * len(yTrain)\n",
        "        test_mask = [False] * len(yTrain)\n",
        "        n = len(yTrain)\n",
        "\n",
        "        set_of_records_range = set(range(n))\n",
        "\n",
        "        train_range = set(random.sample(set_of_records_range, k=int(n * 0.6)))\n",
        "        set_of_records_range = set_of_records_range - train_range\n",
        "\n",
        "        val_range = set(random.sample(set_of_records_range, k=int(n * 0.2)))\n",
        "        set_of_records_range = set_of_records_range - train_range\n",
        "\n",
        "        test_range = set(random.sample(set_of_records_range, k=int(n * 0.2)))\n",
        "\n",
        "        for i in train_range:\n",
        "            yVal[i] = (0,0)\n",
        "            yTest[i] = (0,0)\n",
        "            train_mask[i] = True\n",
        "        for i in val_range:\n",
        "            yTrain[i] = (0,0)\n",
        "            yTest[i] = (0,0)\n",
        "            val_mask[i] = True\n",
        "        for i in test_range:\n",
        "            yVal[i] = (0,0)\n",
        "            yTrain[i] = (0,0)\n",
        "            test_mask[i] = True\n",
        "\n",
        "        return yTrain, yVal, yTest, train_mask, val_mask, test_mask\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMzRtZ8saC0u"
      },
      "source": [
        "    def getComps(  dataset=\"BuzzFeed\"):\n",
        "        print(dataset)\n",
        "        adj =  getAdj(dataset)\n",
        "        features =  getFeatures(dataset)\n",
        "\n",
        "        yTrain, yVal, yTest, train_mask, val_mask, test_mask =  getYs(dataset)\n",
        "\n",
        "        return adj, features, yTrain, yVal, yTest, train_mask, val_mask, test_mask\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG12H5OpaC0u"
      },
      "source": [
        "def sparse_to_tuple(sparse_mx):\n",
        "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjFlJY85aC0u"
      },
      "source": [
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1, dtype=float).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return features.todense(), sparse_to_tuple(features)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H2st7PUaC0u"
      },
      "source": [
        "def adj_to_bias(adj, sizes, nhood=1):\n",
        "    nb_graphs = adj.shape[0]\n",
        "    mt = np.empty(adj.shape)\n",
        "    for g in range(nb_graphs):\n",
        "        mt[g] = np.eye(adj.shape[1])\n",
        "        for _ in range(nhood):\n",
        "            mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))\n",
        "        for i in range(sizes[g]):\n",
        "            for j in range(sizes[g]):\n",
        "                if mt[g][i][j] > 0.0:\n",
        "                    mt[g][i][j] = 1.0\n",
        "    return -1e9 * (1.0 - mt)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Zlg_4RaC0u"
      },
      "source": [
        "\n",
        "# adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = getComps(dataset='BuzzFeed')\n",
        "# features, spars = preprocess_features(features)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twKeogQQDuX_",
        "outputId": "abe17ce0-4811-402e-8f7a-84c550a45b16"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import tensorflow as tf \n",
        "from  utils import *\n",
        "from models import GCN, MLP\n",
        "\n",
        "# Set random seed\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "# Settings\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_string('f', '', 'kernel')\n",
        "# flags.DEFINE_string('dataset', 'cora', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
        "# flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
        "# flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
        "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
        "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
        "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
        "\n",
        "# Load data\n",
        "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = getComps(dataset='BuzzFeed')\n",
        "features=sp.csr_matrix(features)\n",
        "# Some preprocessing\n",
        "features = preprocess_features(features)\n",
        "# if FLAGS.model == 'gcn':\n",
        "support = [preprocess_adj(adj)]\n",
        "num_supports = 1\n",
        "model_func = GCN\n",
        "# elif FLAGS.model == 'gcn_cheby':\n",
        "#     support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
        "#     num_supports = 1 + FLAGS.max_degree\n",
        "#     model_func = GCN\n",
        "# elif FLAGS.model == 'dense':\n",
        "#     support = [preprocess_adj(adj)]  # Not used\n",
        "#     num_supports = 1\n",
        "#     model_func = MLP\n",
        "# else:\n",
        "#     raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
        "\n",
        "y_train  = np.asarray(y_train)\n",
        "y_val = np.asarray(y_val)\n",
        "y_test = np.asarray(y_test)\n",
        "train_mask  = np.asarray(train_mask)\n",
        "val_mask  = np.asarray(val_mask)\n",
        "test_mask  = np.asarray(test_mask)\n",
        "\n",
        "# Define placeholders\n",
        "placeholders = {\n",
        "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
        "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
        "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
        "    'labels_mask': tf.placeholder(tf.int32),\n",
        "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
        "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
        "}\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BuzzFeed\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/feature_matrix.py:49: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/feature_matrix.py:49: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 182\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 182\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] friday morning around 4 : 00 , three men – some of whom were armed – barge ##d in through one of the doors of an atlanta resident ’ s house . the resident ’ s co - worker was over for work - related reasons , and when she heard the commotion , she sprung into action , wielding a handgun . the whole episode was caught on surveillance camera that was set up inside the house . the resident ’ s co - worker un ##loaded all the rounds from her gun into the intruder ##s ’ direction , sending all three of them out the door . one of the intruder ##s even jumped through a glass door . two intruder ##s are [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] friday morning around 4 : 00 , three men – some of whom were armed – barge ##d in through one of the doors of an atlanta resident ’ s house . the resident ’ s co - worker was over for work - related reasons , and when she heard the commotion , she sprung into action , wielding a handgun . the whole episode was caught on surveillance camera that was set up inside the house . the resident ’ s co - worker un ##loaded all the rounds from her gun into the intruder ##s ’ direction , sending all three of them out the door . one of the intruder ##s even jumped through a glass door . two intruder ##s are [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 5958 2851 2105 1018 1024 4002 1010 2093 2273 1516 2070 1997 3183 2020 4273 1516 19398 2094 1999 2083 2028 1997 1996 4303 1997 2019 5865 6319 1521 1055 2160 1012 1996 6319 1521 1055 2522 1011 7309 2001 2058 2005 2147 1011 3141 4436 1010 1998 2043 2016 2657 1996 23960 1010 2016 22057 2046 2895 1010 26974 1037 28497 1012 1996 2878 2792 2001 3236 2006 9867 4950 2008 2001 2275 2039 2503 1996 2160 1012 1996 6319 1521 1055 2522 1011 7309 4895 17468 2035 1996 6241 2013 2014 3282 2046 1996 22841 2015 1521 3257 1010 6016 2035 2093 1997 2068 2041 1996 2341 1012 2028 1997 1996 22841 2015 2130 5598 2083 1037 3221 2341 1012 2048 22841 2015 2024 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 5958 2851 2105 1018 1024 4002 1010 2093 2273 1516 2070 1997 3183 2020 4273 1516 19398 2094 1999 2083 2028 1997 1996 4303 1997 2019 5865 6319 1521 1055 2160 1012 1996 6319 1521 1055 2522 1011 7309 2001 2058 2005 2147 1011 3141 4436 1010 1998 2043 2016 2657 1996 23960 1010 2016 22057 2046 2895 1010 26974 1037 28497 1012 1996 2878 2792 2001 3236 2006 9867 4950 2008 2001 2275 2039 2503 1996 2160 1012 1996 6319 1521 1055 2522 1011 7309 4895 17468 2035 1996 6241 2013 2014 3282 2046 1996 22841 2015 1521 3257 1010 6016 2035 2093 1997 2068 2041 1996 2341 1012 2028 1997 1996 22841 2015 2130 5598 2083 1037 3221 2341 1012 2048 22841 2015 2024 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] story highlights a protest ##er pie ##d sacramento ' s mayor in the face at a charity event wednesday the two sc ##uf ##fle ##d afterward , and the protest ##er was taken to a hospital for stitches ( cnn ) kevin johnson - - the nba star - turned - sacramento mayor - - was pie ##d in the face by a man at a charity dinner wednesday night at the high school he once attended . but the pie - throw ##er appeared to get the worst of it . sean thompson , 32 , approached the mayor , pulled a pie out of the bag and shoving it in his face - - setting off a short sc ##uf ##fle , in which [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] story highlights a protest ##er pie ##d sacramento ' s mayor in the face at a charity event wednesday the two sc ##uf ##fle ##d afterward , and the protest ##er was taken to a hospital for stitches ( cnn ) kevin johnson - - the nba star - turned - sacramento mayor - - was pie ##d in the face by a man at a charity dinner wednesday night at the high school he once attended . but the pie - throw ##er appeared to get the worst of it . sean thompson , 32 , approached the mayor , pulled a pie out of the bag and shoving it in his face - - setting off a short sc ##uf ##fle , in which [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2466 11637 1037 6186 2121 11345 2094 11932 1005 1055 3664 1999 1996 2227 2012 1037 5952 2724 9317 1996 2048 8040 16093 21031 2094 9707 1010 1998 1996 6186 2121 2001 2579 2000 1037 2902 2005 25343 1006 13229 1007 4901 3779 1011 1011 1996 6452 2732 1011 2357 1011 11932 3664 1011 1011 2001 11345 2094 1999 1996 2227 2011 1037 2158 2012 1037 5952 4596 9317 2305 2012 1996 2152 2082 2002 2320 3230 1012 2021 1996 11345 1011 5466 2121 2596 2000 2131 1996 5409 1997 2009 1012 5977 5953 1010 3590 1010 5411 1996 3664 1010 2766 1037 11345 2041 1997 1996 4524 1998 15866 2009 1999 2010 2227 1011 1011 4292 2125 1037 2460 8040 16093 21031 1010 1999 2029 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2466 11637 1037 6186 2121 11345 2094 11932 1005 1055 3664 1999 1996 2227 2012 1037 5952 2724 9317 1996 2048 8040 16093 21031 2094 9707 1010 1998 1996 6186 2121 2001 2579 2000 1037 2902 2005 25343 1006 13229 1007 4901 3779 1011 1011 1996 6452 2732 1011 2357 1011 11932 3664 1011 1011 2001 11345 2094 1999 1996 2227 2011 1037 2158 2012 1037 5952 4596 9317 2305 2012 1996 2152 2082 2002 2320 3230 1012 2021 1996 11345 1011 5466 2121 2596 2000 2131 1996 5409 1997 2009 1012 5977 5953 1010 3590 1010 5411 1996 3664 1010 2766 1037 11345 2041 1997 1996 4524 1998 15866 2009 1999 2010 2227 1011 1011 4292 2125 1037 2460 8040 16093 21031 1010 1999 2029 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 0 shares facebook twitter bernard sans ##ari ##c ##q , former president of the haitian senate , issued a b ##list ##ering statement condemning the clinton foundation , which has been posted at donald trump ’ s campaign website . sans ##ari ##c ##q ’ s statement says : sadly , when an earthquake rocked the nation of haiti in 2010 , corruption moved in faster than the help so desperately needed . today , the people of haiti are still suffering despite the billions of dollars that have flowed into the clinton foundation . the clinton ##s exploited this terrible disaster to steal billions of dollars from the sick and starving people of haiti . the world trusted the clinton ##s to help the haitian [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 0 shares facebook twitter bernard sans ##ari ##c ##q , former president of the haitian senate , issued a b ##list ##ering statement condemning the clinton foundation , which has been posted at donald trump ’ s campaign website . sans ##ari ##c ##q ’ s statement says : sadly , when an earthquake rocked the nation of haiti in 2010 , corruption moved in faster than the help so desperately needed . today , the people of haiti are still suffering despite the billions of dollars that have flowed into the clinton foundation . the clinton ##s exploited this terrible disaster to steal billions of dollars from the sick and starving people of haiti . the world trusted the clinton ##s to help the haitian [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1014 6661 9130 10474 6795 20344 8486 2278 4160 1010 2280 2343 1997 1996 21404 4001 1010 3843 1037 1038 9863 7999 4861 28525 1996 7207 3192 1010 2029 2038 2042 6866 2012 6221 8398 1521 1055 3049 4037 1012 20344 8486 2278 4160 1521 1055 4861 2758 1024 13718 1010 2043 2019 8372 14215 1996 3842 1997 12867 1999 2230 1010 7897 2333 1999 5514 2084 1996 2393 2061 9652 2734 1012 2651 1010 1996 2111 1997 12867 2024 2145 6114 2750 1996 25501 1997 6363 2008 2031 13230 2046 1996 7207 3192 1012 1996 7207 2015 18516 2023 6659 7071 2000 8954 25501 1997 6363 2013 1996 5305 1998 18025 2111 1997 12867 1012 1996 2088 9480 1996 7207 2015 2000 2393 1996 21404 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1014 6661 9130 10474 6795 20344 8486 2278 4160 1010 2280 2343 1997 1996 21404 4001 1010 3843 1037 1038 9863 7999 4861 28525 1996 7207 3192 1010 2029 2038 2042 6866 2012 6221 8398 1521 1055 3049 4037 1012 20344 8486 2278 4160 1521 1055 4861 2758 1024 13718 1010 2043 2019 8372 14215 1996 3842 1997 12867 1999 2230 1010 7897 2333 1999 5514 2084 1996 2393 2061 9652 2734 1012 2651 1010 1996 2111 1997 12867 2024 2145 6114 2750 1996 25501 1997 6363 2008 2031 13230 2046 1996 7207 3192 1012 1996 7207 2015 18516 2023 6659 7071 2000 8954 25501 1997 6363 2013 1996 5305 1998 18025 2111 1997 12867 1012 1996 2088 9480 1996 7207 2015 2000 2393 1996 21404 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] the democrats are using an intimidation tactic which they are prone to use inc ##ess ##antly . if you question the vera ##city of a black president ’ s birth certificate you are racist . that is the outright claim of lynn sweet washington bureau chief for the chicago sun - times made on fox news . consider the slant ##ed claim that if a president is black and his birth certificate is called into question the conclusion is you must be a racist . what is wrong with this obvious non - se ##qui ##tur ? first of all , there is evidence the birth certificate presented was altered . this is discount ##ed by the claims that whoever presented this must be racist . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] the democrats are using an intimidation tactic which they are prone to use inc ##ess ##antly . if you question the vera ##city of a black president ’ s birth certificate you are racist . that is the outright claim of lynn sweet washington bureau chief for the chicago sun - times made on fox news . consider the slant ##ed claim that if a president is black and his birth certificate is called into question the conclusion is you must be a racist . what is wrong with this obvious non - se ##qui ##tur ? first of all , there is evidence the birth certificate presented was altered . this is discount ##ed by the claims that whoever presented this must be racist . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1996 8037 2024 2478 2019 28973 19717 2029 2027 2024 13047 2000 2224 4297 7971 15706 1012 2065 2017 3160 1996 12297 12972 1997 1037 2304 2343 1521 1055 4182 8196 2017 2024 16939 1012 2008 2003 1996 13848 4366 1997 9399 4086 2899 4879 2708 2005 1996 3190 3103 1011 2335 2081 2006 4419 2739 1012 5136 1996 27474 2098 4366 2008 2065 1037 2343 2003 2304 1998 2010 4182 8196 2003 2170 2046 3160 1996 7091 2003 2017 2442 2022 1037 16939 1012 2054 2003 3308 2007 2023 5793 2512 1011 7367 15549 20689 1029 2034 1997 2035 1010 2045 2003 3350 1996 4182 8196 3591 2001 8776 1012 2023 2003 19575 2098 2011 1996 4447 2008 9444 3591 2023 2442 2022 16939 1012 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1996 8037 2024 2478 2019 28973 19717 2029 2027 2024 13047 2000 2224 4297 7971 15706 1012 2065 2017 3160 1996 12297 12972 1997 1037 2304 2343 1521 1055 4182 8196 2017 2024 16939 1012 2008 2003 1996 13848 4366 1997 9399 4086 2899 4879 2708 2005 1996 3190 3103 1011 2335 2081 2006 4419 2739 1012 5136 1996 27474 2098 4366 2008 2065 1037 2343 2003 2304 1998 2010 4182 8196 2003 2170 2046 3160 1996 7091 2003 2017 2442 2022 1037 16939 1012 2054 2003 3308 2007 2023 5793 2512 1011 7367 15549 20689 1029 2034 1997 2035 1010 2045 2003 3350 1996 4182 8196 3591 2001 8776 1012 2023 2003 19575 2098 2011 1996 4447 2008 9444 3591 2023 2442 2022 16939 1012 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] breaking : pipe bombs found in new jersey train station first a pipe bomb exploded on the jersey shore , then new york was next . now , pipe bombs have been found in a new jersey train station . according to the new york daily news , “ authorities discovered three pipe bombs and two smaller devices at a train station in elizabeth . ” new jersey : elizabeth mayor says a bag with wires and pipes was found in a trash can near nj ##t station , fbi on scene . https : / / t . co / q ##60 ##l ##ggs ##52 ##o — ko ##l ##ha ##ola ##m ( @ ko ##l ##ha ##ola ##m ) september 19 , 2016 since [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] breaking : pipe bombs found in new jersey train station first a pipe bomb exploded on the jersey shore , then new york was next . now , pipe bombs have been found in a new jersey train station . according to the new york daily news , “ authorities discovered three pipe bombs and two smaller devices at a train station in elizabeth . ” new jersey : elizabeth mayor says a bag with wires and pipes was found in a trash can near nj ##t station , fbi on scene . https : / / t . co / q ##60 ##l ##ggs ##52 ##o — ko ##l ##ha ##ola ##m ( @ ko ##l ##ha ##ola ##m ) september 19 , 2016 since [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4911 1024 8667 9767 2179 1999 2047 3933 3345 2276 2034 1037 8667 5968 9913 2006 1996 3933 5370 1010 2059 2047 2259 2001 2279 1012 2085 1010 8667 9767 2031 2042 2179 1999 1037 2047 3933 3345 2276 1012 2429 2000 1996 2047 2259 3679 2739 1010 1523 4614 3603 2093 8667 9767 1998 2048 3760 5733 2012 1037 3345 2276 1999 3870 1012 1524 2047 3933 1024 3870 3664 2758 1037 4524 2007 14666 1998 12432 2001 2179 1999 1037 11669 2064 2379 19193 2102 2276 1010 8495 2006 3496 1012 16770 1024 1013 1013 1056 1012 2522 1013 1053 16086 2140 21314 25746 2080 1517 12849 2140 3270 6030 2213 1006 1030 12849 2140 3270 6030 2213 1007 2244 2539 1010 2355 2144 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4911 1024 8667 9767 2179 1999 2047 3933 3345 2276 2034 1037 8667 5968 9913 2006 1996 3933 5370 1010 2059 2047 2259 2001 2279 1012 2085 1010 8667 9767 2031 2042 2179 1999 1037 2047 3933 3345 2276 1012 2429 2000 1996 2047 2259 3679 2739 1010 1523 4614 3603 2093 8667 9767 1998 2048 3760 5733 2012 1037 3345 2276 1999 3870 1012 1524 2047 3933 1024 3870 3664 2758 1037 4524 2007 14666 1998 12432 2001 2179 1999 1037 11669 2064 2379 19193 2102 2276 1010 8495 2006 3496 1012 16770 1024 1013 1013 1056 1012 2522 1013 1053 16086 2140 21314 25746 2080 1517 12849 2140 3270 6030 2213 1006 1030 12849 2140 3270 6030 2213 1007 2244 2539 1010 2355 2144 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "/content/utils.py:126: RuntimeWarning: invalid value encountered in power\n",
            "  d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmxwMSiIYPkg"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d9BAW9aOgnD",
        "outputId": "dc3412c5-95f6-4fd5-a542-b0baaaf7422f"
      },
      "source": [
        "    print('adj shape', type(adj))\n",
        "    print('features shape', type(features))\n",
        "    print('y_train shape', type(y_train))\n",
        "    print('y_val shape', type(y_val))\n",
        "    print('y_test shape', type(y_test))\n",
        "    print('train_mask shape', type(train_mask))\n",
        "    print('val_mask shape', type(val_mask))\n",
        "    print('test_mask shape', type(test_mask)) "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adj shape <class 'scipy.sparse.csr.csr_matrix'>\n",
            "features shape <class 'tuple'>\n",
            "y_train shape <class 'numpy.ndarray'>\n",
            "y_val shape <class 'numpy.ndarray'>\n",
            "y_test shape <class 'numpy.ndarray'>\n",
            "train_mask shape <class 'numpy.ndarray'>\n",
            "val_mask shape <class 'numpy.ndarray'>\n",
            "test_mask shape <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J4pE-cCbeKN",
        "outputId": "04a7cbc7-d752-4cd8-dbf3-b1fc88e6c0d3"
      },
      "source": [
        "train_mask.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(182,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn574ijFOxCI"
      },
      "source": [
        "adj shape <class 'scipy.sparse.csr.csr_matrix'>\n",
        "features shape <class 'scipy.sparse.lil.lil_matrix'>\n",
        "y_train shape <class 'numpy.ndarray'>\n",
        "y_val shape <class 'numpy.ndarray'>\n",
        "y_test shape <class 'numpy.ndarray'>\n",
        "train_mask shape <class 'numpy.ndarray'>\n",
        "val_mask shape <class 'numpy.ndarray'>\n",
        "test_mask shape <class 'numpy.ndarray'>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wcJj2F1G2PW",
        "outputId": "3ac35e3b-f61b-4509-f6fe-52d87fb5d55c"
      },
      "source": [
        "\n",
        "# Create model\n",
        "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
        "\n",
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "\n",
        "\n",
        "# Define model evaluation function\n",
        "def evaluate(features, support, labels, mask, placeholders):\n",
        "    t_test = time.time()\n",
        "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
        "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
        "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
        "\n",
        "\n",
        "# Init variables\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "cost_val = []\n",
        "\n",
        "# Train model\n",
        "for epoch in range(FLAGS.epochs):\n",
        "\n",
        "    t = time.time()\n",
        "    # Construct feed dictionary\n",
        "    print('before feed')\n",
        "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
        "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
        "\n",
        "    # Training step\n",
        "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
        "\n",
        "    # Validation\n",
        "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
        "    cost_val.append(cost)\n",
        "\n",
        "    # Print results\n",
        "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
        "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
        "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
        "        print(\"Early stopping...\")\n",
        "        break\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "\n",
        "# Testing\n",
        "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
        "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
        "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/layers.py:170: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/layers.py:170: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "before feed\n",
            "Epoch: 0001 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.30413\n",
            "before feed\n",
            "Epoch: 0002 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01034\n",
            "before feed\n",
            "Epoch: 0003 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01057\n",
            "before feed\n",
            "Epoch: 0004 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01009\n",
            "before feed\n",
            "Epoch: 0005 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01023\n",
            "before feed\n",
            "Epoch: 0006 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01063\n",
            "before feed\n",
            "Epoch: 0007 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00961\n",
            "before feed\n",
            "Epoch: 0008 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00915\n",
            "before feed\n",
            "Epoch: 0009 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00991\n",
            "before feed\n",
            "Epoch: 0010 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00949\n",
            "before feed\n",
            "Epoch: 0011 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00939\n",
            "before feed\n",
            "Epoch: 0012 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01016\n",
            "before feed\n",
            "Epoch: 0013 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00977\n",
            "before feed\n",
            "Epoch: 0014 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00929\n",
            "before feed\n",
            "Epoch: 0015 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00962\n",
            "before feed\n",
            "Epoch: 0016 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00957\n",
            "before feed\n",
            "Epoch: 0017 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01002\n",
            "before feed\n",
            "Epoch: 0018 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00945\n",
            "before feed\n",
            "Epoch: 0019 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00988\n",
            "before feed\n",
            "Epoch: 0020 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01398\n",
            "before feed\n",
            "Epoch: 0021 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00901\n",
            "before feed\n",
            "Epoch: 0022 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00992\n",
            "before feed\n",
            "Epoch: 0023 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01030\n",
            "before feed\n",
            "Epoch: 0024 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00938\n",
            "before feed\n",
            "Epoch: 0025 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01002\n",
            "before feed\n",
            "Epoch: 0026 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01017\n",
            "before feed\n",
            "Epoch: 0027 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01484\n",
            "before feed\n",
            "Epoch: 0028 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01056\n",
            "before feed\n",
            "Epoch: 0029 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00936\n",
            "before feed\n",
            "Epoch: 0030 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00978\n",
            "before feed\n",
            "Epoch: 0031 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00983\n",
            "before feed\n",
            "Epoch: 0032 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01162\n",
            "before feed\n",
            "Epoch: 0033 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00994\n",
            "before feed\n",
            "Epoch: 0034 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01013\n",
            "before feed\n",
            "Epoch: 0035 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01093\n",
            "before feed\n",
            "Epoch: 0036 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01268\n",
            "before feed\n",
            "Epoch: 0037 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01062\n",
            "before feed\n",
            "Epoch: 0038 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01672\n",
            "before feed\n",
            "Epoch: 0039 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01162\n",
            "before feed\n",
            "Epoch: 0040 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01119\n",
            "before feed\n",
            "Epoch: 0041 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01051\n",
            "before feed\n",
            "Epoch: 0042 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01127\n",
            "before feed\n",
            "Epoch: 0043 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01139\n",
            "before feed\n",
            "Epoch: 0044 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00995\n",
            "before feed\n",
            "Epoch: 0045 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00992\n",
            "before feed\n",
            "Epoch: 0046 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01034\n",
            "before feed\n",
            "Epoch: 0047 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00939\n",
            "before feed\n",
            "Epoch: 0048 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00947\n",
            "before feed\n",
            "Epoch: 0049 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00916\n",
            "before feed\n",
            "Epoch: 0050 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01027\n",
            "before feed\n",
            "Epoch: 0051 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00949\n",
            "before feed\n",
            "Epoch: 0052 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00929\n",
            "before feed\n",
            "Epoch: 0053 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00941\n",
            "before feed\n",
            "Epoch: 0054 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01012\n",
            "before feed\n",
            "Epoch: 0055 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00976\n",
            "before feed\n",
            "Epoch: 0056 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00952\n",
            "before feed\n",
            "Epoch: 0057 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01245\n",
            "before feed\n",
            "Epoch: 0058 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00952\n",
            "before feed\n",
            "Epoch: 0059 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00965\n",
            "before feed\n",
            "Epoch: 0060 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01090\n",
            "before feed\n",
            "Epoch: 0061 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00994\n",
            "before feed\n",
            "Epoch: 0062 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01022\n",
            "before feed\n",
            "Epoch: 0063 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01113\n",
            "before feed\n",
            "Epoch: 0064 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01215\n",
            "before feed\n",
            "Epoch: 0065 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01497\n",
            "before feed\n",
            "Epoch: 0066 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01489\n",
            "before feed\n",
            "Epoch: 0067 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01182\n",
            "before feed\n",
            "Epoch: 0068 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01111\n",
            "before feed\n",
            "Epoch: 0069 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01578\n",
            "before feed\n",
            "Epoch: 0070 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01020\n",
            "before feed\n",
            "Epoch: 0071 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01052\n",
            "before feed\n",
            "Epoch: 0072 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01011\n",
            "before feed\n",
            "Epoch: 0073 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01040\n",
            "before feed\n",
            "Epoch: 0074 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01267\n",
            "before feed\n",
            "Epoch: 0075 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01115\n",
            "before feed\n",
            "Epoch: 0076 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00993\n",
            "before feed\n",
            "Epoch: 0077 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01114\n",
            "before feed\n",
            "Epoch: 0078 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01095\n",
            "before feed\n",
            "Epoch: 0079 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00998\n",
            "before feed\n",
            "Epoch: 0080 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00963\n",
            "before feed\n",
            "Epoch: 0081 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00949\n",
            "before feed\n",
            "Epoch: 0082 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01013\n",
            "before feed\n",
            "Epoch: 0083 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00990\n",
            "before feed\n",
            "Epoch: 0084 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00945\n",
            "before feed\n",
            "Epoch: 0085 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00956\n",
            "before feed\n",
            "Epoch: 0086 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00993\n",
            "before feed\n",
            "Epoch: 0087 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01010\n",
            "before feed\n",
            "Epoch: 0088 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00991\n",
            "before feed\n",
            "Epoch: 0089 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00992\n",
            "before feed\n",
            "Epoch: 0090 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01033\n",
            "before feed\n",
            "Epoch: 0091 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00870\n",
            "before feed\n",
            "Epoch: 0092 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00972\n",
            "before feed\n",
            "Epoch: 0093 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01360\n",
            "before feed\n",
            "Epoch: 0094 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01071\n",
            "before feed\n",
            "Epoch: 0095 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01012\n",
            "before feed\n",
            "Epoch: 0096 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01350\n",
            "before feed\n",
            "Epoch: 0097 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00992\n",
            "before feed\n",
            "Epoch: 0098 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00958\n",
            "before feed\n",
            "Epoch: 0099 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00990\n",
            "before feed\n",
            "Epoch: 0100 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01012\n",
            "before feed\n",
            "Epoch: 0101 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00979\n",
            "before feed\n",
            "Epoch: 0102 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01067\n",
            "before feed\n",
            "Epoch: 0103 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00973\n",
            "before feed\n",
            "Epoch: 0104 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00944\n",
            "before feed\n",
            "Epoch: 0105 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00953\n",
            "before feed\n",
            "Epoch: 0106 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00907\n",
            "before feed\n",
            "Epoch: 0107 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00928\n",
            "before feed\n",
            "Epoch: 0108 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00987\n",
            "before feed\n",
            "Epoch: 0109 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01002\n",
            "before feed\n",
            "Epoch: 0110 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00935\n",
            "before feed\n",
            "Epoch: 0111 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00934\n",
            "before feed\n",
            "Epoch: 0112 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01302\n",
            "before feed\n",
            "Epoch: 0113 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00989\n",
            "before feed\n",
            "Epoch: 0114 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01004\n",
            "before feed\n",
            "Epoch: 0115 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00934\n",
            "before feed\n",
            "Epoch: 0116 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00942\n",
            "before feed\n",
            "Epoch: 0117 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00955\n",
            "before feed\n",
            "Epoch: 0118 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00927\n",
            "before feed\n",
            "Epoch: 0119 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00922\n",
            "before feed\n",
            "Epoch: 0120 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00972\n",
            "before feed\n",
            "Epoch: 0121 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00977\n",
            "before feed\n",
            "Epoch: 0122 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00962\n",
            "before feed\n",
            "Epoch: 0123 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00982\n",
            "before feed\n",
            "Epoch: 0124 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00910\n",
            "before feed\n",
            "Epoch: 0125 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00932\n",
            "before feed\n",
            "Epoch: 0126 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00981\n",
            "before feed\n",
            "Epoch: 0127 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00975\n",
            "before feed\n",
            "Epoch: 0128 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00903\n",
            "before feed\n",
            "Epoch: 0129 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00964\n",
            "before feed\n",
            "Epoch: 0130 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01007\n",
            "before feed\n",
            "Epoch: 0131 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00996\n",
            "before feed\n",
            "Epoch: 0132 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01186\n",
            "before feed\n",
            "Epoch: 0133 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00965\n",
            "before feed\n",
            "Epoch: 0134 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01000\n",
            "before feed\n",
            "Epoch: 0135 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00935\n",
            "before feed\n",
            "Epoch: 0136 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00953\n",
            "before feed\n",
            "Epoch: 0137 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01009\n",
            "before feed\n",
            "Epoch: 0138 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00957\n",
            "before feed\n",
            "Epoch: 0139 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00986\n",
            "before feed\n",
            "Epoch: 0140 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00928\n",
            "before feed\n",
            "Epoch: 0141 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00930\n",
            "before feed\n",
            "Epoch: 0142 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01042\n",
            "before feed\n",
            "Epoch: 0143 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01227\n",
            "before feed\n",
            "Epoch: 0144 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00960\n",
            "before feed\n",
            "Epoch: 0145 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00962\n",
            "before feed\n",
            "Epoch: 0146 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00957\n",
            "before feed\n",
            "Epoch: 0147 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00951\n",
            "before feed\n",
            "Epoch: 0148 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00986\n",
            "before feed\n",
            "Epoch: 0149 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01026\n",
            "before feed\n",
            "Epoch: 0150 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00945\n",
            "before feed\n",
            "Epoch: 0151 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00924\n",
            "before feed\n",
            "Epoch: 0152 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01201\n",
            "before feed\n",
            "Epoch: 0153 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00916\n",
            "before feed\n",
            "Epoch: 0154 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01018\n",
            "before feed\n",
            "Epoch: 0155 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00987\n",
            "before feed\n",
            "Epoch: 0156 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01003\n",
            "before feed\n",
            "Epoch: 0157 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00998\n",
            "before feed\n",
            "Epoch: 0158 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00938\n",
            "before feed\n",
            "Epoch: 0159 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00945\n",
            "before feed\n",
            "Epoch: 0160 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00899\n",
            "before feed\n",
            "Epoch: 0161 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01690\n",
            "before feed\n",
            "Epoch: 0162 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01056\n",
            "before feed\n",
            "Epoch: 0163 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00978\n",
            "before feed\n",
            "Epoch: 0164 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01210\n",
            "before feed\n",
            "Epoch: 0165 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00985\n",
            "before feed\n",
            "Epoch: 0166 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00920\n",
            "before feed\n",
            "Epoch: 0167 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01556\n",
            "before feed\n",
            "Epoch: 0168 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01115\n",
            "before feed\n",
            "Epoch: 0169 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01148\n",
            "before feed\n",
            "Epoch: 0170 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01292\n",
            "before feed\n",
            "Epoch: 0171 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01100\n",
            "before feed\n",
            "Epoch: 0172 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00996\n",
            "before feed\n",
            "Epoch: 0173 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01032\n",
            "before feed\n",
            "Epoch: 0174 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00996\n",
            "before feed\n",
            "Epoch: 0175 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00957\n",
            "before feed\n",
            "Epoch: 0176 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00986\n",
            "before feed\n",
            "Epoch: 0177 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00979\n",
            "before feed\n",
            "Epoch: 0178 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00983\n",
            "before feed\n",
            "Epoch: 0179 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00969\n",
            "before feed\n",
            "Epoch: 0180 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00941\n",
            "before feed\n",
            "Epoch: 0181 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01358\n",
            "before feed\n",
            "Epoch: 0182 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00863\n",
            "before feed\n",
            "Epoch: 0183 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00950\n",
            "before feed\n",
            "Epoch: 0184 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01014\n",
            "before feed\n",
            "Epoch: 0185 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00984\n",
            "before feed\n",
            "Epoch: 0186 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00984\n",
            "before feed\n",
            "Epoch: 0187 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01013\n",
            "before feed\n",
            "Epoch: 0188 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00974\n",
            "before feed\n",
            "Epoch: 0189 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01253\n",
            "before feed\n",
            "Epoch: 0190 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01005\n",
            "before feed\n",
            "Epoch: 0191 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00936\n",
            "before feed\n",
            "Epoch: 0192 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01030\n",
            "before feed\n",
            "Epoch: 0193 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00968\n",
            "before feed\n",
            "Epoch: 0194 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00994\n",
            "before feed\n",
            "Epoch: 0195 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01021\n",
            "before feed\n",
            "Epoch: 0196 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00961\n",
            "before feed\n",
            "Epoch: 0197 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00954\n",
            "before feed\n",
            "Epoch: 0198 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01019\n",
            "before feed\n",
            "Epoch: 0199 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.01191\n",
            "before feed\n",
            "Epoch: 0200 train_loss= nan train_acc= 0.48624 val_loss= nan val_acc= 0.83333 time= 0.00960\n",
            "Optimization Finished!\n",
            "Test set results: cost= nan accuracy= 0.80556 time= 0.00493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txjJqIaKFgIQ",
        "outputId": "6bfe8545-a61b-444f-e689-8799ed5ccd26"
      },
      "source": [
        "feed_dict"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7f34688da630>: (array([[  0,   0],\n",
              "         [  7,   0],\n",
              "         [ 23,   0],\n",
              "         ...,\n",
              "         [178, 181],\n",
              "         [180, 181],\n",
              "         [181, 181]], dtype=int32),\n",
              "  array([0.04651163, 0.01921301, 0.03327792, ...,        nan,        nan,\n",
              "                nan]),\n",
              "  (182, 182)),\n",
              " <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7f3403e64588>: (array([[  0, 127],\n",
              "         [  0, 126],\n",
              "         [  0, 125],\n",
              "         ...,\n",
              "         [181,   2],\n",
              "         [181,   1],\n",
              "         [181,   0]], dtype=int32),\n",
              "  array([0.00020374, 0.00404288, 0.0040249 , ..., 0.00597616, 0.0135185 ,\n",
              "         0.00023126]),\n",
              "  (182, 129)),\n",
              " <tf.Tensor 'PlaceholderWithDefault:0' shape=() dtype=float32>: 0.5,\n",
              " <tf.Tensor 'Placeholder_5:0' shape=(?, 2) dtype=float32>: array([[1, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [0, 0]]),\n",
              " <tf.Tensor 'Placeholder_6:0' shape=<unknown> dtype=int32>: array([ True,  True,  True,  True,  True,  True,  True,  True, False,\n",
              "        False, False,  True, False,  True, False, False,  True, False,\n",
              "        False, False,  True,  True,  True, False,  True,  True,  True,\n",
              "        False, False,  True,  True,  True, False, False,  True, False,\n",
              "         True, False, False,  True, False, False, False, False, False,\n",
              "        False,  True,  True,  True, False,  True,  True, False,  True,\n",
              "        False,  True,  True, False,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True, False,  True,  True,  True, False,\n",
              "        False, False,  True,  True, False,  True,  True, False, False,\n",
              "         True, False, False,  True,  True,  True, False,  True,  True,\n",
              "        False, False,  True, False,  True,  True, False,  True, False,\n",
              "         True,  True, False,  True, False, False,  True,  True, False,\n",
              "         True,  True,  True, False,  True, False,  True,  True, False,\n",
              "         True,  True,  True,  True,  True, False, False,  True,  True,\n",
              "         True, False,  True,  True, False,  True, False,  True,  True,\n",
              "         True, False,  True,  True,  True,  True,  True, False,  True,\n",
              "        False,  True,  True,  True,  True, False,  True,  True, False,\n",
              "        False,  True,  True,  True,  True,  True, False, False, False,\n",
              "        False, False, False, False,  True,  True,  True,  True, False,\n",
              "         True,  True, False,  True, False,  True,  True,  True, False,\n",
              "        False, False]),\n",
              " <tf.Tensor 'Placeholder_7:0' shape=<unknown> dtype=int32>: (22946,)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iVByLkIcJDu"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}